{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1 answer\n",
        "\n",
        "Bayes' theorem is a fundamental concept in probability theory and statistics, which describes how to update the probability of a hypothesis (an event or proposition) based on new evidence. In the context of classification and machine learning, it is often used to calculate conditional probabilities, especially in the field of Bayesian inference.\n",
        "\n",
        "Mathematically, Bayes' theorem is expressed as follows:\n",
        "p(A|B)=P(B|A).P(A)/P(B)\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "P(A∣B) is the probability of event A given evidence B.\n",
        "\n",
        "P(B∣A) is the probability of evidence B given event A.\n",
        "\n",
        "P(A) is the prior probability of event A (the probability of A before considering the evidence).\n",
        "\n",
        "P(B) is the prior probability of evidence B (the probability of B before considering its relationship with A).\n"
      ],
      "metadata": {
        "id": "GwnRd-SLMsff"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qqb0WWsRMqAt",
        "outputId": "7e9fe0c7-be28-404e-b2cb-4eb3b15916f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Posterior probability of event A given evidence B: 0.5333333333333334\n"
          ]
        }
      ],
      "source": [
        "\n",
        "prior_probability_A = 0.4\n",
        "prior_probability_B = 0.6\n",
        "\n",
        "conditional_probability_B_given_A = 0.8\n",
        "\n",
        "posterior_probability_A_given_B = (conditional_probability_B_given_A * prior_probability_A) / prior_probability_B\n",
        "\n",
        "print(\"Posterior probability of event A given evidence B:\", posterior_probability_A_given_B)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 answer\n",
        "\n",
        "Mathematically, Bayes' theorem is expressed as follows:\n",
        "\n",
        "\n",
        "P(A∣B)=\n",
        "P(B∣A)⋅P(A)/P(B)\n",
        "​\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "\n",
        "P(A∣B) is the probability of event A given evidence B.\n",
        "\n",
        "P(B∣A) is the probability of evidence B given event A.\n",
        "\n",
        "P(A) is the prior probability of event A (the probability of A before considering the evidence).\n",
        "\n",
        "P(B) is the prior probability of evidence B (the probability of B before considering its relationship with A)."
      ],
      "metadata": {
        "id": "-r59SmhdNpH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 answer\n",
        "\n",
        "Bayes' theorem is used in practice in various fields and applications for probabilistic reasoning, decision-making, and statistical inference. In Python, it can be applied in different ways depending on the specific problem. Here are a few common use cases of Bayes' theorem in practice, along with Python examples:\n",
        "\n",
        "1. Naive Bayes Classification:\n",
        "\n",
        "Bayes' theorem is widely used in machine learning for classification tasks, especially in the context of Naive Bayes classifiers. These classifiers are used for spam detection, text classification, sentiment analysis, and more.\n",
        "\n",
        "Here's a simplified example of using the Bernoulli Naive Bayes classifier from scikit-learn for text classification:\n",
        "\n"
      ],
      "metadata": {
        "id": "d-UNcZiPOPs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_data, labels, test_size=0.2)\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "classifier = BernoulliNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "dtgw5OHAOOec"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Bayesian Parameter Estimation:\n",
        "\n",
        "Bayes' theorem is used for Bayesian parameter estimation in statistics. It's used to update probability distributions over parameters based on observed data. This is particularly useful in Bayesian inference.\n",
        "Here's a simple example of Bayesian parameter estimation using Python's scipy.stats module:"
      ],
      "metadata": {
        "id": "xyFjRgkNW0xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import beta\n",
        "\n",
        "prior_parameters = (2, 2)  # Beta distribution parameters for prior\n",
        "observed_successes = 20\n",
        "observed_failures = 10\n",
        "\n",
        "posterior_parameters = (prior_parameters[0] + observed_successes, prior_parameters[1] + observed_failures)\n",
        "\n",
        "posterior_distribution = beta(*posterior_parameters)\n",
        "\n",
        "samples = posterior_distribution.rvs(size=1000)\n",
        "\n",
        "mean = posterior_distribution.mean()\n",
        "credible_interval = posterior_distribution.interval(0.95)\n"
      ],
      "metadata": {
        "id": "-Ti39oHwW4r1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Bayesian Networks:\n",
        "\n",
        "Bayes' theorem is a fundamental component of Bayesian networks, which are used for probabilistic graphical modeling and reasoning. Libraries like pgmpy in Python allow you to work with Bayesian networks.\n",
        "\n",
        "Here's a basic example of defining a Bayesian network in Python using pgmpy:"
      ],
      "metadata": {
        "id": "xOMxwnGYW7IA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pgmpy.models import BayesianNetwork\n",
        "from pgmpy.factors.discrete import TabularCPD\n",
        "from pgmpy.inference import VariableElimination\n",
        "\n",
        "model = BayesianNetwork([('A', 'C'), ('B', 'C')])\n",
        "\n",
        "cpd_a = TabularCPD(variable='A', variable_card=2, values=[[0.6], [0.4]])\n",
        "cpd_b = TabularCPD(variable='B', variable_card=2, values=[[0.7], [0.3]])\n",
        "cpd_c = TabularCPD(variable='C', variable_card=2, values=[[0.8, 0.9, 0.7, 0.1], [0.2, 0.1, 0.3, 0.9]],\n",
        "                   evidence=['A', 'B'], evidence_card=[2, 2])\n",
        "\n",
        "model.add_cpds(cpd_a, cpd_b, cpd_c)\n",
        "\n",
        "inference = VariableElimination(model)\n",
        "result = inference.query(variables=['C'], evidence={'A': 1, 'B': 0})\n"
      ],
      "metadata": {
        "id": "gZbIJFTXW_ba"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 answer\n",
        "\n",
        "Bayes' theorem and conditional probability are closely related concepts in probability theory. Bayes' theorem provides a mathematical framework for calculating conditional probabilities, making it a fundamental tool in probabilistic reasoning. The relationship between Bayes' theorem and conditional probability can be expressed using the theorem itself.\n",
        "\n",
        "Bayes' theorem is often used to update or revise our beliefs about the probability of an event based on new evidence or information. It quantifies how the probability of an event (A) should be adjusted when we have observed some evidence (B).\n",
        "\n",
        "Where:\n",
        "\n",
        "P(A∣B) is the posterior probability of event A given evidence B.\n",
        "\n",
        "P(B∣A) is the conditional probability of evidence B given event A.\n",
        "\n",
        "P(A) is the prior probability of event A (the probability of A before considering the evidence).\n",
        "\n",
        "P(B) is the prior probability of evidence B (the probability of B before considering its relationship with A).\n",
        "In this context:\n",
        "\n",
        "P(A∣B) represents the updated probability of event A given that we've observed evidence B. It's a conditional probability because it considers the event A in the context of evidence B.\n",
        "\n",
        "P(B∣A) represents the probability of observing the evidence B if event A is true. This is a conditional probability because it provides the likelihood of evidence B under the assumption that event A has occurred.\n",
        "\n",
        "So, Bayes' theorem essentially tells us how to calculate the conditional probability\n",
        "P(A∣B) based on the conditional probability\n",
        "P(B∣A) and the prior probabilities P(A) and P(B)."
      ],
      "metadata": {
        "id": "vAbQkee7XXZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 answer\n",
        "\n",
        "Choosing the right type of Naive Bayes classifier (Bernoulli, Multinomial, or Gaussian) for a given problem depends on the nature of your data and the assumptions that each classifier makes. Here's a guide on how to make that choice in Python:\n",
        "\n",
        "1. Bernoulli Naive Bayes:\n",
        "\n",
        "Binary Data: Use Bernoulli Naive Bayes when your data is binary, meaning it has only two possible values (e.g., 0 and 1, True and False). This is common in text classification problems where you represent text data as binary features (word presence/absence).\n",
        "\n",
        "Sparse Binary Features: It's suitable for problems where the features are sparse (many zeros) and binary, like text data where you're interested in the presence or absence of words.\n"
      ],
      "metadata": {
        "id": "rlghmtGGYZ29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "classifier = BernoulliNB()\n"
      ],
      "metadata": {
        "id": "LejvpTHwYt6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Multinomial Naive Bayes:\n",
        "\n",
        "Count Data: Use Multinomial Naive Bayes when your data represents counts or frequencies of events. This is common in text classification when you have word frequency or term frequency features.\n",
        "\n",
        "Discrete Features: It's suitable for discrete data where features represent counts, frequencies, or categorical values."
      ],
      "metadata": {
        "id": "X5Kr-JuFYwqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB()\n"
      ],
      "metadata": {
        "id": "mOCcA6VNY4TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Gaussian Naive Bayes:\n",
        "\n",
        "Continuous Data: Use Gaussian Naive Bayes when your data is continuous and can be modeled using a Gaussian (normal) distribution.\n",
        "\n",
        "Real-Valued Features: It's suitable for problems where features are real-valued and can be assumed to follow a normal distribution."
      ],
      "metadata": {
        "id": "wqYJHyY1Y50r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier = GaussianNB()\n"
      ],
      "metadata": {
        "id": "nDxx1ZmnY-43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When deciding which type of Naive Bayes to use in Python, consider the following steps:\n",
        "\n",
        "1. Understand Your Data:\n",
        "\n",
        "Examine the type and distribution of your features. Are they binary, counts, or continuous?\n",
        "2. Consider Assumptions:\n",
        "\n",
        "Think about whether the independence assumptions made by the Naive Bayes classifiers are reasonable for your data. In practice, the \"naive\" assumption of feature independence might not hold.\n",
        "3. Experiment and Evaluate:\n",
        "\n",
        "Implement all three types of Naive Bayes classifiers (Bernoulli, Multinomial, and Gaussian) and evaluate their performance using cross-validation or a hold-out validation set.\n",
        "4. Performance Metrics:\n",
        "\n",
        "Choose the classifier that performs best according to your chosen performance metrics (e.g., accuracy, precision, recall, F1-score).\n",
        "5. Iterate and Tune:\n",
        "\n",
        "Depending on your results, consider hyperparameter tuning, feature engineering, or other preprocessing steps to improve the performance of your chosen Naive Bayes classifier."
      ],
      "metadata": {
        "id": "fJ5xwiN1ZC4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 answer\n"
      ],
      "metadata": {
        "id": "HkZp-mvtZIyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "frequency_table = {\n",
        "    'A': {'X1=1': 3, 'X1=2': 3, 'X1=3': 4, 'X2=1': 4, 'X2=2': 3, 'X2=3': 3, 'X2=4': 3},\n",
        "    'B': {'X1=1': 2, 'X1=2': 2, 'X1=3': 1, 'X2=1': 2, 'X2=2': 2, 'X2=3': 2, 'X2=4': 3}\n",
        "}\n",
        "\n",
        "total_count_A = sum(frequency_table['A'].values())\n",
        "total_count_B = sum(frequency_table['B'].values())\n",
        "\n",
        "likelihood_A = (frequency_table['A']['X1=3'] / total_count_A) * (frequency_table['A']['X2=4'] / total_count_A)\n",
        "likelihood_B = (frequency_table['B']['X1=3'] / total_count_B) * (frequency_table['B']['X2=4'] / total_count_B)\n",
        "\n",
        "predicted_class = 'A' if likelihood_A > likelihood_B else 'B'\n",
        "\n",
        "print(\"Predicted Class:\", predicted_class)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh11KrRZZZQw",
        "outputId": "190b8a45-695e-44d0-e6df-5d27eb649831"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: A\n"
          ]
        }
      ]
    }
  ]
}