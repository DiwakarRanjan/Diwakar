{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1 answer"
      ],
      "metadata": {
        "id": "mHWL4lQtaHG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Given probabilities\n",
        "probability_uses_insurance = 0.70\n",
        "probability_smoker_given_uses_insurance = 0.40\n",
        "\n",
        "probability_smoker_and_uses_insurance = (\n",
        "    probability_smoker_given_uses_insurance * probability_uses_insurance\n",
        ")\n",
        "\n",
        "probability_smoker_given_uses_insurance = (\n",
        "    probability_smoker_and_uses_insurance / probability_uses_insurance\n",
        ")\n",
        "\n",
        "print(\"Probability that an employee is a smoker given they use the insurance plan:\", probability_smoker_given_uses_insurance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_f_P4V-aI-k",
        "outputId": "45c79adc-1d21-463a-fd6e-27deff128a75"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability that an employee is a smoker given they use the insurance plan: 0.39999999999999997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We define the given probabilities: probability_uses_insurance is the probability that an employee uses the insurance plan (70%), and probability_smoker_given_uses_insurance is the probability that an employee is a smoker given they use the plan (40%).\n",
        "\n",
        "2. We calculate the joint probability that an employee is both a smoker and uses the insurance plan (probability_smoker_and_uses_insurance) by multiplying the conditional probability by the probability of using the insurance plan.\n",
        "\n",
        "3. Finally, we calculate the conditional probability that an employee is a smoker given they use the insurance plan by dividing the joint probability by the probability of using the insurance plan."
      ],
      "metadata": {
        "id": "w0I0F87taagG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 answer\n",
        "\n",
        "Bernoulli Naive Bayes and Multinomial Naive Bayes are two different variants of the Naive Bayes algorithm used for classification tasks. They differ in terms of the type of data they are designed to work with and the assumptions they make about the data. Here are the key differences between them:\n",
        "\n",
        "1. Data Type:\n",
        "\n",
        "Bernoulli Naive Bayes: This variant is designed for binary data, where each feature can take on one of two values (usually 0 and 1). It's particularly suitable for problems where you want to classify data based on the presence or absence of certain features. For example, text classification where you're interested in whether words are present or not.\n",
        "\n",
        "Multinomial Naive Bayes: Multinomial Naive Bayes is designed for count-based data, such as word counts in text data. It's appropriate for problems where features represent counts or frequencies of events, and the values are non-negative integers.\n",
        "\n",
        "2. Feature Representation:\n",
        "\n",
        "Bernoulli Naive Bayes: Features are typically represented as binary values (0 or 1) to indicate the absence or presence of a feature.\n",
        "\n",
        "Multinomial Naive Bayes: Features are typically represented as counts or frequencies. For example, in text classification, features may represent the frequency of each word in a document.\n",
        "\n",
        "3. Probability Distribution:\n",
        "\n",
        "Bernoulli Naive Bayes: It models the data as a collection of binary random variables. It uses the Bernoulli distribution to model the likelihood of observing binary features.\n",
        "\n",
        "Multinomial Naive Bayes: It models the data as a collection of discrete random variables. It uses the Multinomial distribution to model the likelihood of observing counts or frequencies.\n",
        "\n",
        "4. Mathematical Formulation:\n",
        "\n",
        "Bernoulli Naive Bayes: It calculates probabilities based on the presence or absence of features and assumes feature independence.\n",
        "\n",
        "Multinomial Naive Bayes: It calculates probabilities based on the counts or frequencies of features and assumes feature independence.\n",
        "\n",
        "5. Use Cases:\n",
        "\n",
        "Bernoulli Naive Bayes: It is commonly used in text classification tasks where the goal is to classify documents as belonging to one of two classes (e.g., spam or not spam) based on the presence or absence of specific words or features.\n",
        "\n",
        "Multinomial Naive Bayes: It is widely used in text classification for problems where you want to consider the frequency of words or features in documents. It's also used in other count-based data classification tasks, such as sentiment analysis.\n"
      ],
      "metadata": {
        "id": "v2INCFQ_afyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 answer\n",
        "\n",
        "Bernoulli Naive Bayes, like other variants of the Naive Bayes algorithm, assumes that all features are present and binary (taking values of 0 or 1). When dealing with missing values in a dataset, you need to handle them before applying Bernoulli Naive Bayes. Here are some common approaches to handling missing values with Bernoulli Naive Bayes in Python:\n",
        "\n",
        "1. Imputation:\n",
        "\n",
        "One common approach is to impute missing values by replacing them with a default value. For Bernoulli Naive Bayes, you might choose to replace missing values with either 0 or 1, depending on your domain knowledge or the nature of the data."
      ],
      "metadata": {
        "id": "khciW6x9ayLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
        "\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n"
      ],
      "metadata": {
        "id": "4d-Pz_UZbUKD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Exclude Missing Data:\n",
        "\n",
        "Another approach is to exclude rows or instances with missing values from your dataset. This is suitable when the number of missing values is small and won't significantly impact your analysis."
      ],
      "metadata": {
        "id": "C1uj7NGUbWEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_cleaned = df.dropna()\n"
      ],
      "metadata": {
        "id": "qDACE0QHbY39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Missing Value Indicator:\n",
        "\n",
        "Instead of imputing, you can create an additional binary feature (missing value indicator) that indicates whether a value was missing for a particular feature. This way, you explicitly consider the information that some values are missing."
      ],
      "metadata": {
        "id": "7Po31uSCbbeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "missing_indicator = pd.DataFrame(X.isnull().astype(int), columns=X.columns)\n",
        "\n",
        "X_with_indicator = pd.concat([X, missing_indicator], axis=1)\n"
      ],
      "metadata": {
        "id": "GpNqrIQ8bfOE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Model-Based Imputation:\n",
        "\n",
        "For more complex scenarios, you can use model-based imputation techniques like regression imputation, k-nearest neighbors imputation, or predictive modeling to estimate missing values based on the relationships with other features."
      ],
      "metadata": {
        "id": "gyPuC6P7bhcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "imputer = IterativeImputer()\n",
        "\n",
        "X_imputed = imputer.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "s0YCfsdEbj4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 answer\n",
        "\n",
        "yes, Gaussian Naive Bayes can be used for multi-class classification in Python. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes that continuous features are normally distributed within each class. It can be extended to handle multi-class classification problems.\n",
        "\n",
        "In scikit-learn, a popular Python machine learning library, you can use the GaussianNB class for multi-class classification. Here's how to use it:"
      ],
      "metadata": {
        "id": "VbBYq-Ryb7RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "classifier = GaussianNB()\n",
        "\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8LvvBBicRTK",
        "outputId": "592bca8b-3b8a-49fb-e8ca-3674b5f91769"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 answer\n",
        "\n",
        "To perform the tasks you've described, you'll need to follow these steps:\n",
        "\n",
        "1. Data Preparation:\n",
        "\n",
        "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository: Spambase Data Set.\n",
        "\n",
        "2. Load the dataset into a Pandas DataFrame and preprocess it as needed (e.g., handle missing values, scale features if necessary).\n",
        "\n",
        "Implementation:\n",
        "\n",
        "1. Implement the three Naive Bayes classifiers (Bernoulli, Multinomial, and Gaussian) using scikit-learn's library.\n",
        "\n",
        "2. Split the dataset into features (X) and the target variable (y), where y represents whether an email is spam or not.\n",
        "\n",
        "3. Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You can use scikit-learn's StratifiedKFold for this purpose.\n",
        "\n",
        "4. For each fold, fit the model on the training data and evaluate it on the test data.\n",
        "\n",
        "Performance Metrics:\n",
        "\n",
        "1. Calculate and report the following performance metrics for each classifier:\n",
        "\n",
        "Accuracy: The proportion of correctly classified instances.\n",
        "Precision: The proportion of true positives among all predicted positives.\n",
        "Recall: The proportion of true positives among all actual positives.\n",
        "F1 score: The harmonic mean of precision and recall.\n",
        "Discussion:\n",
        "\n",
        "1. Analyze the results obtained from the three Naive Bayes classifiers:\n",
        "\n",
        "Which variant of Naive Bayes performed the best in terms of accuracy, precision, recall, and F1 score?\n",
        "Provide insights into why one variant might have performed better than the others. For example, consider the nature of the data and the assumptions each variant makes.\n",
        "Limitations of Naive Bayes:\n",
        "\n",
        "Discuss any limitations or challenges you observed when using Naive Bayes for this task. Some limitations may include the assumption of independence between features, which may not hold in real-world data.\n",
        "Conclusion:\n",
        "\n",
        "1. Summarize your findings, highlighting the best-performing Naive Bayes variant and the reasons behind its performance.\n",
        "\n",
        "2. Provide suggestions for future work or improvements. For example, you could explore hyperparameter tuning for the Naive Bayes classifiers or try more advanced machine learning algorithms to see if they outperform Naive Bayes on this dataset."
      ],
      "metadata": {
        "id": "Amiw2h1xdDe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "data = pd.read_csv('/content/spambase.data', header=None)\n",
        "\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "classifiers = {\n",
        "    'Bernoulli Naive Bayes': BernoulliNB(),\n",
        "    'Multinomial Naive Bayes': MultinomialNB(),\n",
        "    'Gaussian Naive Bayes': GaussianNB()\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, classifier in classifiers.items():\n",
        "\n",
        "    scores = cross_val_score(classifier, X, y, cv=10, scoring='accuracy')\n",
        "\n",
        "    accuracy = scores.mean()\n",
        "    precision = precision_score(y, classifier.fit(X, y).predict(X), average='macro')\n",
        "    recall = recall_score(y, classifier.fit(X, y).predict(X), average='macro')\n",
        "    f1 = f1_score(y, classifier.fit(X, y).predict(X), average='macro')\n",
        "\n",
        "    results[name] = {\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1 Score': f1\n",
        "    }\n",
        "\n",
        "for name, metrics in results.items():\n",
        "    print(f\"Classifier: {name}\")\n",
        "    print(f\"Accuracy: {metrics['Accuracy']:.2f}\")\n",
        "    print(f\"Precision: {metrics['Precision']:.2f}\")\n",
        "    print(f\"Recall: {metrics['Recall']:.2f}\")\n",
        "    print(f\"F1 Score: {metrics['F1 Score']:.2f}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "best_classifier = max(results, key=lambda k: results[k]['Accuracy'])\n",
        "print(f\"The best performing classifier is {best_classifier} with an accuracy of {results[best_classifier]['Accuracy']:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2YGdsRrTdhjy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}