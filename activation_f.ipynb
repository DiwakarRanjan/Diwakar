{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1 answer\n",
        "\n",
        "In the context of artificial neural networks, an activation function is a mathematical function that determines the output of a neuron or node in a neural network based on its weighted input. It introduces non-linearity into the network, allowing it to learn complex patterns and make more accurate predictions.\n",
        "\n",
        "Each neuron in a neural network receives input from the previous layer (or the input data itself) and calculates a weighted sum of these inputs. This weighted sum is then passed through the activation function, which transforms it into the neuron's output. The purpose of the activation function is to introduce non-linearity into the model, which enables the network to learn and approximate complex, non-linear relationships in data.\n",
        "\n",
        "There are several types of activation functions commonly used in neural networks, including:\n",
        "\n",
        "1. Sigmoid Function: The sigmoid function maps input values to a range between 0 and 1. It was commonly used in the past but has fallen out of favor for hidden layers in deep neural networks due to the vanishing gradient problem.\n",
        "\n",
        "2. Hyperbolic Tangent Function (tanh): Similar to the sigmoid function, but it maps input values to a range between -1 and 1. It also suffers from the vanishing gradient problem but can be used in some cases.\n",
        "\n",
        "3. Rectified Linear Unit (ReLU): The ReLU activation function is defined as f(x) = max(0, x). It is widely used in deep learning because of its simplicity and effectiveness in training deep networks. However, it can suffer from the \"dying ReLU\" problem, where neurons can get stuck in an inactive state during training.\n",
        "\n",
        "4. Leaky ReLU: A variant of ReLU that addresses the dying ReLU problem by allowing a small, non-zero gradient when the input is negative.\n",
        "\n",
        "5. Parametric ReLU (PReLU): An extension of Leaky ReLU where the slope of the negative part of the function is learned during training, rather than being a fixed parameter.\n",
        "\n",
        "6. Exponential Linear Unit (ELU): ELU is another variant of ReLU that aims to mitigate some of its drawbacks, including the dying ReLU problem. It introduces a smooth, non-zero slope for negative input values.\n",
        "\n",
        "7. Scaled Exponential Linear Unit (SELU): SELU is a self-normalizing variant of the ELU activation function, which can help maintain stable activations during training and improve convergence."
      ],
      "metadata": {
        "id": "P0Gu0L9jWupI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 answer\n",
        "\n",
        "Common types of activation functions used in neural networks include:\n",
        "\n",
        "Sigmoid Function (Logistic):\n",
        "\n",
        "Formula: σ(x) = 1 / (1 + e^(-x))\n",
        "Range: (0, 1)\n",
        "Historically used for binary classification problems, but less common in hidden layers of deep networks due to the vanishing gradient problem.\n",
        "Hyperbolic Tangent Function (tanh):\n",
        "\n",
        "Formula: tanh(x) = (e^(x) - e^(-x)) / (e^(x) + e^(-x))\n",
        "Range: (-1, 1)\n",
        "Similar to the sigmoid function but centered around zero, which can help with training convergence.\n",
        "Rectified Linear Unit (ReLU):\n",
        "\n",
        "Formula: ReLU(x) = max(0, x)\n",
        "Range: [0, ∞)\n",
        "Widely used due to its simplicity and effectiveness in training deep networks. However, it can suffer from the \"dying ReLU\" problem.\n",
        "Leaky Rectified Linear Unit (Leaky ReLU):\n",
        "\n",
        "Formula: Leaky ReLU(x) = x if x > 0, else αx (where α is a small positive constant, typically around 0.01)\n",
        "Range: (-∞, ∞)\n",
        "Addresses the dying ReLU problem by allowing a small, non-zero gradient for negative inputs.\n",
        "Parametric Rectified Linear Unit (PReLU):\n",
        "\n",
        "Formula: PReLU(x) = x if x > 0, else αx (where α is a learnable parameter)\n",
        "Similar to Leaky ReLU but allows the model to learn the value of α during training.\n",
        "Exponential Linear Unit (ELU):\n",
        "\n",
        "Formula: ELU(x) = x if x > 0, else α(e^x - 1) (where α is a positive constant, typically around 1.0)\n",
        "Introduces a smooth, non-zero slope for negative inputs, addressing some issues with ReLU.\n",
        "Scaled Exponential Linear Unit (SELU):\n",
        "\n",
        "Formula: SELU(x) = λ * (e^x - 1) if x < 0, else x (where λ and α are scaling parameters)\n",
        "Designed to enable self-normalization of activations, which can improve convergence in deep networks.\n",
        "Swish:\n",
        "\n",
        "Formula: Swish(x) = x * σ(x) (where σ is the sigmoid function)\n",
        "Introduced as an alternative to ReLU, Swish combines the benefits of ReLU and the sigmoid function.\n",
        "Gated Activation Functions (e.g., LSTM and GRU):\n",
        "\n",
        "Used in recurrent neural networks (RNNs) and related architectures, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). These functions introduce gating mechanisms to control information flow within the network."
      ],
      "metadata": {
        "id": "7Y30omZQW_kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 answer\n",
        "\n",
        "Activation functions play a crucial role in the training process and performance of a neural network. They impact both the network's ability to learn complex patterns and its convergence during training. Here's how activation functions affect these aspects:\n",
        "\n",
        "1. Non-Linearity and Representation Power:\n",
        "\n",
        "Activation functions introduce non-linearity into the network. This non-linearity allows neural networks to approximate and learn complex, non-linear relationships in data. Without non-linear activation functions, a neural network would behave like a linear model, limiting its representation power.\n",
        "2. Vanishing and Exploding Gradients:\n",
        "\n",
        "The choice of activation function can affect the gradients during backpropagation, which is essential for updating the model's weights. Some activation functions, like the sigmoid and tanh functions, can lead to vanishing gradients for very large or small inputs. This can make training slow or even prevent convergence.\n",
        "Activation functions like ReLU are less prone to vanishing gradients, but they can suffer from the exploding gradient problem for very large inputs.\n",
        "3. Training Speed and Convergence:\n",
        "\n",
        "Activation functions can impact how quickly a neural network converges during training. Functions like ReLU tend to converge faster than functions like sigmoid and tanh because they do not saturate for positive inputs. However, ReLU variants may also suffer from faster initial convergence followed by periods of slow learning (the \"dying ReLU\" problem), especially if the learning rate is not properly tuned.\n",
        "Activation functions like SELU are designed to promote self-normalization of activations, which can lead to faster convergence in deep networks.\n",
        "4. Sparsity and Capacity Control:\n",
        "\n",
        "Activation functions like ReLU and its variants can introduce sparsity into the network because they set some activations to zero for negative inputs. This can help the network focus on relevant features and reduce overfitting.\n",
        "Sparsity can be beneficial, but it may also reduce the network's capacity to represent certain patterns. Careful design and tuning of the network architecture are needed to balance sparsity and capacity.\n",
        "5. Robustness to Input Variations:\n",
        "\n",
        "Different activation functions respond differently to variations in input data. For example, ReLU can be sensitive to small input changes around zero, potentially leading to unstable behavior. ELU and SELU are designed to be more robust to input variations.\n",
        "6. Memory Efficiency:\n",
        "\n",
        "Activation functions also affect the memory requirements of a neural network. Some functions require storing additional parameters (e.g., α in Leaky ReLU or ELU), which can increase memory consumption.\n",
        "7. Generalization and Overfitting:\n",
        "\n",
        "The choice of activation function can influence a network's generalization performance. Some functions may generalize better to unseen data by promoting smoother decision boundaries, while others may be more prone to overfitting the training data."
      ],
      "metadata": {
        "id": "eD6u_I1oXMh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 answer\n",
        "\n",
        "The sigmoid activation function, also known as the logistic function, is a common non-linear activation function used in artificial neural networks. It works by mapping input values to a range between 0 and 1. Here's how the sigmoid function works:\n",
        "\n",
        "Sigmoid Function Formula:\n",
        "The sigmoid function is defined as:\n",
        "\n",
        "σ(x) = 1 / (1 + e^(-x))\n",
        "\n",
        "\"x\" represents the weighted sum of inputs and biases for a neuron.\n",
        "\"e\" is the base of the natural logarithm, approximately equal to 2.71828.\n",
        "Working of Sigmoid Function:\n",
        "\n",
        "1. For positive values of \"x,\" the sigmoid function approaches 1, making the neuron's output close to 1. This is similar to activation.\n",
        "2. For negative values of \"x,\" the sigmoid function approaches 0, making the neuron's output close to 0. This is similar to deactivation.\n",
        "3. For \"x\" near 0, the sigmoid function returns a value close to 0.5, indicating that the neuron is in a somewhat neutral state.\n",
        "Advantages of the Sigmoid Activation Function:\n",
        "\n",
        "1. Smooth and Differentiable: The sigmoid function is smooth and differentiable everywhere, which makes it suitable for gradient-based optimization techniques like backpropagation.\n",
        "\n",
        "2. Output Range: The sigmoid function maps input values to a bounded range between 0 and 1, which can be interpreted as probabilities. This is useful in binary classification tasks where the output represents the probability of belonging to one of the classes.\n",
        "\n",
        "3. Historical Significance: The sigmoid function was historically used in the early days of neural networks and logistic regression, and it has a well-understood mathematical formulation.\n",
        "\n",
        "Disadvantages of the Sigmoid Activation Function:\n",
        "\n",
        "1. Vanishing Gradient: One of the significant disadvantages of the sigmoid function is the vanishing gradient problem. When the input values are very large or very small, the gradients of the sigmoid function become extremely small, leading to slow convergence during training. This can make it challenging to train deep networks effectively.\n",
        "\n",
        "2. Output Range Limitation: The output of the sigmoid function is confined to a small range (0 to 1), which can lead to a problem known as \"saturation.\" This means that neurons may saturate (output values close to 0 or 1) and not update their weights effectively during training, especially in the presence of large errors.\n",
        "\n",
        "3. Not Centered at Zero: Unlike some other activation functions like ReLU, the sigmoid function is not centered at zero. This can cause issues when training deep networks, as it may lead to vanishing gradients.\n",
        "\n",
        "4. Computationally Expensive: Computing the exponential function \"e^(-x)\" can be computationally expensive, especially when dealing with large batches or deep networks."
      ],
      "metadata": {
        "id": "nnOJW9urXkHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 answer\n",
        "\n",
        "The Rectified Linear Unit (ReLU) activation function is a non-linear activation function commonly used in artificial neural networks, especially in the hidden layers. It introduces non-linearity by outputting the input directly if it's positive and setting it to zero otherwise. Here's how the ReLU function works:\n",
        "\n",
        "ReLU Function Formula:\n",
        "The ReLU function is defined as:\n",
        "\n",
        "ReLU(x) = max(0, x)\n",
        "\n",
        "\"x\" represents the weighted sum of inputs and biases for a neuron.\n",
        "If \"x\" is greater than or equal to zero, ReLU returns \"x.\"\n",
        "If \"x\" is negative, ReLU returns zero.\n",
        "Key Characteristics of ReLU:\n",
        "\n",
        "1. Non-linearity: ReLU is a non-linear activation function because it introduces a threshold and outputs zero for negative inputs, creating a piecewise linear function.\n",
        "\n",
        "2. Simplicity: ReLU is computationally efficient because it involves only a simple thresholding operation and is much faster to compute compared to the sigmoid and tanh functions, which involve exponentiation.\n",
        "\n",
        "3. Sparsity: ReLU can introduce sparsity into the network because it sets negative activations to zero. This sparsity can help reduce overfitting and focus the network on relevant features.\n",
        "\n",
        "Differences from the Sigmoid Function:\n",
        "\n",
        "1. Output Range:\n",
        "\n",
        "Sigmoid: The sigmoid function maps input values to the range (0, 1). It produces values close to zero for very negative inputs and values close to one for very positive inputs.\n",
        "ReLU: The ReLU function maps negative inputs to zero and leaves positive inputs unchanged, resulting in a range of [0, ∞).\n",
        "2. Vanishing Gradient:\n",
        "\n",
        "Sigmoid: The sigmoid function is prone to the vanishing gradient problem. For very large or very small inputs, the gradient becomes close to zero, making it challenging to train deep networks effectively.\n",
        "ReLU: ReLU mitigates the vanishing gradient problem to some extent. It provides a constant gradient of 1 for positive inputs, ensuring that gradients do not vanish for positive values. However, it can suffer from the \"dying ReLU\" problem, where neurons can get stuck in an inactive state (outputting zero) during training.\n",
        "3. Training Speed:\n",
        "\n",
        "Sigmoid: Sigmoid functions have a smooth gradient, but this smoothness can slow down convergence, especially in deep networks.\n",
        "ReLU: ReLU functions have a more abrupt gradient, which can lead to faster convergence in many cases. However, the initial training phase may be very fast, followed by slower convergence as some neurons become inactive (dying ReLU problem).\n",
        "4. Centered at Zero:\n",
        "\n",
        "Sigmoid: The sigmoid function is centered at zero, meaning that its output can take both positive and negative values.\n",
        "ReLU: ReLU is not centered at zero and only produces positive values or zero. This can impact the network's behavior, especially when dealing with data centered around zero."
      ],
      "metadata": {
        "id": "rLIwcdJeYFY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 answer\n",
        "\n",
        "Using the Rectified Linear Unit (ReLU) activation function over the sigmoid function offers several benefits, particularly in the context of training deep neural networks. Here are some of the key advantages of ReLU:\n",
        "\n",
        "1. Faster Convergence:\n",
        "\n",
        "ReLU typically leads to faster convergence during training compared to sigmoid and tanh activations. This is because ReLU provides a constant gradient (1) for positive inputs, which speeds up the weight updates in gradient-based optimization algorithms like stochastic gradient descent (SGD).\n",
        "2. Mitigating the Vanishing Gradient Problem:\n",
        "\n",
        "Sigmoid and tanh activations can suffer from the vanishing gradient problem for very large or very small inputs, which can lead to slow convergence or difficulty in training deep networks. ReLU, by design, mitigates this issue for positive inputs, as its gradient remains non-zero.\n",
        "3. Sparsity and Feature Selection:\n",
        "\n",
        "ReLU introduces sparsity into the network by setting negative activations to zero. This sparsity can help reduce overfitting and improve the generalization of the network. Additionally, it acts as a form of automatic feature selection by allowing the network to focus on relevant features.\n",
        "4. Efficiency:\n",
        "\n",
        "Computationally, ReLU is more efficient to compute compared to sigmoid and tanh, which involve expensive exponentiation operations. This efficiency is especially important when dealing with deep networks and large datasets.\n",
        "5. Avoiding the \"Sigmoid Saturation\" Problem:\n",
        "\n",
        "Sigmoid functions can saturate, causing the gradients to become extremely small. This can lead to very slow learning or convergence. ReLU does not saturate for positive inputs, avoiding this problem.\n",
        "6. Non-Linearity:\n",
        "\n",
        "ReLU introduces non-linearity into the network, which is crucial for enabling neural networks to approximate complex, non-linear relationships in data. This non-linearity is similar to sigmoid but without its saturation issues.\n",
        "7. Ease of Interpretation:\n",
        "\n",
        "ReLU activations are closer to biological neurons' behavior. When a neuron receives a strong enough input signal, it fires, similar to ReLU's behavior for positive inputs.\n",
        "8. State-of-the-Art Performance:\n",
        "\n",
        "In practice, many state-of-the-art deep learning models, including convolutional neural networks (CNNs) and deep neural networks for natural language processing tasks, use ReLU or its variants as their activation functions."
      ],
      "metadata": {
        "id": "cKHOzlhiYjmw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 answer\n",
        "\n",
        "\n",
        "\n",
        "Leaky ReLU, short for Leaky Rectified Linear Unit, is an activation function that addresses some of the limitations of the traditional Rectified Linear Unit (ReLU) activation function, particularly the \"dying ReLU\" problem and the vanishing gradient problem.\n",
        "\n",
        "Leaky ReLU Function:\n",
        "The Leaky ReLU function is defined as:\n",
        "\n",
        "Leaky ReLU(x) = x if x > 0\n",
        "αx if x ≤ 0\n",
        "\n",
        "\"x\" represents the weighted sum of inputs and biases for a neuron.\n",
        "α (alpha) is a small positive constant, typically set to a small value like 0.01.\n",
        "Concept of Leaky ReLU:\n",
        "Leaky ReLU introduces a small slope (αx) for negative inputs, whereas ReLU sets negative inputs to zero. This small, non-zero slope for negative inputs is what differentiates Leaky ReLU from the original ReLU. The purpose of this non-zero slope is to address the \"dying ReLU\" problem.\n",
        "\n",
        "Advantages and How Leaky ReLU Addresses the Vanishing Gradient Problem:\n",
        "\n",
        "1. Avoiding \"Dying ReLU\":\n",
        "\n",
        "In the standard ReLU activation function, once a neuron's output becomes zero for a negative input, it stays zero for all subsequent inputs during training. This can happen if the weights of a neuron are updated in such a way that it consistently receives negative inputs.\n",
        "Leaky ReLU mitigates the \"dying ReLU\" problem by allowing a small, non-zero gradient (αx) for negative inputs. As a result, even if a neuron's output becomes zero, it can still recover and start learning again if it receives positive gradients in the future.\n",
        "2. Mitigating Vanishing Gradients:\n",
        "\n",
        "The vanishing gradient problem occurs when gradients become very small during backpropagation, leading to slow or stalled training. This is particularly problematic in deep networks with many layers.\n",
        "By providing a non-zero gradient for negative inputs, Leaky ReLU helps mitigate the vanishing gradient problem, especially for neurons with negative activations. This means that gradients can flow through the network more effectively during training, allowing for faster convergence.\n",
        "3. Flexibility in α:\n",
        "\n",
        "The choice of the α parameter in Leaky ReLU provides flexibility in controlling the amount of \"leak\" for negative inputs. Different values of α can be experimented with to find the best setting for a particular problem.\n",
        "While Leaky ReLU addresses some of the issues associated with standard ReLU, it's essential to note that it may not always be the best choice for all problems. In practice, the choice of activation function, whether it's ReLU, Leaky ReLU, or another variant, depends on empirical experimentation and the specific characteristics of the dataset and network architecture. Researchers have also proposed other variants of ReLU, such as Parametric ReLU (PReLU) and Exponential Linear Unit (ELU), which aim to further improve training dynamics and address issues related to activations in deep neural networks."
      ],
      "metadata": {
        "id": "QRtEy3-eY-x0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 answer\n",
        "\n",
        "\n",
        "The softmax activation function is commonly used in artificial neural networks, especially in the output layer of classification models. Its primary purpose is to transform a vector of real numbers into a probability distribution over multiple classes. Here's how the softmax function works and when it is commonly used:\n",
        "\n",
        "Purpose of the Softmax Activation Function:\n",
        "\n",
        "1. Probability Distribution:\n",
        "\n",
        "The softmax function takes a vector of real-valued scores (often referred to as logits) and converts them into a probability distribution. Each element of the resulting vector represents the probability of the input belonging to a particular class.\n",
        "It ensures that the sum of these probabilities across all classes is equal to 1, which makes it suitable for multi-class classification tasks.\n",
        "2. Class Prediction:\n",
        "\n",
        "In a multi-class classification scenario, the class with the highest probability according to the softmax output is typically chosen as the predicted class.\n",
        "The softmax function helps the neural network make a final decision by assigning a probability to each class.\n",
        "Mathematical Formula for Softmax:\n",
        "Given an input vector of scores (logits) z = [z1, z2, ..., zn], the softmax function computes the probabilities as follows:\n",
        "\n",
        "Softmax(z)_i = e^(zi) / Σ(e^(zj)) for i = 1, 2, ..., n\n",
        "\n",
        "\"e\" is the base of the natural logarithm, approximately equal to 2.71828.\n",
        "\"zi\" is the score associated with class i.\n",
        "The denominator Σ(e^(zj)) sums the exponentials of all the scores.\n",
        "Common Use Cases for Softmax:\n",
        "\n",
        "The softmax activation function is commonly used in the following scenarios:\n",
        "\n",
        "1. Multi-Class Classification:\n",
        "\n",
        "Softmax is the go-to activation function for multi-class classification tasks where an input needs to be classified into one of several possible classes.\n",
        "For example, it is used in image classification, natural language processing, and speech recognition to assign probabilities to multiple classes.\n",
        "2. Neural Network Output Layer:\n",
        "\n",
        "Softmax is typically applied in the output layer of neural networks designed for multi-class classification problems.\n",
        "It transforms the raw network outputs (logits) into class probabilities that can be interpreted as the likelihood of the input belonging to each class.\n",
        "3. Evaluation Metrics:\n",
        "\n",
        "Softmax probabilities can be used in conjunction with evaluation metrics like cross-entropy loss to assess the performance of a classification model.\n",
        "Cross-entropy loss measures the dissimilarity between predicted class probabilities and the true class labels.\n",
        "4. Ensemble Models:\n",
        "\n",
        "In ensemble models like gradient boosting, the softmax function can be used as the activation function for the individual base learners to handle multi-class classification tasks."
      ],
      "metadata": {
        "id": "iHzj0_MCZT7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9 answer\n",
        "\n",
        "\n",
        "\n",
        "The hyperbolic tangent (tanh) activation function is a non-linear activation function commonly used in artificial neural networks. It is similar in some ways to the sigmoid activation function but has a different output range and properties. Here's how the tanh function works and how it compares to the sigmoid function:\n",
        "\n",
        "Tanh Function Formula:\n",
        "The hyperbolic tangent (tanh) function is defined as:\n",
        "\n",
        "tanh(x) = (e^(x) - e^(-x)) / (e^(x) + e^(-x))\n",
        "\n",
        "\"x\" represents the weighted sum of inputs and biases for a neuron.\n",
        "\"e\" is the base of the natural logarithm, approximately equal to 2.71828.\n",
        "Key Characteristics of Tanh:\n",
        "\n",
        "1. Output Range:\n",
        "\n",
        "Tanh maps input values to a range between -1 and 1. This is different from the sigmoid function, which maps inputs to a range between 0 and 1.\n",
        "The output range of tanh includes negative values, making it zero-centered, unlike the sigmoid function.\n",
        "2. Non-Linearity:\n",
        "\n",
        "Similar to the sigmoid function, tanh introduces non-linearity into the network. It has an S-shaped curve that allows neural networks to approximate complex, non-linear relationships in data.\n",
        "3. Zero-Centered:\n",
        "\n",
        "Tanh is zero-centered, meaning that its output has a mean of zero. This zero-centered property can help mitigate some issues related to gradient descent in deep networks and can be useful when dealing with data that has zero mean.\n",
        "Comparison to the Sigmoid Function:\n",
        "\n",
        "1. Output Range:\n",
        "\n",
        "Sigmoid: The sigmoid function maps inputs to a range between 0 and 1, with an output centered around 0.5.\n",
        "Tanh: Tanh maps inputs to a range between -1 and 1, with an output centered around zero.\n",
        "2. Vanishing Gradient:\n",
        "\n",
        "Sigmoid: Like the sigmoid function, tanh can also suffer from the vanishing gradient problem for very large or very small inputs. This can slow down training and affect convergence, especially in deep networks.\n",
        "3. Zero-Centeredness:\n",
        "\n",
        "Sigmoid: The sigmoid function is not zero-centered; its outputs are always positive.\n",
        "Tanh: Tanh is zero-centered, which can make it more suitable for certain optimization algorithms, as it helps avoid biases in weight updates.\n",
        "4. Similarity in Shape:\n",
        "\n",
        "Both sigmoid and tanh functions have an S-shaped curve, and they share some similarities in terms of non-linearity. However, the tanh function has a larger output range (-1 to 1) compared to sigmoid (0 to 1).\n",
        "In practice, the choice between sigmoid and tanh activations, especially in hidden layers of neural networks, depends on the specific problem and empirical experimentation. While tanh has some advantages, such as being zero-centered, it may still suffer from the vanishing gradient problem in deep networks. For this reason, other activation functions like ReLU and its variants are often preferred in modern deep learning architectures."
      ],
      "metadata": {
        "id": "ypuH_M25ZqI5"
      }
    }
  ]
}