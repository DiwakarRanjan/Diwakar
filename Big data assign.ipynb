{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1 answer\n",
        "\n",
        "Certainly! The Hadoop ecosystem comprises several core components that work together to process and store big data efficiently. Here's an overview of three key components: Hadoop Distributed File System (HDFS), MapReduce, and Yet Another Resource Negotiator (YARN):\n",
        "\n",
        "Hadoop Distributed File System (HDFS):\n",
        "\n",
        "HDFS is a distributed file system designed to store large volumes of data across multiple machines in a Hadoop cluster.\n",
        "It is highly fault-tolerant, meaning it can handle hardware failures gracefully by replicating data across multiple nodes.\n",
        "HDFS operates on a master-slave architecture, with the NameNode acting as the master and DataNodes as slaves.\n",
        "The NameNode manages the file system namespace and regulates access to files, while DataNodes store the actual data blocks and perform read/write operations.\n",
        "MapReduce:\n",
        "\n",
        "MapReduce is a programming model and processing engine for parallel processing of large datasets across a distributed cluster.\n",
        "It consists of two main phases: the Map phase and the Reduce phase.\n",
        "In the Map phase, input data is divided into smaller chunks, processed in parallel by multiple nodes, and transformed into intermediate key-value pairs.\n",
        "In the Reduce phase, the intermediate key-value pairs are shuffled and sorted, and then processed to produce the final output.\n",
        "MapReduce is highly scalable and fault-tolerant, making it suitable for processing large-scale data-intensive tasks.\n",
        "Yet Another Resource Negotiator (YARN):\n",
        "\n",
        "YARN is a resource management and job scheduling framework in Hadoop 2.x, replacing the JobTracker and TaskTracker components of Hadoop 1.x.\n",
        "It separates the resource management and job scheduling functionalities, allowing for more efficient resource utilization and scalability.\n",
        "YARN consists of two main components: ResourceManager and NodeManager.\n",
        "ResourceManager manages the allocation of cluster resources and schedules jobs, while NodeManager runs on individual cluster nodes and monitors resource usage.\n",
        "YARN supports multiple programming models beyond MapReduce, such as Apache Spark, Apache Flink, and Apache Tez, making it more versatile for various types of workloads.\n",
        "In summary, HDFS provides a reliable and scalable storage layer for big data, MapReduce facilitates distributed data processing, and YARN manages resource allocation and job scheduling in a Hadoop cluster. Together, these components form the backbone of the Hadoop ecosystem, enabling organizations to efficiently process and analyze large volumes of data."
      ],
      "metadata": {
        "id": "ODRVmqbgdY0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 answer\n",
        "\n",
        "Hadoop Distributed File System (HDFS) is a distributed file system designed to store and manage large volumes of data across clusters of commodity hardware. It is a core component of the Apache Hadoop ecosystem and serves as the primary storage layer for big data processing applications. Here's a detailed overview of HDFS and its key concepts:\n",
        "\n",
        "NameNode:\n",
        "\n",
        "The NameNode is the centerpiece of the HDFS architecture and acts as the master server.\n",
        "It maintains metadata about the file system namespace, including the directory structure, file permissions, and file-to-block mappings.\n",
        "The NameNode is responsible for coordinating access to files, handling client requests, and managing data block locations.\n",
        "However, it does not store the actual data; instead, it keeps metadata information in memory and on disk.\n",
        "DataNode:\n",
        "\n",
        "DataNodes are worker nodes in the HDFS cluster responsible for storing and managing data blocks.\n",
        "Each DataNode manages storage on the local disk of the machine it runs on and communicates with the NameNode and other DataNodes in the cluster.\n",
        "DataNodes periodically send heartbeat signals to the NameNode to report their status and availability.\n",
        "They store data blocks in a local file system and replicate them across multiple DataNodes for fault tolerance.\n",
        "Blocks:\n",
        "\n",
        "HDFS stores data in the form of blocks, which are fixed-size (typically 128 MB or 256 MB) contiguous chunks of data.\n",
        "Large files are divided into multiple blocks, and each block is stored independently across DataNodes in the cluster.\n",
        "The block size is configurable and is chosen to optimize disk I/O performance and reduce the overhead of metadata management.\n",
        "HDFS replicates each block across multiple DataNodes (usually three replicas by default) to ensure data reliability and fault tolerance.\n",
        "Replication helps in mitigating the risk of data loss due to hardware failures or node outages.\n",
        "How data is stored and managed in HDFS in a distributed environment:\n",
        "\n",
        "Write process:\n",
        "\n",
        "When a client wants to write a file to HDFS, it communicates with the NameNode to obtain the list of DataNodes where the file's blocks will be stored.\n",
        "The client then divides the file into blocks and streams them to the designated DataNodes.\n",
        "Each DataNode writes the received blocks to its local disk and acknowledges the completion of the write operation to the client.\n",
        "The NameNode updates its metadata to reflect the new file and block locations.\n",
        "Read process:\n",
        "\n",
        "When a client wants to read a file from HDFS, it first contacts the NameNode to obtain the locations of the file's blocks.\n",
        "The client then retrieves the blocks directly from the DataNodes where they are stored in parallel.\n",
        "If a DataNode becomes unavailable during the read process, the client can retrieve the data from one of the other replicas stored on different DataNodes.\n",
        "In summary, HDFS provides a reliable and fault-tolerant storage solution for big data by distributing data across multiple nodes in a cluster, replicating data blocks for redundancy, and using a master-slave architecture with NameNode and DataNode components to manage metadata and data storage operations."
      ],
      "metadata": {
        "id": "f9sZZWXydfUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 answer\n",
        "\n",
        "Sure, here's a step-by-step explanation of how the MapReduce framework works, along with a real-world example:\n",
        "\n",
        "Input Splitting:\n",
        "\n",
        "The input dataset is divided into smaller manageable chunks called input splits.\n",
        "Each input split typically corresponds to a block of data in the Hadoop Distributed File System (HDFS).\n",
        "The number of input splits corresponds to the number of map tasks that will be executed in parallel.\n",
        "Map Phase:\n",
        "\n",
        "In this phase, each map task processes one input split independently.\n",
        "The input split is passed to the map function, which applies a user-defined operation to each record in the split.\n",
        "The map function generates a set of intermediate key-value pairs based on the input records.\n",
        "These intermediate key-value pairs are stored in memory or spilled to disk, and each pair is associated with an intermediate key.\n",
        "Shuffle and Sort:\n",
        "\n",
        "After the map phase completes, the intermediate key-value pairs are shuffled and sorted based on their keys.\n",
        "This step ensures that all values associated with the same key are grouped together and passed to the same reducer in the next phase.\n",
        "Reduce Phase:\n",
        "\n",
        "In this phase, each reducer receives a subset of the intermediate key-value pairs produced by the map phase.\n",
        "The reducer applies a user-defined operation to each key and its associated list of values.\n",
        "The output of the reduce function is typically aggregated values or transformed data.\n",
        "The final output of the reduce phase is written to the output file in HDFS.\n",
        "Example: Word Count:\n",
        "Let's consider the classic example of word count to illustrate the MapReduce framework:\n",
        "\n",
        "Map Phase: Each map task processes a portion of the input text and emits key-value pairs, where the key is a word and the value is the count of occurrences of that word in the input.\n",
        "Shuffle and Sort: Intermediate key-value pairs are shuffled and sorted based on the word keys.\n",
        "Reduce Phase: Each reducer receives a unique word and a list of counts associated with that word. The reducer sums up the counts for each word to produce the final word count.\n",
        "Advantages of MapReduce for processing large datasets:\n",
        "\n",
        "Scalability: MapReduce can process massive datasets by distributing the workload across multiple nodes in a cluster.\n",
        "Fault Tolerance: It automatically handles node failures and retries failed tasks, ensuring reliable processing of data.\n",
        "Simplicity: MapReduce abstracts away the complexities of distributed computing, allowing developers to focus on writing simple map and reduce functions.\n",
        "Limitations of MapReduce:\n",
        "\n",
        "Latency: MapReduce is optimized for batch processing and may not be suitable for low-latency applications that require real-time data processing.\n",
        "Programming Model: Writing MapReduce programs can be cumbersome, especially for complex data processing tasks, as it requires developers to think in terms of map and reduce functions.\n",
        "Overhead: The overhead of task scheduling, data shuffling, and disk I/O can impact performance, especially for small or frequent operations.\n",
        "Overall, MapReduce remains a powerful tool for processing large datasets in parallel, but newer frameworks like Apache Spark have emerged to address some of its limitations and provide more flexibility and efficiency for big data processing tasks."
      ],
      "metadata": {
        "id": "AM-sIU8RdqGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 answer\n",
        "\n",
        "here's a step-by-step explanation of how the MapReduce framework works, along with a real-world example:\n",
        "\n",
        "Input Splitting:\n",
        "\n",
        "The input dataset is divided into smaller manageable chunks called input splits.\n",
        "Each input split typically corresponds to a block of data in the Hadoop Distributed File System (HDFS).\n",
        "The number of input splits corresponds to the number of map tasks that will be executed in parallel.\n",
        "Map Phase:\n",
        "\n",
        "In this phase, each map task processes one input split independently.\n",
        "The input split is passed to the map function, which applies a user-defined operation to each record in the split.\n",
        "The map function generates a set of intermediate key-value pairs based on the input records.\n",
        "These intermediate key-value pairs are stored in memory or spilled to disk, and each pair is associated with an intermediate key.\n",
        "Shuffle and Sort:\n",
        "\n",
        "After the map phase completes, the intermediate key-value pairs are shuffled and sorted based on their keys.\n",
        "This step ensures that all values associated with the same key are grouped together and passed to the same reducer in the next phase.\n",
        "Reduce Phase:\n",
        "\n",
        "In this phase, each reducer receives a subset of the intermediate key-value pairs produced by the map phase.\n",
        "The reducer applies a user-defined operation to each key and its associated list of values.\n",
        "The output of the reduce function is typically aggregated values or transformed data.\n",
        "The final output of the reduce phase is written to the output file in HDFS.\n",
        "Example: Word Count:\n",
        "Let's consider the classic example of word count to illustrate the MapReduce framework:\n",
        "\n",
        "Map Phase: Each map task processes a portion of the input text and emits key-value pairs, where the key is a word and the value is the count of occurrences of that word in the input.\n",
        "Shuffle and Sort: Intermediate key-value pairs are shuffled and sorted based on the word keys.\n",
        "Reduce Phase: Each reducer receives a unique word and a list of counts associated with that word. The reducer sums up the counts for each word to produce the final word count.\n",
        "Advantages of MapReduce for processing large datasets:\n",
        "\n",
        "Scalability: MapReduce can process massive datasets by distributing the workload across multiple nodes in a cluster.\n",
        "Fault Tolerance: It automatically handles node failures and retries failed tasks, ensuring reliable processing of data.\n",
        "Simplicity: MapReduce abstracts away the complexities of distributed computing, allowing developers to focus on writing simple map and reduce functions.\n",
        "Limitations of MapReduce:\n",
        "\n",
        "Latency: MapReduce is optimized for batch processing and may not be suitable for low-latency applications that require real-time data processing.\n",
        "Programming Model: Writing MapReduce programs can be cumbersome, especially for complex data processing tasks, as it requires developers to think in terms of map and reduce functions.\n",
        "Overhead: The overhead of task scheduling, data shuffling, and disk I/O can impact performance, especially for small or frequent operations.\n",
        "Overall, MapReduce remains a powerful tool for processing large datasets in parallel, but newer frameworks like Apache Spark have emerged to address some of its limitations and provide more flexibility and efficiency for big data processing tasks.\n",
        "\n",
        "User\n",
        "Explore the role of YARN in Hadoop. Explain how it manages cluster resources and schedules applications.\n",
        "Compare YARN with the earlier Hadoop 1.x architecture and highlight the benefits of YARN.\n",
        "ChatGPT\n",
        "YARN (Yet Another Resource Negotiator) is a key component of Apache Hadoop 2.x and is responsible for managing cluster resources and scheduling applications. It addresses the limitations of the earlier Hadoop 1.x architecture by decoupling resource management from job scheduling, thus providing more flexibility and scalability. Here's an exploration of YARN's role and benefits compared to the Hadoop 1.x architecture:\n",
        "\n",
        "Role of YARN in Hadoop:\n",
        "\n",
        "Resource Management:\n",
        "\n",
        "YARN separates the resource management functionality from the processing framework, unlike the Hadoop 1.x architecture, where resource management and job scheduling were tightly coupled.\n",
        "It introduces the ResourceManager (RM), which oversees the allocation of cluster resources and manages resource requests from applications.\n",
        "ResourceManager maintains information about available resources in the cluster and allocates them to different applications based on their resource requirements.\n",
        "Job Scheduling:\n",
        "\n",
        "YARN introduces a new component called the ApplicationMaster (AM), which is responsible for negotiating resources with the ResourceManager and coordinating the execution of application tasks.\n",
        "Each application running on YARN has its own ApplicationMaster, which manages the execution of tasks and monitors their progress.\n",
        "ApplicationMasters are launched on cluster nodes dynamically as needed and are responsible for requesting additional resources, handling failures, and reporting progress to the ResourceManager.\n",
        "Resource Containers:\n",
        "\n",
        "YARN introduces the concept of resource containers, which represent a fixed amount of CPU and memory resources on a node.\n",
        "Applications running on YARN request resource containers from the ResourceManager, specifying their resource requirements.\n",
        "Resource containers provide isolation between applications, ensuring that each application receives the resources it needs to execute efficiently without interfering with other applications.\n",
        "Comparison with Hadoop 1.x Architecture:\n",
        "\n",
        "Hadoop 1.x Architecture:\n",
        "\n",
        "In the Hadoop 1.x architecture, the JobTracker was responsible for both resource management and job scheduling.\n",
        "The JobTracker maintained a queue of jobs, allocated map and reduce slots on TaskTrackers, and monitored job progress.\n",
        "This architecture had scalability limitations, as the JobTracker could become a bottleneck in large clusters with a high number of jobs and tasks.\n",
        "YARN Architecture:\n",
        "\n",
        "YARN decouples resource management from job scheduling, introducing separate components for ResourceManager and ApplicationMaster.\n",
        "ResourceManager focuses solely on resource allocation and management, while ApplicationMaster handles job-specific coordination and task execution.\n",
        "This separation of concerns improves scalability and resource utilization, as ResourceManager can handle a larger number of resource requests without being overloaded by job scheduling tasks.\n",
        "Benefits of YARN:\n",
        "\n",
        "Improved Scalability:\n",
        "\n",
        "YARN's decoupled architecture allows for better scalability, as ResourceManager can handle a larger number of resource requests and ApplicationMasters can be dynamically launched on cluster nodes as needed.\n",
        "Enhanced Resource Utilization:\n",
        "\n",
        "YARN's resource containers and fine-grained resource management enable more efficient utilization of cluster resources, leading to better overall performance and resource allocation.\n",
        "Support for Diverse Workloads:\n",
        "\n",
        "YARN supports multiple processing frameworks beyond MapReduce, such as Apache Spark, Apache Flink, and Apache Tez, making it more versatile for different types of workloads and processing paradigms.\n",
        "Improved Fault Tolerance:\n",
        "\n",
        "YARN's distributed architecture improves fault tolerance by distributing critical components such as ResourceManager and ApplicationMaster across multiple nodes in the cluster, reducing the risk of single points of failure."
      ],
      "metadata": {
        "id": "87N00Vizd6SZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 answer\n",
        "Certainly! Here's an overview of some popular components within the Hadoop ecosystem:\n",
        "\n",
        "HBase:\n",
        "\n",
        "HBase is a distributed, scalable, and NoSQL database that runs on top of the Hadoop Distributed File System (HDFS).\n",
        "It provides real-time read/write access to large datasets, making it suitable for applications requiring low-latency data access.\n",
        "HBase is commonly used for storing and querying semi-structured or unstructured data, such as sensor data, social media feeds, and log files.\n",
        "Hive:\n",
        "\n",
        "Hive is a data warehouse infrastructure built on top of Hadoop that provides a SQL-like interface for querying and analyzing data stored in HDFS.\n",
        "It enables analysts and data scientists to write SQL queries to process and analyze large datasets without needing to write complex MapReduce or Java code.\n",
        "Hive supports schema-on-read, allowing users to apply a schema to their data at query time, making it flexible for handling semi-structured and structured data formats like CSV, JSON, and Avro.\n",
        "Pig:\n",
        "\n",
        "Pig is a high-level scripting language and platform for analyzing large datasets in Hadoop.\n",
        "It provides a data flow language called Pig Latin, which abstracts away the complexities of MapReduce programming and enables users to express data transformations and analysis tasks concisely.\n",
        "Pig is commonly used for ETL (Extract, Transform, Load) tasks, data cleaning, and data processing pipelines.\n",
        "Spark:\n",
        "\n",
        "Apache Spark is a fast and general-purpose distributed computing framework that extends the MapReduce model to support more types of computations, including interactive queries, stream processing, and machine learning.\n",
        "Spark provides in-memory processing and caching capabilities, making it significantly faster than traditional MapReduce for iterative and interactive workloads.\n",
        "It offers high-level APIs in Scala, Java, Python, and R, as well as libraries for SQL (Spark SQL), streaming (Spark Streaming), machine learning (MLlib), and graph processing (GraphX).\n",
        "Integration of a Component: Apache Spark\n",
        "\n",
        "Apache Spark can be seamlessly integrated into a Hadoop ecosystem for specific data processing tasks, offering advantages such as speed, ease of use, and support for various processing paradigms. Here's how Spark can be integrated and used within a Hadoop ecosystem:\n",
        "\n",
        "Data Processing:\n",
        "\n",
        "Spark can read and write data from various data sources, including HDFS, HBase, Hive, and external databases, using its rich set of connectors and APIs.\n",
        "It can be used for a wide range of data processing tasks, including ETL, batch processing, real-time analytics, machine learning, and graph processing.\n",
        "Integration with Hadoop Components:\n",
        "\n",
        "Spark can run directly on Hadoop YARN, leveraging YARN's resource management capabilities to allocate resources dynamically across a cluster.\n",
        "Spark can also interact with other Hadoop ecosystem components such as HDFS, Hive, and HBase, allowing users to leverage existing data and infrastructure.\n",
        "Performance:\n",
        "\n",
        "Spark's in-memory processing engine enables significantly faster execution compared to traditional MapReduce, making it well-suited for iterative algorithms and interactive analytics.\n",
        "It can process data up to 100 times faster in memory and up to 10 times faster on disk compared to MapReduce.\n",
        "Ease of Use:\n",
        "\n",
        "Spark provides high-level APIs in multiple programming languages, including Scala, Java, Python, and R, making it accessible to a wide range of users, from data engineers to data scientists.\n",
        "Users can express complex data processing workflows using concise and expressive code, without needing to manage low-level details of distributed computing."
      ],
      "metadata": {
        "id": "kPnlD9K_eKA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 answer\n",
        "\n",
        "Apache Spark and Hadoop MapReduce are both distributed computing frameworks designed for processing large-scale data sets, but they differ significantly in terms of architecture, performance, and supported functionalities. Here are the key differences between Apache Spark and Hadoop MapReduce:\n",
        "\n",
        "In-Memory Processing:\n",
        "\n",
        "Spark: Spark performs in-memory processing, meaning it can store intermediate data in memory between processing stages. This allows Spark to significantly reduce disk I/O and improve performance, especially for iterative algorithms and interactive analytics.\n",
        "MapReduce: MapReduce primarily relies on disk-based processing, where intermediate data is written to disk after each map and reduce stage. This can lead to high disk I/O overhead and slower processing times, especially for iterative algorithms that require repeated data access.\n",
        "Data Processing Models:\n",
        "\n",
        "Spark: Spark supports multiple data processing models, including batch processing, interactive queries, streaming, machine learning, and graph processing. It provides high-level APIs and libraries for each model, making it versatile for various use cases.\n",
        "MapReduce: MapReduce is primarily designed for batch processing of large datasets using a map and reduce paradigm. While it can be extended for other use cases, such as graph processing and machine learning, it often requires writing custom code and is less flexible compared to Spark.\n",
        "Execution Model:\n",
        "\n",
        "Spark: Spark executes tasks in a directed acyclic graph (DAG) of stages, where each stage consists of multiple tasks that can run in parallel. This allows Spark to optimize task execution and perform pipelining of operations, resulting in better resource utilization and reduced latency.\n",
        "MapReduce: MapReduce executes tasks in a strict sequence of map and reduce phases, with intermediate data written to disk between stages. This can introduce latency and overhead due to frequent disk I/O operations and lack of pipelining.\n",
        "Ease of Use:\n",
        "\n",
        "Spark: Spark provides high-level APIs in multiple programming languages, including Scala, Java, Python, and R. It also offers interactive shells and notebooks for exploratory data analysis and experimentation. Additionally, Spark's rich set of libraries and built-in functions simplify common data processing tasks.\n",
        "MapReduce: MapReduce requires writing custom Java code for map and reduce functions, which can be complex and time-consuming, especially for users unfamiliar with Java programming. While higher-level abstractions like Apache Pig and Apache Hive can simplify MapReduce development, they still require understanding of the underlying MapReduce paradigm.\n",
        "Fault Tolerance:\n",
        "\n",
        "Spark: Spark provides fault tolerance through lineage information, where it tracks the sequence of transformations applied to each RDD (Resilient Distributed Dataset). If a partition of an RDD is lost, Spark can reconstruct it by reapplying the transformations that led to its creation.\n",
        "MapReduce: MapReduce achieves fault tolerance through replication of intermediate data and task retry mechanisms. However, the replication overhead can be high, especially for large datasets, and task retries may result in increased processing time."
      ],
      "metadata": {
        "id": "tJxp9Ba2eZBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7 answer\n",
        "\n",
        "Certainly! Below is a simple Spark application in Python that reads a text file, counts the occurrences of each word, and returns the top 10 most frequent words:"
      ],
      "metadata": {
        "id": "TuHJpG01epBQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_OlUMZAdU7m"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, explode, col\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"WordCount\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read the text file into a DataFrame\n",
        "input_file = \"path_to_your_text_file.txt\"\n",
        "lines_df = spark.read.text(input_file)\n",
        "\n",
        "# Split each line into words and explode them into separate rows\n",
        "words_df = lines_df.select(explode(split(col(\"value\"), \"\\s+\")).alias(\"word\"))\n",
        "\n",
        "# Group by word and count occurrences\n",
        "word_counts_df = words_df.groupBy(\"word\").count()\n",
        "\n",
        "# Sort by count in descending order\n",
        "sorted_word_counts_df = word_counts_df.orderBy(\"count\", ascending=False)\n",
        "\n",
        "# Take top 10 most frequent words\n",
        "top_10_words = sorted_word_counts_df.limit(10)\n",
        "\n",
        "# Show the result\n",
        "top_10_words.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 answer\n",
        "\n",
        "Sure, let's perform the specified tasks using Spark RDDs (Resilient Distributed Datasets) on a sample dataset. For this example, let's consider a dataset containing information about sales transactions, with columns such as \"product_id\", \"quantity_sold\", and \"unit_price\". We'll perform the following tasks:\n",
        "\n",
        "a. Filter the data to select only rows where the quantity sold is greater than 10.\n",
        "b. Map a transformation to calculate the total sales amount for each transaction (quantity_sold * unit_price).\n",
        "c. Reduce the dataset to calculate the total sales amount across all transactions.\n",
        "\n",
        "Here's the code in Python using PySpark:"
      ],
      "metadata": {
        "id": "_LkU2sT7e76U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create a SparkConf object and set the application name\n",
        "conf = SparkConf().setAppName(\"RDD Operations\")\n",
        "# Create a SparkContext object\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Load the sample dataset into an RDD (replace 'path_to_your_dataset' with the actual path)\n",
        "dataset_rdd = sc.textFile(\"path_to_your_dataset\")\n",
        "\n",
        "# Task a: Filter the data to select only rows where the quantity sold is greater than 10\n",
        "filtered_rdd = dataset_rdd.filter(lambda line: int(line.split(',')[1]) > 10)\n",
        "\n",
        "# Task b: Map a transformation to calculate the total sales amount for each transaction\n",
        "sales_amount_rdd = filtered_rdd.map(lambda line: (line.split(',')[0], int(line.split(',')[1]) * float(line.split(',')[2])))\n",
        "\n",
        "# Task c: Reduce the dataset to calculate the total sales amount across all transactions\n",
        "total_sales_amount = sales_amount_rdd.values().reduce(lambda x, y: x + y)\n",
        "\n",
        "# Print the total sales amount\n",
        "print(\"Total sales amount: \", total_sales_amount)\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "id": "88ceQ06ffAfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9 answer"
      ],
      "metadata": {
        "id": "XJvtzOkdfIDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum, avg\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DataFrame Operations\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load the dataset (CSV file) into a DataFrame\n",
        "dataset_df = spark.read.csv(\"path_to_your_dataset.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Task a: Select specific columns from the DataFrame\n",
        "selected_columns_df = dataset_df.select(\"column1\", \"column2\", \"column3\")\n",
        "\n",
        "# Task b: Filter rows based on certain conditions\n",
        "filtered_df = dataset_df.filter(col(\"column1\") > 10)\n",
        "\n",
        "# Task c: Group the data by a particular column and calculate aggregations\n",
        "aggregated_df = dataset_df.groupBy(\"column1\").agg(sum(\"column2\").alias(\"total_column2\"), avg(\"column3\").alias(\"avg_column3\"))\n",
        "\n",
        "# Load another dataset (CSV file) into another DataFrame\n",
        "other_dataset_df = spark.read.csv(\"path_to_other_dataset.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Task d: Join two DataFrames based on a common key\n",
        "joined_df = dataset_df.join(other_dataset_df, dataset_df[\"common_key\"] == other_dataset_df[\"common_key\"], \"inner\")\n",
        "\n",
        "# Show the results of each operation\n",
        "selected_columns_df.show()\n",
        "filtered_df.show()\n",
        "aggregated_df.show()\n",
        "joined_df.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "jgwgGMU2fZLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10 answer\n",
        "\n",
        "To set up a Spark Streaming application to process real-time data from a source and perform the specified tasks, we'll use Apache Kafka as the data source and output the processed data to the console as a sink. Here's how you can do it in Python using PySpark:"
      ],
      "metadata": {
        "id": "ezaOGPmofaYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark Streaming Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define the Kafka topic to subscribe to\n",
        "kafka_topic = \"your_kafka_topic\"\n",
        "\n",
        "# Define the Kafka broker address\n",
        "kafka_bootstrap_servers = \"localhost:9092\"\n",
        "\n",
        "# Create a DataFrame representing the stream of input lines from Kafka\n",
        "stream_df = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
        "    .option(\"subscribe\", kafka_topic) \\\n",
        "    .load()\n",
        "\n",
        "# Convert the value column of the DataFrame from bytes to string\n",
        "stream_df = stream_df.selectExpr(\"CAST(value AS STRING)\")\n",
        "\n",
        "# Apply transformation to the streaming data (e.g., filtering)\n",
        "filtered_stream_df = stream_df.filter(col(\"column_name\") == \"desired_value\")\n",
        "\n",
        "# Output the processed data to the console sink\n",
        "query = filtered_stream_df \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "# Wait for the termination of the streaming query\n",
        "query.awaitTermination()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "ENkOqjdkfgOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11 answer\n",
        "\n",
        "Apache Kafka is a distributed streaming platform that is designed to handle large volumes of data in real-time. It is a highly scalable, fault-tolerant, and distributed system that is widely used for building real-time data pipelines and streaming applications. Here are the fundamental concepts of Apache Kafka:\n",
        "\n",
        "1. Publish-Subscribe Messaging System:\n",
        "\n",
        "Kafka follows the publish-subscribe messaging paradigm, where producers publish messages to topics, and consumers subscribe to these topics to receive messages.\n",
        "Topics are logical channels or categories where messages are published. Each message published to a topic is assigned a partition, and each partition is replicated across multiple Kafka brokers for fault tolerance.\n",
        "Producers are applications that publish messages to Kafka topics, while consumers are applications that consume messages from Kafka topics.\n",
        "2. Distributed and Fault-Tolerant:\n",
        "\n",
        "Kafka is designed as a distributed system and can scale horizontally by adding more Kafka brokers to the cluster.\n",
        "It provides built-in fault tolerance by replicating partitions across multiple brokers. If a broker fails, Kafka can continue to serve messages without data loss by promoting one of the replicas as the new leader for the partition.\n",
        "3. Message Retention:\n",
        "\n",
        "Kafka stores messages persistently on disk for a configurable period or until a certain size threshold is reached.\n",
        "Messages are retained even after they are consumed by consumers, allowing new consumers to replay messages from the beginning of the topic.\n",
        "Kafka supports different retention policies, such as time-based retention or size-based retention, to control how long messages are retained in the system.\n",
        "4. Scalability and Performance:\n",
        "\n",
        "Kafka is designed for high throughput and low latency, making it suitable for real-time data processing applications.\n",
        "It can handle millions of messages per second across thousands of partitions and brokers, making it highly scalable.\n",
        "Kafka's architecture allows it to achieve high throughput by parallelizing message processing across multiple partitions and brokers.\n",
        "5. Connectivity and Integration:\n",
        "\n",
        "Kafka provides client libraries for various programming languages, making it easy to integrate with existing applications and systems.\n",
        "It supports integration with other big data frameworks such as Apache Spark, Apache Storm, and Apache Flink for real-time stream processing and analytics.\n",
        "Kafka Connect is a framework for building and running connectors that import/export data between Kafka and other data systems such as databases, file systems, and messaging systems.\n",
        "Apache Kafka aims to solve several problems in the context of big data and real-time data processing:\n",
        "\n",
        "1. Scalable Data Ingestion: Kafka provides a scalable and reliable platform for ingesting large volumes of data from diverse sources, including sensors, applications, and databases.\n",
        "\n",
        "2. Real-Time Data Processing: Kafka enables real-time stream processing by allowing applications to consume and process data as it arrives, making it suitable for use cases such as real-time analytics, monitoring, and fraud detection.\n",
        "\n",
        "3. Data Integration and ETL: Kafka acts as a central data hub for integrating data from multiple sources and systems. It can be used for building real-time data pipelines for extract, transform, and load (ETL) operations.\n",
        "\n",
        "4. Event Sourcing and Event-Driven Architecture: Kafka facilitates event sourcing and event-driven architectures by providing a reliable and scalable platform for capturing, storing, and processing events in real-time."
      ],
      "metadata": {
        "id": "Naqa_GiEfi5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12 answer\n",
        "\n",
        "Apache Kafka's architecture is designed to be distributed, fault-tolerant, and highly scalable. It consists of several key components that work together to achieve data streaming and real-time message processing. Here's an overview of the architecture and key components of Kafka:\n",
        "\n",
        "1. Producers:\n",
        "\n",
        "Producers are applications that publish messages to Kafka topics.\n",
        "They write data to Kafka topics and are responsible for determining which topic and partition to publish each message to.\n",
        "Producers do not interact directly with consumers or brokers; they simply send messages to Kafka clusters.\n",
        "2. Topics:\n",
        "\n",
        "Topics are logical channels or categories where messages are published by producers.\n",
        "Each topic is divided into one or more partitions, which are ordered and immutable sequences of messages.\n",
        "Topics can have multiple consumers, and each message published to a topic is delivered to all consumers subscribed to that topic.\n",
        "3. Partitions:\n",
        "\n",
        "Partitions are the basic unit of parallelism in Kafka.\n",
        "Each partition is an ordered and immutable sequence of messages, and messages within a partition are assigned a sequential offset.\n",
        "Partitions allow Kafka to scale horizontally by distributing message storage and processing across multiple brokers.\n",
        "4. Brokers:\n",
        "\n",
        "Brokers are the Kafka servers that form a Kafka cluster.\n",
        "They are responsible for storing and managing the topic partitions, handling message replication, and serving consumer requests.\n",
        "Each broker in the cluster is identified by a unique broker ID and can host multiple partitions across multiple topics.\n",
        "5. Consumers:\n",
        "\n",
        "Consumers are applications that subscribe to Kafka topics and consume messages from them.\n",
        "They read data from Kafka topics and process it according to their application logic.\n",
        "Consumers can be part of consumer groups, where each consumer in the group receives messages from a different subset of partitions for load balancing and parallelism.\n",
        "6. Consumer Groups:\n",
        "\n",
        "Consumer groups are logical groupings of consumers that work together to consume messages from Kafka topics.\n",
        "Each consumer group is assigned one or more partitions from each topic it subscribes to.\n",
        "Kafka ensures that messages within a partition are consumed in order, but messages across different partitions may be consumed out of order by different consumers in the same group.\n",
        "7. ZooKeeper:\n",
        "\n",
        "ZooKeeper is used by Kafka for distributed coordination and management of the Kafka cluster.\n",
        "It is responsible for maintaining metadata about Kafka brokers, topics, partitions, and consumer groups.\n",
        "ZooKeeper helps Kafka in leader election, partition reassignment, and detecting broker failures and cluster membership changes."
      ],
      "metadata": {
        "id": "Rnnskc63fvUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13 answer\n",
        "Let's create a step-by-step guide to produce data to a Kafka topic using Python as the programming language and then consume that data from the topic. We'll use the confluent-kafka library, which is a Python client for Apache Kafka, to interact with Kafka.\n",
        "\n",
        "Step 1: Install Dependencies\n",
        "Make sure you have Python installed on your system. You also need to install the confluent-kafka library. You can install it using pip:"
      ],
      "metadata": {
        "id": "NW5WNH3Vgflm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install confluent-kafka"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEY5Ogl3gm9P",
        "outputId": "cc711b06-d783-4c30-b411-6003331dad6f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting confluent-kafka\n",
            "  Downloading confluent_kafka-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: confluent-kafka\n",
            "Successfully installed confluent-kafka-2.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Create a Kafka Producer\n",
        "Create a Python script to produce data to a Kafka topic. Here's an example:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MDz_XSEBgtOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka import Producer\n",
        "\n",
        "# Kafka broker settings\n",
        "bootstrap_servers = 'localhost:9092'\n",
        "topic = 'test_topic'\n",
        "\n",
        "# Create a Kafka producer\n",
        "producer = Producer({'bootstrap.servers': bootstrap_servers})\n",
        "\n",
        "# Define a callback function to handle delivery reports\n",
        "def delivery_report(err, msg):\n",
        "    if err is not None:\n",
        "        print(f'Message delivery failed: {err}')\n",
        "    else:\n",
        "        print(f'Message delivered to {msg.topic()} [{msg.partition()}]')\n",
        "\n",
        "# Produce data to the Kafka topic\n",
        "for i in range(10):\n",
        "    key = f'key-{i}'\n",
        "    value = f'value-{i}'\n",
        "    producer.produce(topic, key=key.encode('utf-8'), value=value.encode('utf-8'), callback=delivery_report)\n",
        "\n",
        "# Flush the producer to ensure all messages are delivered\n",
        "producer.flush()\n"
      ],
      "metadata": {
        "id": "nbU2iVjtgrHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Create a Kafka Consumer\n",
        "Create another Python script to consume data from the Kafka topic. Here's an example:"
      ],
      "metadata": {
        "id": "KZGmaYQDg06E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka import Consumer, KafkaError\n",
        "\n",
        "# Kafka broker settings\n",
        "bootstrap_servers = 'localhost:9092'\n",
        "topic = 'test_topic'\n",
        "\n",
        "# Create a Kafka consumer\n",
        "consumer = Consumer({\n",
        "    'bootstrap.servers': bootstrap_servers,\n",
        "    'group.id': 'my_consumer_group',\n",
        "    'auto.offset.reset': 'earliest'  # Start consuming from the beginning of the topic\n",
        "})\n",
        "\n",
        "# Subscribe to the Kafka topic\n",
        "consumer.subscribe([topic])\n",
        "\n",
        "# Poll for new messages and process them\n",
        "try:\n",
        "    while True:\n",
        "        msg = consumer.poll(1.0)  # Poll for new messages with a timeout of 1 second\n",
        "        if msg is None:\n",
        "            continue\n",
        "        if msg.error():\n",
        "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
        "                # End of partition, continue polling\n",
        "                continue\n",
        "            else:\n",
        "                # Handle other errors\n",
        "                print(f'Error: {msg.error()}')\n",
        "                break\n",
        "        # Process the message\n",
        "        print(f'Message received: {msg.key().decode(\"utf-8\")}={msg.value().decode(\"utf-8\")}')\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "finally:\n",
        "    # Close the Kafka consumer\n",
        "    consumer.close()\n"
      ],
      "metadata": {
        "id": "kAEMJgPfg3wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this script:\n",
        "\n",
        "We import the Consumer class from the confluent_kafka library.\n",
        "We create a Kafka consumer with the specified bootstrap servers, consumer group ID, and auto offset reset policy (to start consuming from the beginning of the topic).\n",
        "We subscribe to the Kafka topic.\n",
        "We continuously poll for new messages from the topic and process them.\n",
        "We handle different types of Kafka errors, such as end of partition errors.\n",
        "We close the Kafka consumer when done.\n",
        "Running the Scripts\n",
        "Save both scripts as separate Python files (e.g., producer.py and consumer.py). First, run the producer.py script to produce data to the Kafka topic. Then, run the consumer.py script to consume data from the same topic.\n",
        "\n",
        "Role of Kafka Producers and Consumers\n",
        "Producers: Kafka producers are responsible for publishing messages to Kafka topics. They write data to topics and are typically used to send data from various sources (e.g., applications, sensors, logs) to Kafka for further processing.\n",
        "Consumers: Kafka consumers are responsible for subscribing to Kafka topics and consuming messages from them. They read data from topics and process it according to their application logic. Consumers are typically used to receive and process data published by producers."
      ],
      "metadata": {
        "id": "oYBrHYwgg6zM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14 answer\n",
        "\n",
        "Data retention and data partitioning are crucial aspects of Apache Kafka's architecture that play a significant role in ensuring efficient data storage, processing, and scalability. Let's discuss the importance of these features and how they can be configured:\n",
        "\n",
        "Data Retention:\n",
        "Importance:\n",
        "\n",
        "1. Historical Analysis: Data retention allows Kafka to store messages for a specified period, enabling historical analysis and replayability of data.\n",
        "2. Fault Tolerance: Retained data provides a safety net against data loss in case of failures, allowing consumers to recover and replay messages if necessary.\n",
        "3. Compliance Requirements: Many industries have compliance requirements mandating the retention of data for a certain period, such as financial transactions or healthcare records.\n",
        "Configuration:\n",
        "\n",
        "Data retention can be configured at the topic level in Kafka. You can specify the retention period either based on time (e.g., hours, days) or based on size (e.g., gigabytes).\n",
        "Example: To retain messages for 7 days, you can set the log.retention.hours property to 168 (7 days * 24 hours).\n",
        "Implications:\n",
        "\n",
        "Longer retention periods require more storage space, so you need to allocate sufficient disk space to accommodate the retained data.\n",
        "Retained data may impact the performance of Kafka brokers, especially during log segment rollover and deletion processes.\n",
        "Data Partitioning:\n",
        "Importance:\n",
        "\n",
        "1. Parallelism: Partitioning allows Kafka to scale out and process messages in parallel across multiple brokers and consumers.\n",
        "2. Scalability: By distributing messages across multiple partitions, Kafka can handle higher throughput and support more consumers.\n",
        "3. Ordering Guarantees: Kafka guarantees message ordering within a partition, ensuring that messages with the same key are processed in order.\n",
        "Configuration:\n",
        "\n",
        "Partitioning is configured when creating a Kafka topic. You specify the number of partitions for the topic.\n",
        "Example: To create a topic with 3 partitions, you can set the --partitions option to 3 when using the kafka-topics.sh script.\n",
        "Implications:\n",
        "\n",
        "The number of partitions determines the level of parallelism and scalability of the Kafka cluster. More partitions allow for higher throughput but may also increase overhead.\n",
        "Careful consideration is needed when choosing the number of partitions to avoid over- or under-partitioning, as it can impact the distribution of messages and the efficiency of data processing.\n",
        "Conclusion:\n",
        "Data retention and data partitioning are essential features of Apache Kafka that play a crucial role in its scalability, fault tolerance, and performance. Configuring these features requires careful consideration of factors such as storage requirements, processing throughput, and ordering guarantees. By understanding and appropriately configuring data retention and partitioning, Kafka users can build robust and efficient data streaming pipelines to meet their specific requirements."
      ],
      "metadata": {
        "id": "6cn7N-lvg94W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15 answer\n",
        "\n",
        "Apache Kafka is widely used in various industries and use cases where real-time data streaming, scalability, reliability, and fault tolerance are critical. Here are some examples of real-world use cases where Apache Kafka is employed:\n",
        "\n",
        "1. Log Aggregation and Monitoring:\n",
        "\n",
        "Use Case: Many organizations use Kafka to aggregate and centralize logs from distributed systems, applications, and services. Kafka acts as a durable and scalable buffer, allowing logs to be collected in real-time and processed by downstream systems for monitoring, analytics, and troubleshooting.\n",
        "Benefits:\n",
        "Kafka's distributed architecture enables high throughput and low-latency log ingestion, even at scale.\n",
        "Kafka's fault-tolerant design ensures that log data is not lost, even in the event of system failures or outages.\n",
        "Kafka's partitioning allows logs to be processed in parallel, facilitating efficient log processing and analysis.\n",
        "2. Real-Time Analytics and Metrics:\n",
        "\n",
        "Use Case: Kafka is used to stream real-time data from various sources, such as sensors, IoT devices, websites, and applications, for analytics and metrics monitoring. Data streams are processed in real-time to derive insights, detect anomalies, and trigger alerts.\n",
        "Benefits:\n",
        "Kafka provides low-latency data ingestion and processing, allowing organizations to analyze and respond to events in real-time.\n",
        "Kafka's scalability enables organizations to handle large volumes of streaming data from diverse sources.\n",
        "Kafka's durability ensures that data is reliably stored and processed, even during system failures or network interruptions.\n",
        "3. Event Sourcing and CQRS (Command Query Responsibility Segregation):\n",
        "\n",
        "Use Case: Event sourcing architectures use Kafka to log and store domain events as a source of truth for the system's state. Kafka acts as a distributed event log, allowing applications to replay events and rebuild state as needed. CQRS patterns separate reads and writes, with Kafka serving as the communication layer between command and query services.\n",
        "Benefits:\n",
        "Kafka provides a reliable and durable event log for storing domain events, ensuring consistency and fault tolerance.\n",
        "Kafka's partitioning allows for parallel processing of events and scalability across multiple consumers.\n",
        "Kafka's ordered message delivery ensures that events are processed in the order they were produced, preserving causality and consistency.\n",
        "4. Microservices Communication:\n",
        "\n",
        "Use Case: Kafka serves as a communication backbone for microservices architectures, enabling asynchronous and decoupled communication between services. Services produce and consume events via Kafka topics, facilitating integration, scalability, and fault isolation.\n",
        "Benefits:\n",
        "Kafka decouples producers and consumers, allowing services to communicate asynchronously without direct dependencies.\n",
        "Kafka's fault tolerance ensures reliable message delivery, even in the presence of service failures or network partitions.\n",
        "Kafka's scalability enables horizontal scaling of microservices and handles fluctuating workloads with ease.\n",
        "5. Data Integration and ETL (Extract, Transform, Load):\n",
        "\n",
        "Use Case: Kafka is used as a data integration platform for ingesting, transforming, and loading data from diverse sources into data lakes, data warehouses, and analytical systems. Kafka Connect connectors are used to import/export data between Kafka and other systems, while Kafka Streams and other stream processing frameworks are used for real-time ETL.\n",
        "Benefits:\n",
        "Kafka provides a scalable and fault-tolerant data pipeline for ingesting data from various sources in real-time.\n",
        "Kafka's connectors simplify integration with databases, file systems, messaging systems, and other data sources.\n",
        "Kafka's stream processing capabilities enable real-time data transformations, enrichments, and aggregations before loading data into target systems."
      ],
      "metadata": {
        "id": "B8dgG3jxhe3t"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CQKc_QzTh3_a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}