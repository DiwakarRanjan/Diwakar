{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1 answer\n",
        "\n",
        "Grid Search Cross-Validation (GridSearchCV) is a technique used in machine learning for hyperparameter tuning, which is the process of finding the best set of hyperparameters for a machine learning model. Hyperparameters are the settings that are not learned from the data but must be specified prior to training a model. These parameters can have a significant impact on a model's performance, and finding the optimal combination of hyperparameters is crucial for building effective models.\n",
        "\n",
        "The purpose of GridSearchCV is to systematically search through a predefined set of hyperparameters for a given machine learning algorithm and evaluate the model's performance using cross-validation to identify the best combination of hyperparameters. Here's how it works:\n",
        "\n",
        "1. Hyperparameter Space: First, you define a grid of hyperparameters and their possible values. This grid represents the space of hyperparameters you want to search through. For example, if you're using a decision tree classifier, you might want to tune hyperparameters like the maximum depth of the tree and the minimum number of samples required to split a node. You would specify a range of values for each of these hyperparameters.\n",
        "\n",
        "2. Model and Data: Next, you choose a machine learning algorithm (e.g., decision tree, support vector machine, random forest) and prepare your dataset for training and evaluation.\n",
        "\n",
        "3. Cross-Validation: GridSearchCV uses k-fold cross-validation to assess the performance of different hyperparameter combinations. It divides your dataset into k subsets (folds) and iteratively trains and evaluates the model k times. In each iteration, it uses k-1 folds for training and the remaining fold for validation. This process helps to ensure that the model's performance estimates are more robust and less sensitive to the specific choice of the training and validation data.\n",
        "\n",
        "4. Evaluation Metric: You specify an evaluation metric (e.g., accuracy, F1-score, mean squared error) that GridSearchCV uses to determine the best combination of hyperparameters. The metric you choose should align with the specific problem you are trying to solve (e.g., classification or regression).\n",
        "\n",
        "5. Search: GridSearchCV then exhaustively searches through all possible combinations of hyperparameters within the predefined grid. It trains and evaluates the model using each combination.\n",
        "\n",
        "6. Best Model Selection: After evaluating all combinations, GridSearchCV selects the combination of hyperparameters that performed the best according to the chosen evaluation metric.\n",
        "\n",
        "7. Final Model: Finally, you can train the model with the selected hyperparameters on the entire dataset (or a separate training set) to obtain the final model for making predictions on new, unseen data.\n",
        "\n",
        "GridSearchCV helps automate and systematize the process of hyperparameter tuning, saving you time and ensuring that you find the most optimal hyperparameters for your model. However, it can be computationally expensive, especially when the hyperparameter space is large, as it requires training and evaluating the model multiple times. To mitigate this, more advanced techniques like RandomizedSearchCV and Bayesian optimization can be used to search for hyperparameters more efficiently.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZiHyqyAetLju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 answer\n",
        "\n",
        "rid Search Cross-Validation (GridSearchCV) and Randomized Search Cross-Validation (RandomizedSearchCV) are both techniques used for hyperparameter tuning in machine learning, but they differ in how they explore the hyperparameter space. Here are the key differences between them and when you might choose one over the other:\n",
        "\n",
        "Grid Search Cross-Validation (GridSearchCV):\n",
        "\n",
        "1. Exhaustive Search: GridSearchCV performs an exhaustive search over a predefined grid of hyperparameters. It considers all possible combinations of hyperparameters within the specified ranges or values.\n",
        "\n",
        "2. Deterministic: Grid search explores the hyperparameter space systematically and deterministically, meaning it tries every combination in the grid.\n",
        "\n",
        "3. Computational Cost: Grid search can be computationally expensive, especially when the hyperparameter space is large, as it evaluates every combination. This can lead to longer training times.\n",
        "\n",
        "4. Use Cases: GridSearchCV is a good choice when you have a reasonable understanding of the hyperparameter space, and you want to ensure that you've explored all possible combinations thoroughly. It's suitable for smaller hyperparameter spaces.\n",
        "\n",
        "Randomized Search Cross-Validation (RandomizedSearchCV):\n",
        "\n",
        "1. Random Sampling: RandomizedSearchCV, as the name suggests, samples hyperparameters randomly from predefined distributions. Instead of considering all possible combinations, it selects a random subset of hyperparameters to evaluate.\n",
        "\n",
        "2. Efficiency: Randomized search is more efficient in terms of computation because it doesn't exhaustively search the entire hyperparameter space. It provides a good balance between exploration and exploitation of the space.\n",
        "\n",
        "3. Flexibility: It allows you to specify probability distributions for each hyperparameter, which gives you more flexibility in defining the search space. This can be especially useful when you have limited computational resources.\n",
        "\n",
        "4. Use Cases: RandomizedSearchCV is a better choice when the hyperparameter space is vast, and you have limited computational resources or time. It can help you quickly identify promising regions of the hyperparameter space without evaluating all possible combinations.\n",
        "\n",
        "When to Choose One Over the Other:\n",
        "\n",
        "1. Grid Search vs. Randomized Search: If you have ample computational resources and a relatively small hyperparameter space, GridSearchCV can be a reasonable choice, as it guarantees that you'll explore all combinations. However, if your hyperparameter space is large or you have limited resources, RandomizedSearchCV is a more efficient option.\n",
        "\n",
        "2. Exploration vs. Exploitation: If you want to explore the entire hyperparameter space thoroughly to find the absolute best combination, GridSearchCV may be preferred. However, if you're looking for a good set of hyperparameters and are willing to accept a slightly suboptimal solution to save time, RandomizedSearchCV is more efficient in balancing exploration and exploitation.\n",
        "\n",
        "3. Complexity: Consider the complexity of your model and the cost of training. If your model is simple and quick to train, GridSearchCV may be feasible. For complex models with lengthy training times, RandomizedSearchCV can save a significant amount of time.\n"
      ],
      "metadata": {
        "id": "KMB17lEatglT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 answer\n",
        "\n",
        "Data leakage, also known as leakage or data snooping, is a critical issue in machine learning where information from outside the training dataset is unintentionally used to train a model or make predictions. Data leakage can lead to overly optimistic performance estimates, making a model appear better than it actually is, and can result in poor generalization to new, unseen data. It can occur at various stages of the machine learning pipeline, including during data preprocessing, feature engineering, or model evaluation.\n",
        "\n",
        "Data leakage is a problem in machine learning for several reasons:\n",
        "\n",
        "1. Biased Model Evaluation: Leakage can lead to overly optimistic model evaluation results because the model has seen information it should not have during training or evaluation. This can result in the selection of suboptimal models or hyperparameters.\n",
        "\n",
        "2. Ineffective Generalization: Models trained with leaked information may perform well on the training and validation data but generalize poorly to new, unseen data, as they have learned patterns that do not hold outside the dataset.\n",
        "\n",
        "3. Unrealistic Expectations: Data leakage can create unrealistic expectations about a model's performance in real-world applications, leading to disappointment when the model underperforms in practice.\n",
        "\n",
        "Here's an example of data leakage in Python:\n",
        "\n",
        "Suppose you are building a binary classification model to predict whether a customer will default on a loan. You have a dataset with features like income, credit score, and employment status. Additionally, you have a column called \"has_defaulted\" that indicates whether a customer has defaulted on a previous loan.\n"
      ],
      "metadata": {
        "id": "3zQe1gpyuDjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data={\n",
        "    'income':[50000,60000,30000,80000,70000],\n",
        "    'credit_score':[650,720,600,750,700],\n",
        "    'employment_status':['Employed','Employed','Unemployed', 'Employed', 'Employed'],\n",
        "    'has_defaulted':[0,0,1,0,1]\n",
        "\n",
        "}\n",
        "\n",
        "df=pd.DataFrame(data)\n",
        "X = df[['income', 'credit_score', 'employment_status', 'has_defaulted']]\n",
        "y = df['has_defaulted']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model=LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred=model.predict(X_test)\n",
        "accuracy=accuracy_score(y_test,y_pred)\n",
        "print(f'Accuracy:{accuracy:.2f}')"
      ],
      "metadata": {
        "id": "z7ehX424tgB_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 answer\n",
        "Preventing data leakage in Python when building a machine learning model involves implementing best practices and being cautious at various stages of your workflow. Here's a step-by-step guide using Python and common libraries like scikit-learn to help prevent data leakage:\n",
        "\n",
        "1. Data Splitting:\n",
        "Use proper data splitting techniques to separate your data into training, validation, and test sets. The train_test_split function from scikit-learn is helpful for this task. Ensure that no information from the validation or test set is used during model development.\n"
      ],
      "metadata": {
        "id": "QEil1mEyxJHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "CRXS71eKxhpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Feature Engineering:\n",
        "\n",
        "Be cautious when engineering new features. Only use information that would be available at the time of prediction. Avoid using features that are derived from the target variable or that leak information from the future.\n",
        "2. Feature Scaling:\n",
        "\n",
        "Scale or standardize features based on statistics computed from the training data. Use the StandardScaler from scikit-learn, and make sure you fit it only on the training data and transform both training and validation data consistently."
      ],
      "metadata": {
        "id": "SYltM3n1xjuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n"
      ],
      "metadata": {
        "id": "Mwp5kh5nxoNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Cross-Validation:\n",
        "Use cross-validation to evaluate your model. Scikit-learn's cross_val_score or cross_val_predict can help. Ensure that data splitting within each fold is done correctly."
      ],
      "metadata": {
        "id": "_4DsWUw6xp7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Example of cross-validation with a classifier (e.g., RandomForestClassifier)\n",
        "scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n"
      ],
      "metadata": {
        "id": "gOFflkM7xvZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Feature Selection:\n",
        "If you perform feature selection, do it based solely on the training data. You can use methods like feature importance from tree-based models or scikit-learn's SelectKBest for statistical tests."
      ],
      "metadata": {
        "id": "LeAVnzB9xyAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "selector = SelectKBest(score_func=f_classif, k=5)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_val_selected = selector.transform(X_val)\n"
      ],
      "metadata": {
        "id": "5DIy4qfjx3Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Pipeline and Transformers:\n",
        "Use scikit-learn's Pipeline to encapsulate preprocessing steps. Ensure that transformers in the pipeline are fit only on the training data and applied consistently to the validation data."
      ],
      "metadata": {
        "id": "-fO5ZDH4x5Ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('clf', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_val_pred = pipeline.predict(X_val)\n"
      ],
      "metadata": {
        "id": "fN2OfTQrx-AQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Documentation and Code Review:\n",
        "Clearly document all preprocessing steps in your code and ensure that your code is reviewed by peers to catch potential data leakage issues."
      ],
      "metadata": {
        "id": "2RLAtUBcyERe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 answer\n",
        "\n",
        "A confusion matrix is a fundamental tool in the evaluation of classification models in machine learning. It provides a summary of the performance of a classification model by breaking down the predicted and actual class labels into four categories. It is particularly useful when dealing with binary classification (two classes), but it can be extended to multiclass problems as well.\n",
        "\n",
        "A confusion matrix consists of four key elements:\n",
        "\n",
        "1. True Positives (TP): These are instances that were correctly predicted as positive (belonging to the positive class) by the model.\n",
        "\n",
        "2. True Negatives (TN): These are instances that were correctly predicted as negative (belonging to the negative class) by the model.\n",
        "\n",
        "3. False Positives (FP): Also known as Type I errors or \"false alarms,\" these are instances that were incorrectly predicted as positive when they are actually negative.\n",
        "\n",
        "4. False Negatives (FN): Also known as Type II errors, these are instances that were incorrectly predicted as negative when they are actually positive."
      ],
      "metadata": {
        "id": "JMHy0dlwyHmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "             Actual Positive    Actual Negative\n",
        "Predicted Positive   TP              FP\n",
        "Predicted Negative   FN              TN\n"
      ],
      "metadata": {
        "id": "4Vvt948Zylnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's discuss what a confusion matrix can tell you about the performance of a classification model:\n",
        "\n",
        "1. Accuracy: The diagonal elements of the confusion matrix (TP and TN) represent the correct predictions made by the model. The accuracy of the model is calculated as (TP + TN) / (TP + FP + FN + TN), indicating the proportion of correctly classified instances.\n",
        "\n",
        "2. Precision (Positive Predictive Value): Precision is the ratio of true positives to the total number of instances predicted as positive, i.e., TP / (TP + FP). It measures the model's ability to avoid false positives. A higher precision indicates that the model has a lower rate of false alarms.\n",
        "\n",
        "3. Recall (Sensitivity or True Positive Rate): Recall is the ratio of true positives to the total number of actual positives, i.e., TP / (TP + FN). It measures the model's ability to identify all positive instances. A higher recall indicates that the model captures a larger proportion of actual positives.\n",
        "\n",
        "4. F1-Score: The F1-Score is the harmonic mean of precision and recall, calculated as 2 * (Precision * Recall) / (Precision + Recall). It provides a balanced measure of a model's performance, especially when there is an imbalance between the classes.\n",
        "\n",
        "5. Specificity (True Negative Rate): Specificity is the ratio of true negatives to the total number of actual negatives, i.e., TN / (TN + FP). It measures the model's ability to identify all negative instances.\n",
        "\n",
        "6. False Positive Rate (FPR): FPR is the ratio of false positives to the total number of actual negatives, i.e., FP / (FP + TN). It quantifies the model's propensity to make false alarms.\n",
        "\n",
        "7. Negative Predictive Value (NPV): NPV is the ratio of true negatives to the total number of instances predicted as negative, i.e., TN / (TN + FN). It measures the model's ability to correctly identify negatives."
      ],
      "metadata": {
        "id": "Az8TMMhbyqgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 answer\n",
        "\n",
        "Precision and recall are two important metrics used in the context of a confusion matrix to evaluate the performance of a classification model, especially in scenarios where class imbalance is present. They provide insights into different aspects of a model's performance:\n",
        "\n",
        "1. Precision:\n",
        "\n",
        "Precision is a metric that measures the proportion of true positive predictions (correctly predicted positive instances) among all instances predicted as positive by the model. In other words, it answers the question: \"Of all the instances the model predicted as positive, how many were actually positive?\"\n",
        "\n",
        "Precision focuses on the model's ability to avoid making false positive errors. A high precision value indicates that the model is conservative in its positive predictions, making fewer false alarms. It's useful in situations where false positives are costly or undesirable, such as medical diagnoses or spam email classification.\n",
        "\n",
        "2. Recall (Sensitivity or True Positive Rate):\n",
        "\n",
        "Recall, also known as sensitivity or the true positive rate, is a metric that measures the proportion of true positive predictions (correctly predicted positive instances) among all actual positive instances in the dataset. In other words, it answers the question: \"Of all the actual positive instances, how many did the model correctly identify?\"\n",
        "\n",
        "Recall focuses on the model's ability to capture as many actual positive instances as possible, minimizing false negatives. A high recall value indicates that the model is good at identifying positive instances, which is important in scenarios where missing positive cases can have severe consequences, such as disease detection or fraud detection.\n",
        "\n",
        "In summary, the key difference between precision and recall is in their emphasis:\n",
        "\n",
        "Precision emphasizes the accuracy of positive predictions and measures the model's ability to avoid false positives. It is concerned with the quality of positive predictions.\n",
        "\n",
        "Recall emphasizes the completeness of positive predictions and measures the model's ability to identify all actual positive instances. It is concerned with the quantity of positive predictions."
      ],
      "metadata": {
        "id": "ECHF_Th1y1rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision is calculated as:\n",
        "Precision = True Positives (TP) / (True Positives (TP) + False Positives (FP))\n",
        "# Recall is calculated as:\n",
        "Recall = True Positives (TP) / (True Positives (TP) + False Negatives (FN))\n"
      ],
      "metadata": {
        "id": "92f_X_pXzNNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 answer\n",
        "\n",
        "True Positives (TP): These are instances that were correctly predicted as positive by the model. In a binary classification context, these are the cases where the model correctly identified positive instances.\n",
        "\n",
        "True Negatives (TN): These are instances that were correctly predicted as negative by the model. These are cases where the model correctly identified negative instances.\n",
        "\n",
        "False Positives (FP): Also known as Type I errors or \"false alarms,\" these are instances that were incorrectly predicted as positive when they are actually negative. In other words, the model made a positive prediction, but it was incorrect.\n",
        "\n",
        "False Negatives (FN): Also known as Type II errors, these are instances that were incorrectly predicted as negative when they are actually positive. In other words, the model made a negative prediction, but it was incorrect.\n",
        "\n",
        "Now, let's interpret these types of errors:\n",
        "\n",
        "1. False Positives (FP):\n",
        "\n",
        "These are instances where the model predicted the positive class, but they were actually negative. In some cases, false positives can be problematic, especially if the consequences of false alarms are significant.\n",
        "Example: In a medical diagnosis model, a false positive could lead to unnecessary medical procedures or treatments.\n",
        "2. False Negatives (FN):\n",
        "\n",
        "These are instances where the model predicted the negative class, but they were actually positive. False negatives can also have serious consequences, particularly when missing positive cases is costly.\n",
        "Example: In a disease detection model, a false negative could result in a missed diagnosis and delayed treatment.\n",
        "3. True Positives (TP):\n",
        "\n",
        "These are instances where the model correctly predicted the positive class, and they were indeed positive. True positives represent the successful positive predictions by the model.\n",
        "4. True Negatives (TN):\n",
        "\n",
        "These are instances where the model correctly predicted the negative class, and they were indeed negative. True negatives represent the successful negative predictions by the model.\n",
        "Analyzing these errors allows you to assess the strengths and weaknesses of your model:\n",
        "\n",
        "If you have a high number of false positives, your model may be too liberal in predicting the positive class. You might want to focus on improving precision.\n",
        "\n",
        "If you have a high number of false negatives, your model may be missing positive cases. In such cases, improving recall might be a priority.\n",
        "\n",
        "If you have high numbers of both false positives and false negatives, you may need to strike a balance between precision and recall, possibly by adjusting the decision threshold of your model."
      ],
      "metadata": {
        "id": "yRB2Al6KzcWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
        "y_pred = [1, 1, 1, 0, 0, 1, 0, 1, 0, 1]\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "[[True Negatives  False Positives]\n",
        " [False Negatives True Positives]]\n",
        "\n",
        "tp = conf_matrix[1, 1]\n",
        "print(f\"True Positives: {tp}\")\n",
        "\n",
        "tn = conf_matrix[0, 0]\n",
        "print(f\"True Negatives: {tn}\")\n",
        "\n",
        "fp = conf_matrix[0, 1]\n",
        "print(f\"False Positives: {fp}\")\n",
        "\n",
        "fn = conf_matrix[1, 0]\n",
        "print(f\"False Negatives: {fn}\")\n",
        "\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "precision = tp / (tp + fp)\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "\n",
        "recall = tp / (tp + fn)\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "print(f\"F1-Score: {f1_score:.2f}\")\n"
      ],
      "metadata": {
        "id": "gx6idMoLyG-N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 answer\n",
        "\n",
        "Common metrics that can be derived from a confusion matrix in the context of a binary classification problem (two classes, typically labeled as \"positive\" and \"negative\") include the following:\n",
        "\n",
        "1. Accuracy (ACC):\n",
        "\n",
        "Accuracy measures the overall correctness of a model's predictions.\n",
        "Formula: (TP + TN) / (TP + TN + FP + FN)\n",
        "2. Precision (Positive Predictive Value):\n",
        "\n",
        "Precision measures the accuracy of positive predictions made by the model.\n",
        "Formula: TP / (TP + FP)\n",
        "3. Recall (Sensitivity or True Positive Rate):\n",
        "\n",
        "Recall measures the ability of the model to capture positive instances.\n",
        "Formula: TP / (TP + FN)\n",
        "4. F1-Score:\n",
        "\n",
        "The F1-Score is the harmonic mean of precision and recall, providing a balanced measure of a model's performance.\n",
        "Formula: 2 * (Precision * Recall) / (Precision + Recall)\n",
        "5. Specificity (True Negative Rate):\n",
        "\n",
        "Specificity measures the ability of the model to identify negative instances.\n",
        "Formula: TN / (TN + FP)\n",
        "6. False Positive Rate (FPR):\n",
        "\n",
        "FPR measures the model's propensity to make false alarms.\n",
        "Formula: FP / (FP + TN)\n",
        "7. Negative Predictive Value (NPV):\n",
        "\n",
        "NPV measures the accuracy of negative predictions made by the model.\n",
        "Formula: TN / (TN + FN)\n",
        "8. True Negative Rate (TNR) or Specificity:\n",
        "\n",
        "TNR or specificity is a measure of the model's ability to correctly identify negative instances.\n",
        "Formula: TN / (TN + FP)\n",
        "9. False Negative Rate (FNR):\n",
        "\n",
        "FNR measures the rate at which the model misses actual positive instances.\n",
        "Formula: FN / (FN + TP)\n",
        "10. False Discovery Rate (FDR):\n",
        "\n",
        "FDR measures the rate at which the model makes false positive predictions among all positive predictions.\n",
        "Formula: FP / (FP + TP)\n",
        "These metrics provide a comprehensive understanding of a classification model's performance, each focusing on different aspects of correctness, completeness, and quality of predictions. Depending on the specific problem and its requirements, you may prioritize one metric over another. For instance:\n",
        "\n",
        "Precision: Use when minimizing false positives is crucial (e.g., spam email detection).\n",
        "Recall: Use when capturing all positive instances is more important, even if it results in some false positives (e.g., disease detection).\n",
        "F1-Score: Use when you want a balance between precision and recall.\n",
        "Accuracy: Use when you want an overall measure of correctness, but be cautious when dealing with imbalanced datasets."
      ],
      "metadata": {
        "id": "7_MR1ixt0Uz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9 answer\n",
        "\n",
        "The accuracy of a classification model is related to the values in its confusion matrix, specifically to the true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) that make up the matrix. The accuracy metric quantifies the overall correctness of a model's predictions and can be calculated using the following formula:\n",
        "\n",
        "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "Here's how the confusion matrix components contribute to accuracy:\n",
        "\n",
        "True Positives (TP): These are cases where both the true labels and the model's predictions are positive. TP contributes positively to accuracy because they represent correct positive predictions.\n",
        "\n",
        "True Negatives (TN): These are cases where both the true labels and the model's predictions are negative. TN also contributes positively to accuracy because they represent correct negative predictions.\n",
        "\n",
        "False Positives (FP): FP represents cases where the model predicts positive when the true label is negative. FP reduces accuracy because they represent incorrect positive predictions.\n",
        "\n",
        "False Negatives (FN): FN represents cases where the model predicts negative when the true label is positive. FN also reduces accuracy because they represent incorrect negative predictions."
      ],
      "metadata": {
        "id": "F2lCk4iB06Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
        "y_pred = [1, 1, 1, 0, 0, 1, 0, 1, 0, 1]\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j8KS5AC1CcU",
        "outputId": "70a14eb8-7406-46ba-8974-8d3dce9d1ea7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[2 3]\n",
            " [2 3]]\n",
            "Accuracy: 0.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10 answer\n",
        "\n",
        "1. Class Imbalance: Check if there is a significant class imbalance by comparing the number of true positives (TP) and true negatives (TN) to false positives (FP) and false negatives (FN). If one class dominates the other, it may indicate an imbalance issue.\n",
        "\n",
        "2. Misclassification Patterns: Examine the distribution of false positives and false negatives across classes. Are there specific classes that your model tends to misclassify more often? This could indicate biases or limitations related to certain classes.\n",
        "\n",
        "3. Performance Discrepancy: Look for performance differences between classes. For example, if your model performs well on one class (high TP and TN) but poorly on another (high FP or FN), it may reveal biases or limitations.\n",
        "\n",
        "4. Threshold Analysis: Consider the impact of the decision threshold on your model's performance. By adjusting the threshold, you can trade off precision and recall. Analyze how changing the threshold affects the confusion matrix and the model's performance on different classes.\n",
        "\n",
        "5. False Positives and False Negatives: Identify which type of error (false positives or false negatives) is more concerning for your application. False positives may lead to false alarms, while false negatives may lead to missed opportunities or risks.\n",
        "\n",
        "6. Business Implications: Consider the business or application context. Some classes may have higher costs associated with errors (e.g., medical diagnosis or fraud detection). Evaluate whether the model's errors align with the priorities and constraints of the problem.\n",
        "\n",
        "7. Data Collection and Labeling: Examine potential biases in the training data or labeling process. Biases in the data can propagate into the model's predictions. Investigate whether there are systematic biases related to specific classes.\n",
        "\n",
        "8. Fairness and Ethical Considerations: Assess whether the model's performance is equitable across different demographic or sensitive groups. Evaluate the fairness and ethical implications of the model's predictions."
      ],
      "metadata": {
        "id": "43ZtpBxu1LxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
        "y_pred = [1, 1, 1, 0, 0, 1, 0, 1, 0, 1]\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovw_Waqu1mVJ",
        "outputId": "376b5ca9-745a-4188-d5ed-906fbcad09d5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[2 3]\n",
            " [2 3]]\n"
          ]
        }
      ]
    }
  ]
}