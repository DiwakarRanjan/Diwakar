{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1 answer\n",
        "Min-Max scaling, also known as Min-Max normalization or feature scaling, is a data preprocessing technique used to transform numerical features to a specific range, typically between 0 and\n",
        "\n",
        "1. This scaling method is useful when you want to ensure that all features contribute equally to the modeling process and that they are on a similar scale, preventing some features from dominating others due to their larger magnitudes."
      ],
      "metadata": {
        "id": "HL3wk5OBQ1Ye"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7BKtNjaOgF0",
        "outputId": "8e219cf1-5692-4d4e-c660-a2619f719fda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orignal Data:\n",
            "[[ 2.   5. ]\n",
            " [ 1.  10. ]\n",
            " [ 0.5  8. ]]\n",
            "\n",
            "Scaled Data:\n",
            "[[1.         0.        ]\n",
            " [0.33333333 1.        ]\n",
            " [0.         0.6       ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "data=np.array([[2.0,5.0],\n",
        "               [1.0,10.0],\n",
        "               [0.5,8.0]])\n",
        "scaler=MinMaxScaler()\n",
        "scaled_data=scaler.fit_transform(data)\n",
        "print(\"Orignal Data:\")\n",
        "print(data)\n",
        "print(\"\\nScaled Data:\")\n",
        "print(scaled_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 answer\n",
        "The Unit Vector technique, also known as \"Normalization,\" is a feature scaling method used to transform numerical features into unit vectors. In this technique, each data point (row) is scaled independently, resulting in a vector of unit length (magnitude equal to 1). The purpose of unit vector scaling is to emphasize the direction of the data points while eliminating the influence of their magnitude. It is particularly useful when the direction of the data points is more important than their actual values."
      ],
      "metadata": {
        "id": "_FTgg18CR9-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import Normalizer\n",
        "data = np.array([[2.0, 5.0],\n",
        "                 [1.0, 10.0],\n",
        "                 [0.0, 8.0]])\n",
        "scaler = Normalizer(norm='l2')\n",
        "scaled_data = scaler.transform(data)\n",
        "\n",
        "print(\"Original Data:\")\n",
        "print(data)\n",
        "print(\"\\nUnit Vector Scaled Data:\")\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W12EN6QvR07U",
        "outputId": "83544419-da29-4dd3-8320-fd0201196f4f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "[[ 2.  5.]\n",
            " [ 1. 10.]\n",
            " [ 0.  8.]]\n",
            "\n",
            "Unit Vector Scaled Data:\n",
            "[[0.37139068 0.92847669]\n",
            " [0.09950372 0.99503719]\n",
            " [0.         1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Differences from Min-Max Scaling:\n",
        "\n",
        "1. Min-Max scaling (MinMaxScaler) scales each feature independently to a specified range (e.g., [0, 1]), preserving the original relationships between data points' magnitudes.\n",
        "2. Unit Vector scaling (Normalizer) scales each feature vector (data point) independently to have a unit length, emphasizing the direction of the data points while disregarding their magnitudes.\n",
        "3. Min-Max scaling is typically used when preserving the original feature magnitudes is important, while Unit Vector scaling is used when the direction of the data points matters more than their absolute values."
      ],
      "metadata": {
        "id": "q_nC_MmYTNhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 answer\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique used to simplify the complexity in high-dimensional data while retaining trends and patterns. It does this by transforming the original features into a new set of uncorrelated features called principal components. These principal components are ordered by the amount of variance they explain, with the first principal component explaining the most variance in the data.\n",
        "\n",
        "PCA is primarily used for the following purposes:\n",
        "1. Dimensionality Reduction: PCA reduces the number of features in a dataset while preserving as much of the original variance as possible. This helps in reducing computational complexity, alleviating the curse of dimensionality, and improving the efficiency of machine learning algorithms.\n",
        "\n",
        "2. Noise Reduction: By focusing on the principal components that capture the most significant variations in the data, PCA can effectively filter out noise and redundant information from the dataset.\n",
        "\n",
        "3. Visualization: PCA can be used for data visualization by projecting high-dimensional data into a lower-dimensional space (e.g., 2D or 3D) for better understanding and visualization of data patterns.\n",
        "\n",
        "Here's a step-by-step explanation of how PCA is used for dimensionality reduction:\n",
        "\n",
        "1. Standardize the Data: If the features in your dataset are measured on different scales, it's important to standardize them (mean centering and scaling to unit variance) to ensure that PCA isn't biased toward features with larger variances.\n",
        "\n",
        "2. Calculate the Covariance Matrix: Compute the covariance matrix of the standardized data. The covariance matrix summarizes how features are related to each other.\n",
        "\n",
        "3. Calculate Eigenvectors and Eigenvalues: Find the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions of maximum variance (principal components), while eigenvalues indicate the amount of variance explained by each principal component.\n",
        "\n",
        "4. Select Principal Components: Sort the eigenvalues in descending order and select the top\n",
        "k eigenvectors (principal components) that correspond to the\n",
        "k largest eigenvalues. Typically, you choose a value of\n",
        "k based on the desired level of dimensionality reduction.\n",
        "\n",
        "5. Project Data onto Principal Components: Transform the original data by projecting it onto the selected principal components. This is done by multiplying the standardized data by the matrix of selected eigenvectors."
      ],
      "metadata": {
        "id": "bVFQREFNTVkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "data=np.array([[1.0,2.0,3.0],\n",
        "               [4.0,5.0,6.0],\n",
        "               [7.0,8.0,9.0],\n",
        "               [10.0,11.0,12.0]])\n",
        "mean=np.mean(data,axis=0)\n",
        "std_dev=np.std(data,axis=0)\n",
        "standardized_data=(data-mean)/std_dev\n",
        "pca=PCA(n_components=2)\n",
        "reduced_data=pca.fit_transform(standardized_data)\n",
        "print(\"orignal Data\")\n",
        "print(data)\n",
        "print(\"\\nReduced Data:\")\n",
        "print(reduced_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atLdgyRXS5xq",
        "outputId": "7b516599-00fe-4f9a-8059-991a688a6a7e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orignal Data\n",
            "[[ 1.  2.  3.]\n",
            " [ 4.  5.  6.]\n",
            " [ 7.  8.  9.]\n",
            " [10. 11. 12.]]\n",
            "\n",
            "Reduced Data:\n",
            "[[-2.32379001e+00  2.83777291e-16]\n",
            " [-7.74596669e-01 -5.26712619e-17]\n",
            " [ 7.74596669e-01  5.26712619e-17]\n",
            " [ 2.32379001e+00  2.48663116e-16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 answer\n",
        "\n",
        "PCA (Principal Component Analysis) can be used as a feature extraction technique in machine learning. Feature extraction refers to the process of transforming the original features of a dataset into a new set of features that captures the most important information while reducing dimensionality. PCA achieves feature extraction by converting the original features into a set of uncorrelated features called principal components. These principal components are linear combinations of the original features and can be considered as new features.\n",
        "\n",
        "\n",
        "1. Dimensionality Reduction: PCA is often used for dimensionality reduction, which is a specific form of feature extraction. It reduces the number of features in a dataset while retaining the most important information. This is done by selecting a subset of the principal components that capture the most variance in the data.\n",
        "\n",
        "2. Uncorrelated Features: One of the key characteristics of PCA is that the principal components are uncorrelated with each other. This means that they capture different aspects of the data's variation and are not redundant. Uncorrelated features can be desirable in various machine learning tasks.\n",
        "\n",
        "3. Variance Explained: PCA ranks the principal components in order of the amount of variance they explain in the original data. The first principal component explains the most variance, the second explains the second-most variance, and so on. By selecting a subset of these components, you can focus on the most significant sources of variation in the data.\n",
        "\n",
        "example:-"
      ],
      "metadata": {
        "id": "bg4_jCPQWQPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import KernelPCA\n",
        "data=np.array([[1.0,2.0,3.0,4.0],\n",
        "               [4.0,5.0,6.0,7.0],\n",
        "               [7.0,8.0,9.0,10.0],\n",
        "               [10.0,11.0,12.0,13.0],\n",
        "               [13.0,14.0,15.0,16.0]])\n",
        "mean=np.mean(data,axis=0)\n",
        "std_dev=np.std(data,axis=0)\n",
        "standardized_data=(data-mean)/std_dev\n",
        "pca=PCA(n_components=2)\n",
        "extracted_features=pca.fit_transform(standardized_data)\n",
        "print(\"Orignal Data:\")\n",
        "print(data)\n",
        "print(\"\\nExtracted Features:\")\n",
        "print(extracted_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e38-DgvVjWB",
        "outputId": "6e4b3951-b501-4a0a-b3ed-67896958fdb4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orignal Data:\n",
            "[[ 1.  2.  3.  4.]\n",
            " [ 4.  5.  6.  7.]\n",
            " [ 7.  8.  9. 10.]\n",
            " [10. 11. 12. 13.]\n",
            " [13. 14. 15. 16.]]\n",
            "\n",
            "Extracted Features:\n",
            "[[-2.82842712e+00  0.00000000e+00]\n",
            " [-1.41421356e+00  0.00000000e+00]\n",
            " [ 8.88178420e-17  0.00000000e+00]\n",
            " [ 1.41421356e+00 -0.00000000e+00]\n",
            " [ 2.82842712e+00 -0.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 answer\n",
        "To preprocess the features in your dataset for building a recommendation system for a food delivery service, you can use Min-Max scaling to standardize and normalize the numerical features like price, rating, and delivery time. Min-Max scaling will transform these features to a common scale, typically between 0 and 1, making them suitable for modeling and ensuring that no single feature dominates others due to differences in their scales. Here's how you can use Min-Max scaling to preprocess the data:\n",
        "\n",
        "1. Load and Prepare the Data:\n",
        "\n",
        "Load the dataset containing features such as price, rating, and delivery time.\n",
        "Perform any necessary data cleaning and preprocessing steps, such as handling missing values or encoding categorical features.\n",
        "2. Select the Numerical Features:\n",
        "\n",
        "Identify the numerical features that require Min-Max scaling. In your case, price, rating, and delivery time are numerical features.\n",
        "3. Apply Min-Max Scaling:\n",
        "\n",
        "Use the Min-Max scaling formula to scale each numerical feature to a range between 0 and 1\n",
        "\n",
        "4. Implement Min-Max Scaling in Python:\n",
        "\n",
        "Here's an example of how to perform Min-Max scaling using Python and the scikit-learn library:\n",
        "\n",
        "5. Use the Scaled Data for Modeling:\n",
        "\n",
        "After applying Min-Max scaling, you can use the scaled data as input for building your recommendation system. The scaled features are now on a consistent scale and can be fed into machine learning algorithms or recommendation algorithms for modeling and making personalized recommendations."
      ],
      "metadata": {
        "id": "bjJwlSL_X380"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement Min-Max Scaling in Python:\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "data=np.array([[10.0,45,30.0],\n",
        "               [20.0,4.0,45.0],\n",
        "               [15.0,4.8,25.0]])\n",
        "scaler=MinMaxScaler()\n",
        "scaled_data=scaler.fit_transform(data)\n",
        "print(\"orignal Data\")\n",
        "print(data)\n",
        "print(\"\\nScaled data (after Min-Max scaling):\")\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfIpmHcmXtI0",
        "outputId": "a4fff5cc-e783-4524-aab7-e0d474c0d8c4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orignal Data\n",
            "[[10.  45.  30. ]\n",
            " [20.   4.  45. ]\n",
            " [15.   4.8 25. ]]\n",
            "\n",
            "Scaled data (after Min-Max scaling):\n",
            "[[0.        1.        0.25     ]\n",
            " [1.        0.        1.       ]\n",
            " [0.5       0.0195122 0.       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 answer\n",
        "\n",
        "Using Principal Component Analysis (PCA) for dimensionality reduction in a project to predict stock prices is a common practice to handle high-dimensional datasets and potentially improve the efficiency and interpretability of your predictive model. Here's how you can use PCA to reduce the dimensionality of the dataset:\n",
        "\n",
        "1. Data Preprocessing:\n",
        "\n",
        "Begin by loading and preprocessing your dataset. This may involve handling missing values, encoding categorical variables, and scaling or normalizing numerical features as needed. In the context of stock price prediction, your dataset likely contains features related to company financial data and market trends.\n",
        "2. Standardize the Data:\n",
        "\n",
        "Standardize the numerical features in your dataset by subtracting the mean and dividing by the standard deviation. Standardization ensures that all features are on a similar scale and prevents features with larger magnitudes from dominating the PCA analysis.\n",
        "\n",
        "3. PCA Implementation:\n",
        "\n",
        "Create a PCA object in Python using libraries such as scikit-learn. You will need to specify the number of principal components (PCs) you want to retain after dimensionality reduction. The choice of the number of PCs depends on your specific goals and the desired level of dimensionality reduction.\n",
        "\n",
        "4. Fit and Transform Data:\n",
        "\n",
        "Fit the PCA model to the standardized dataset and then transform the data into the reduced-dimensional space using the fit_transform method.\n",
        "\n",
        "5. Explained Variance:\n",
        "\n",
        "After applying PCA, you can access the explained variance ratio of each principal component. This ratio tells you the proportion of total variance in the original data that is explained by each PC. It helps you understand how much information is retained in the reduced-dimensional space.\n",
        "\n",
        "6. Select the Number of Components:\n",
        "\n",
        "Analyze the explained variance ratios to determine how many principal components to retain. You can choose a threshold (e.g., 95% of variance explained) or specify a fixed number of components based on your requirements.\n",
        "7. Reconstruction (Optional):\n",
        "\n",
        "If necessary, you can also perform reconstruction of the data back to the original feature space using the inverse transformation. This allows you to see what the reduced-dimensional data looks like in the original feature space.\n",
        "\n",
        "8. Modeling and Evaluation:\n",
        "\n",
        "Finally, you can use the reduced-dimensional data as input for your stock price prediction model. The reduced feature set can potentially improve model training efficiency and reduce the risk of overfitting, especially if your original dataset had a high dimensionality.\n",
        "9. Hyperparameter Tuning (Optional):\n",
        "\n",
        "You can also experiment with different numbers of principal components and evaluate their impact on the predictive performance of your model to find the optimal balance between dimensionality reduction and model accuracy."
      ],
      "metadata": {
        "id": "0kZnbHnhZPI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca=PCA(n_components=5)\n",
        "pca.fit(standardized_data)\n",
        "reduced_data=pca.transform(standardized_data)\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "reconstructed_data = pca.inverse_transform(reduced_data)"
      ],
      "metadata": {
        "id": "ba1DWjtXZNTS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 answer\n",
        "To perform Min-Max scaling to transform the values in your dataset to a range of -1 to 1, you need to calculate the scaling for each value using the Min-Max scaling formula and then apply it individually to each data point. Here's how you can do it:\n",
        "\n",
        "1. Define the Min-Max Scaling Formula:\n",
        "2. Apply Min-Max Scaling:"
      ],
      "metadata": {
        "id": "qd7iGc3taoM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "data=np.array([8,16,24,32,40])\n",
        "min_val=-1\n",
        "max_val=1\n",
        "scaled_data=min_val+(max_val-min_val)*((data - min(data)) / (max(data) - min(data)))\n",
        "print (\"Orignal Data:\")\n",
        "print(data)\n",
        "print(\"\\nScaled Data(-1 ti 1):\")\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5WUrWcfagzc",
        "outputId": "eb1ed8d0-6c94-4496-b9fa-67ef07c38d31"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orignal Data:\n",
            "[ 8 16 24 32 40]\n",
            "\n",
            "Scaled Data(-1 ti 1):\n",
            "[-1.  -0.5  0.   0.5  1. ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 answer\n",
        "The decision of how many principal components to retain in a PCA analysis depends on several factors, including the goals of your analysis, the explained variance, and the trade-off between dimensionality reduction and information retention. Here's a general approach to decide how many principal components to retain for your dataset containing the features: height, weight, age, gender, and blood pressure:\n",
        "\n",
        "1. Standardize the Data:\n",
        "\n",
        "Begin by standardizing the numerical features in your dataset (height, weight, age, and blood pressure). This ensures that all features are on the same scale, which is a prerequisite for PCA.\n",
        "2. Apply PCA:\n",
        "\n",
        "Perform PCA on the standardized dataset.\n",
        "3. Analyze Explained Variance:\n",
        "\n",
        "After applying PCA, examine the explained variance ratio for each principal component. The explained variance ratio indicates the proportion of the total variance in the original data that is explained by each principal component.\n",
        "4. Select the Number of Principal Components:\n",
        "\n",
        "Decide how many principal components to retain based on your specific objectives and the explained variance.\n",
        "One common criterion is to retain a sufficient number of principal components to explain a high percentage of the total variance. For example, you might aim to retain enough components to explain 95% or 99% of the total variance. The choice of this threshold depends on how much variance you're willing to retain in your reduced-dimensional representation.\n",
        "Another approach is to use a scree plot, which shows the explained variance for each principal component in descending order. You can look for an \"elbow\" point where adding more components doesn't significantly increase the explained variance. The point before the explained variance starts to level off is often chosen as the number of components to retain.\n",
        "Alternatively, you can choose a specific number of components based on your domain knowledge or the trade-off between dimensionality reduction and predictive performance.\n",
        "5. Interpretability and Use Case:\n",
        "\n",
        "Consider the interpretability of the retained principal components. Fewer components are often preferred for interpretability, as it's easier to explain the relationships between variables.\n",
        "6. Modeling Performance:\n",
        "\n",
        "Evaluate the impact of dimensionality reduction on the performance of your machine learning or statistical models. Sometimes, reducing dimensionality too aggressively can lead to a loss of predictive power."
      ],
      "metadata": {
        "id": "nSjy0pr4bpZA"
      }
    }
  ]
}