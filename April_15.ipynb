{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1 answer\n",
        "\n",
        "Step 1: Automated Feature Selection (SelectKBest)\n",
        "Step 2: Numerical Pipeline\n",
        "Step 3: Categorical Pipeline\n",
        "Step 4: Combine Numerical and Categorical Pipelines\n",
        "Step 5: Final Model (Random Forest Classifier)\n",
        "Step 6: Evaluate the Model\n",
        "\n",
        "1. Interpretation of Results:\n",
        "- The p# Interpretation of Results:\n",
        "- The pipeline automates feature selection, handling missing values, and preprocessing for both numerical and categorical features.\n",
        "- It uses SelectKBest to select the top K features based on their importance.\n",
        "- Missing values in numerical columns are imputed with the mean, and then the numerical columns are standardized.\n",
        "- Missing values in categorical columns are imputed with the most frequent value, and then one-hot encoding is applied.\n",
        "- The final model is a Random Forest Classifier.\n",
        "- The accuracy of the model on the test dataset is printed.\n",
        "\n",
        "2. Possible Improvements:\n",
        "- You can tune hyperparameters for the Random Forest Classifier using GridSearchCV or RandomizedSearchCV.\n",
        "- Consider using other feature selection methods, such as recursive feature elimination (RFE), for better feature selection.\n",
        "- Experiment with different imputation strategies and scaling methods.\n",
        "- Explore other classification algorithms and ensemble techniques to see if they improve model performance.\n",
        "- Assess model performance with additional metrics like precision, recall, and F1-score for a more comprehensive evaluation.\n",
        "- Handle class imbalance if present in the target variable using techniques like oversampling or undersampling.ipeline automates feature selection, handling missing values, and preprocessing for both numerical and categorical features.\n",
        "- It uses SelectKBest to select the top K features based on their importance.\n",
        "- Missing values in numerical columns are imputed with the mean, and then the numerical columns are standardized.\n",
        "- Missing values in categorical columns are imputed with the most frequent value, and then one-hot encoding is applied.\n",
        "- The final model is a Random Forest Classifier.\n",
        "- The accuracy of the model on the test dataset is printed.\n",
        "\n"
      ],
      "metadata": {
        "id": "OPOt9rNp2pZz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g1gDGU7d2oqk"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = pd.read_csv(\"/content/dataset.csv\")\n",
        "\n",
        "X = data.drop(\"trestbps\", axis=1)\n",
        "y = data[\"chol\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "numerical_columns = X.select_dtypes(include=[np.number]).columns\n",
        "categorical_columns = X.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "\n",
        "feature_selector = SelectKBest(k='all')  # You can specify the number of top features you want to select\n",
        "\n",
        "\n",
        "numerical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_pipeline, numerical_columns),\n",
        "        ('cat', categorical_pipeline, categorical_columns)\n",
        "    ])\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('feature_selector', feature_selector),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', model)\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred = pipeline.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy on test data: {accuracy}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 answer"
      ],
      "metadata": {
        "id": "Nr125HyU4tkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "lr_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "voting_classifier = VotingClassifier(estimators=[('rf', rf_classifier), ('lr', lr_classifier)], voting='soft')\n",
        "\n",
        "voting_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = voting_classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Voting Classifier: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-n1L9Mxi4u9K",
        "outputId": "8404b9f2-c10b-4a59-c25d-470ac71c1bb1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Voting Classifier: 1.0\n"
          ]
        }
      ]
    }
  ]
}