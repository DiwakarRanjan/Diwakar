{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1 answer\n",
        "\n",
        "An ensemble technique in machine learning refers to a strategy where multiple machine learning models are combined to improve the overall predictive performance and robustness of the system. The idea behind ensemble methods is that by aggregating the predictions or decisions of multiple models, the collective wisdom of these models can often produce more accurate and reliable results than any single model on its own.\n",
        "\n",
        "Ensemble techniques are widely used in machine learning and are particularly effective when dealing with complex and noisy data, as well as for improving generalization and reducing overfitting. Some common ensemble techniques include:\n",
        "\n",
        "1. Voting Ensembles: In this approach, multiple models are trained independently on the same dataset, and their predictions are combined by a voting mechanism. There are two main types:\n",
        "\n",
        "Hard Voting: Each model in the ensemble gets one vote, and the final prediction is the majority vote.\n",
        "Soft Voting: Instead of a simple majority vote, each model provides a probability score for each class, and the final prediction is based on the weighted average of these probabilities.\n",
        "2. Bagging (Bootstrap Aggregating): Bagging involves training multiple instances of the same model on different subsets of the training data. Each model is trained independently, and their predictions are averaged or combined in some way to make the final prediction. The most famous bagging algorithm is the Random Forest.\n",
        "\n",
        "3. Boosting: Boosting is an iterative ensemble technique where multiple weak models (models that perform slightly better than random guessing) are trained sequentially. Each new model in the sequence focuses on the examples that the previous models found difficult, giving more weight to those examples. Popular boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost.\n",
        "\n",
        "Stacking: Stacking involves training multiple diverse models and then using another model (often called a meta-learner or blender) to combine their predictions. The idea is to leverage the strengths of different types of models and have the meta-learner learn how to best combine their outputs.\n",
        "\n",
        "4. Blending: Similar to stacking, blending involves dividing the training data into two parts, where one part is used to train multiple models and the other part is used to train a meta-model. The difference is that blending uses a separate validation set for the meta-model, while stacking uses cross-validation."
      ],
      "metadata": {
        "id": "F8gjofcBOZw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 answer\n",
        "\n",
        "Ensemble techniques are used in machine learning in Python (and in other programming languages) for several important reasons:\n",
        "\n",
        "1. Improved Predictive Performance: One of the primary motivations for using ensemble techniques is to improve the predictive performance of machine learning models. By combining the predictions of multiple models, ensembles can often achieve higher accuracy and better generalization to unseen data compared to single models. This can be crucial for solving complex and challenging machine learning tasks.\n",
        "\n",
        "2. Robustness: Ensembles are more robust to noise and outliers in the data. Individual models may make errors on certain data points, but ensembles can help reduce the impact of these errors by considering a consensus of predictions from multiple models.\n",
        "\n",
        "3. Reduction of Overfitting: Ensemble methods can help reduce overfitting, a common problem in machine learning where a model performs well on the training data but poorly on unseen data. By combining multiple models that may overfit differently, the ensemble often produces a more generalized and less overfit prediction.\n",
        "\n",
        "4. Handling Complex Relationships: For problems with intricate or nonlinear relationships in the data, ensembles can capture these relationships more effectively. Different models within the ensemble may excel at capturing different aspects of the data, and their combined output can provide a more complete representation of the underlying patterns.\n",
        "\n",
        "5. Model Diversity: Ensembles typically involve using different types of models or training multiple instances of the same model with different subsets of data or feature sets. This diversity in modeling approaches allows ensembles to leverage the strengths of various models, which can lead to better overall performance.\n",
        "\n",
        "6. Flexibility and Adaptability: Ensemble methods are flexible and can be applied to a wide range of machine learning algorithms and problem types. They can be used with decision trees, neural networks, support vector machines, and many other algorithms, making them versatile for different applications.\n",
        "\n",
        "7. State-of-the-Art Performance: In many machine learning competitions and real-world applications, ensemble techniques have consistently demonstrated state-of-the-art performance. They are often a go-to approach when seeking the highest possible predictive accuracy.\n"
      ],
      "metadata": {
        "id": "jg46-X1xOuJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 answer\n",
        "\n",
        "Bagging, which stands for Bootstrap Aggregating, is an ensemble machine learning technique used to improve the predictive performance and robustness of models. In bagging, multiple instances of the same base model are trained on different subsets of the training data, and their predictions are then combined to make a final prediction. The key idea is to reduce variance and increase model stability by introducing randomness during the training process.\n",
        "\n",
        "In Python, you can implement bagging using various libraries and frameworks. One of the most popular libraries for implementing bagging is scikit-learn. Here's a high-level overview of how bagging can be used in Python with scikit-learn:\n",
        "\n",
        "\n",
        "1. Import the Necessary Libraries:"
      ],
      "metadata": {
        "id": "VpTurt91PghE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n"
      ],
      "metadata": {
        "id": "7CMNzTrdOtbb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll typically use BaggingClassifier for classification tasks and BaggingRegressor for regression tasks. You also need to specify a base estimator, which is the machine learning model that will be used for each \"bag\" (subset of the data).\n",
        "\n",
        "2. Create the Base Estimator:\n",
        "\n",
        "You need to define the base estimator, which can be any scikit-learn classifier or regressor. For example, you can create a decision tree classifier as the base estimator:"
      ],
      "metadata": {
        "id": "Pc-JY9G4QCwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_estimator = DecisionTreeClassifier()\n"
      ],
      "metadata": {
        "id": "MCjW5FpRQHKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create the Bagging Model:\n",
        "\n",
        "Next, you create the bagging model by specifying the number of bags (base estimators) and other parameters like random sampling options:"
      ],
      "metadata": {
        "id": "PMQyVNblQJ0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bagging_model = BaggingClassifier(base_estimator=base_estimator, n_estimators=10, random_state=42)\n"
      ],
      "metadata": {
        "id": "eQowBO1qQNVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, n_estimators is set to 10, which means you'll train 10 decision tree classifiers on different subsets of the data.\n",
        "\n",
        "4. Train the Bagging Model:\n",
        "\n",
        "Fit the bagging model to your training data:\n"
      ],
      "metadata": {
        "id": "v5WOaQyTQOz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bagging_model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "wBvn36p9QVJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Make Predictions:\n",
        "\n",
        "Once the bagging model is trained, you can use it to make predictions on new or unseen data:"
      ],
      "metadata": {
        "id": "T1Q12-B6QWj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = bagging_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "vi6_r3tXQZUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging helps improve model performance by reducing overfitting and increasing the stability of predictions. It's particularly effective when the base estimator has high variance, such as decision trees. By training multiple trees on different subsets of data and combining their predictions, bagging can lead to a more robust and accurate model."
      ],
      "metadata": {
        "id": "v1zxZq6yQbA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 answer\n",
        "\n",
        "Boosting is another ensemble machine learning technique used to improve the predictive performance of models. Unlike bagging, which trains multiple instances of the same base model independently, boosting trains a sequence of weak learners (models that perform slightly better than random guessing) in a sequential manner. Each new weak learner is trained to correct the errors made by the previous ones, leading to a strong overall model.\n",
        "\n",
        "In Python, you can implement boosting using various libraries and frameworks. One of the most popular libraries for implementing boosting is scikit-learn. Here's a high-level overview of how boosting can be used in Python with scikit-learn:\n",
        "\n",
        "1. Import the Necessary Libraries:"
      ],
      "metadata": {
        "id": "4Y9_noDSQd3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n"
      ],
      "metadata": {
        "id": "vvXXW6P8QrPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll typically use AdaBoostClassifier for classification tasks and AdaBoostRegressor for regression tasks. You also need to specify a base estimator, which is the weak learner that will be used for boosting.\n",
        "\n",
        "2. Create the Base Estimator:\n",
        "\n",
        "You need to define the base estimator, which can be any scikit-learn classifier or regressor. For example, you can create a decision tree classifier as the base estimator:"
      ],
      "metadata": {
        "id": "tOxbN6sGQsyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_estimator = DecisionTreeClassifier(max_depth=1)  # A weak decision tree with a limited depth.\n"
      ],
      "metadata": {
        "id": "HUCll2wTQwib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of a weak learner is essential for boosting, as it should perform slightly better than random guessing but not be too strong.\n",
        "\n",
        "3. Create the Boosting Model:\n",
        "\n",
        "Next, you create the boosting model by specifying the base estimator and other parameters like the number of boosting rounds (iterations):"
      ],
      "metadata": {
        "id": "sHBZ__fmQyT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "boosting_model = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, random_state=42)\n"
      ],
      "metadata": {
        "id": "8MamRGjvQ1rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, n_estimators is set to 50, which means you'll train 50 weak learners in sequence.\n",
        "\n",
        "4. Train the Boosting Model:\n",
        "\n",
        "Fit the boosting model to your training data:"
      ],
      "metadata": {
        "id": "IOVX_oM3Q3KF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "boosting_model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "0vKTyFNiQ7z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Make Predictions:\n",
        "\n",
        "Once the boosting model is trained, you can use it to make predictions on new or unseen data:"
      ],
      "metadata": {
        "id": "wJBejdalQ8_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = boosting_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "EEx2qyJpRAGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting works by giving more weight to the examples that the current weak learner misclassifies. In each boosting round, the model focuses on the examples that are difficult to classify, and the weights of these examples are adjusted. This iterative process continues until a predefined number of rounds is reached or until the model achieves a satisfactory level of performance.\n",
        "\n",
        "Boosting is effective at improving model accuracy and generalization, especially when used with weak learners. It is known for its ability to reduce bias and variance, making it a powerful tool in machine learning."
      ],
      "metadata": {
        "id": "UEu9gsc3RDUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 answer\n",
        "\n",
        "Ensemble techniques offer several benefits in machine learning, making them a valuable tool for improving predictive performance and model robustness. Here are some of the key benefits of using ensemble techniques:\n",
        "\n",
        "1. Improved Predictive Performance: Ensemble techniques often lead to better predictive performance compared to individual models. By combining the predictions of multiple models, ensembles can capture different aspects of the data and reduce errors, resulting in more accurate predictions.\n",
        "\n",
        "2. Reduction of Overfitting: Ensembles can help reduce overfitting, which occurs when a model performs well on the training data but poorly on unseen data. By combining multiple models, each of which may overfit differently, ensembles produce more generalized and less overfit predictions.\n",
        "\n",
        "3. Robustness to Noise and Outliers: Ensembles are typically more robust to noisy data and outliers. Individual models may make errors on noisy or outlier data points, but ensembles can mitigate these errors by aggregating predictions, leading to more stable and reliable results.\n",
        "\n",
        "4. Enhanced Generalization: Ensembles can improve the model's ability to generalize patterns in the data. Different models within the ensemble may capture different relationships or features, allowing the ensemble to generalize better across different subsets of the data.\n",
        "\n",
        "5. Model Diversity: Ensembles encourage model diversity by using different types of models or training multiple instances of the same model with different subsets of data or feature sets. This diversity can lead to more comprehensive coverage of the underlying data distribution.\n",
        "\n",
        "6. Out-of-the-Box Performance: Ensembles often provide strong performance without extensive hyperparameter tuning. They can be a robust choice for practitioners who may not have the time or expertise to fine-tune individual models.\n",
        "\n",
        "7. Adaptability: Ensemble techniques are versatile and can be applied to various machine learning algorithms and problem types, including classification, regression, and ranking. They can complement a wide range of models and are compatible with different learning algorithms.\n",
        "\n",
        "8. State-of-the-Art Results: In many machine learning competitions and real-world applications, ensemble techniques have consistently demonstrated state-of-the-art performance. They are often a go-to approach when seeking the highest possible predictive accuracy.\n",
        "\n",
        "9. Interpretability: Some ensemble methods, such as Random Forests, provide feature importance scores, which can help users understand which features are most influential in making predictions. This can be valuable for feature selection and understanding the underlying data.\n",
        "\n",
        "10. Reduction of Bias: Ensembles can reduce bias by compensating for any inherent biases present in the individual models. By aggregating diverse perspectives, ensembles can provide a more balanced and unbiased prediction.\n",
        "\n",
        "11. Ensemble Size: Ensembles can be scaled by adding more base models to the ensemble, which can lead to further improvements in performance. However, there may be diminishing returns beyond a certain ensemble size."
      ],
      "metadata": {
        "id": "HxqmFBX6REgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 answer\n",
        "\n",
        "Ensemble techniques are powerful tools for improving predictive performance and model robustness, but whether they are always better than individual models in Python (or any other programming language) depends on several factors, and there are situations where using ensemble techniques may not be the best choice. Here are some considerations:\n",
        "\n",
        "1. Data Quality: If your dataset is small or of low quality, ensemble techniques may not always provide significant improvements. Ensembles excel when there is enough diversity in the base models and when each base model can contribute something valuable. If the data is noisy or contains many outliers, ensembles can still help, but they may not completely compensate for fundamental issues with the data.\n",
        "\n",
        "2. Model Selection: The choice of the base model matters. If you select a strong and well-tuned individual model that already performs exceptionally well on your problem, adding an ensemble layer might not lead to substantial improvements and could increase computational complexity unnecessarily.\n",
        "\n",
        "3. Computational Resources: Ensembles can be computationally expensive, especially if you use a large number of base models or if each base model is computationally intensive. In cases where you have limited computational resources, training and maintaining an ensemble might not be feasible.\n",
        "\n",
        "4. Interpretability: Ensembles can make model interpretation more challenging. If interpretability is a critical requirement for your application, using an ensemble of complex models may not be suitable. In such cases, simpler models with clear interpretability might be preferred.\n",
        "\n",
        "5. Training Time: Ensembles typically require more time to train compared to individual models, especially if the ensemble consists of many base models. If you have strict time constraints for model development, you might opt for a faster single model.\n",
        "\n",
        "6. Overfitting Risk: While ensembles can help reduce overfitting in many cases, there's still a risk of overfitting the ensemble itself if not properly tuned. Overfitting can occur when the ensemble memorizes the training data rather than generalizing from it.\n",
        "\n",
        "7. Diminishing Returns: There can be diminishing returns with ensemble size. As you add more base models to the ensemble, the improvement in predictive performance may plateau, and the computational cost may increase significantly.\n",
        "\n",
        "8. Application Specifics: The suitability of ensemble techniques depends on the nature of your specific machine learning problem. Some problems may benefit greatly from ensembles, while others may not see significant gains.\n",
        "\n",
        "In practice, it's essential to perform empirical testing and evaluation to determine whether ensemble techniques are beneficial for your particular problem and dataset. This involves experimenting with both ensemble and individual models, measuring their performance on validation or test data, and considering trade-offs such as computational cost and interpretability."
      ],
      "metadata": {
        "id": "qdbhLsfDRfZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 answer\n",
        "\n",
        "A confidence interval is a range of values that is likely to contain the true population parameter with a certain level of confidence. Bootstrapping is a resampling technique that can be used to estimate confidence intervals for various statistics, such as the mean, median, standard deviation, and more, without relying on strong assumptions about the data distribution.\n",
        "\n",
        "Here's a step-by-step process for calculating a confidence interval using the bootstrap method:\n",
        "\n",
        "1. Collect Your Data:\n",
        "Start with your dataset, which consists of a sample of observations. This sample is used to estimate a statistic (e.g., mean, median) for which you want to calculate a confidence interval.\n",
        "\n",
        "2. Resampling (Bootstrap):\n",
        "Bootstrapping involves creating multiple (often thousands) resamples (bootstrap samples) from your original dataset. Each bootstrap sample is obtained by randomly selecting observations with replacement from the original dataset. These bootstrap samples are the same size as your original dataset but may contain duplicate observations.\n",
        "\n",
        "3. Compute Statistic:\n",
        "For each bootstrap sample, calculate the statistic of interest (e.g., mean, median). This gives you a distribution of the statistic across your bootstrap samples.\n",
        "\n",
        "4. Calculate Percentiles:\n",
        "To create a confidence interval, you typically find the desired percentiles of the distribution of the statistic. The most common choice is the 95% confidence interval, which corresponds to the 2.5th and 97.5th percentiles of the distribution.\n",
        "\n",
        "Lower Bound: Calculate the 2.5th percentile of the distribution of the statistic. This is your lower confidence limit.\n",
        "Upper Bound: Calculate the 97.5th percentile of the distribution of the statistic. This is your upper confidence limit.\n",
        "5. Report the Confidence Interval:\n",
        "Your confidence interval is the range between the lower and upper bounds you calculated in the previous step. You can express this interval as follows:\n",
        "\n",
        "\"With 95% confidence, the true [statistic of interest] lies between [lower bound] and [upper bound].\"\n"
      ],
      "metadata": {
        "id": "kPJEs9NhR9K3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.array([10, 15, 12, 14, 18, 20, 11, 13, 16, 19])\n",
        "\n",
        "num_bootstrap_samples = 1000\n",
        "\n",
        "bootstrap_sample_means = []\n",
        "\n",
        "for _ in range(num_bootstrap_samples):\n",
        "\n",
        "    bootstrap_sample = np.random.choice(data, size=len(data), replace=True)\n",
        "\n",
        "    bootstrap_sample_mean = np.mean(bootstrap_sample)\n",
        "    bootstrap_sample_means.append(bootstrap_sample_mean)\n",
        "\n",
        "lower_bound = np.percentile(bootstrap_sample_means, 2.5)\n",
        "upper_bound = np.percentile(bootstrap_sample_means, 97.5)\n",
        "\n",
        "print(f\"95% Confidence Interval for the Mean: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe-wR7UkSTCj",
        "outputId": "052069d9-0c16-4a15-e648-476951795d9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95% Confidence Interval for the Mean: [12.80, 16.80]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 answer\n",
        "\n",
        "Bootstrap is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly drawing samples (with replacement) from the original dataset. It's particularly useful when you want to make statistical inferences or estimate properties of a population based on a sample. Here are the steps involved in performing bootstrap in Python:\n",
        "\n",
        "1. Collect Your Data:\n",
        "Start with your original dataset, which consists of a sample of observations. This sample is used to estimate a statistic (e.g., mean, median, standard deviation) or make inferences about the population.\n",
        "\n",
        "2. Specify the Number of Bootstrap Samples:\n",
        "Decide how many bootstrap samples you want to generate. A common choice is to create thousands of bootstrap samples, but the number can vary depending on the precision required and the computational resources available.\n",
        "\n",
        "3. Resampling (Bootstrap):\n",
        "For each bootstrap sample, you randomly select observations from your original dataset, with replacement. This means that some observations may appear more than once in a bootstrap sample, while others may not appear at all. The bootstrap samples have the same size as your original dataset.\n",
        "\n",
        "4. Calculate the Statistic:\n",
        "For each bootstrap sample, calculate the statistic of interest (e.g., mean, median, standard deviation, etc.). This gives you a distribution of the statistic across your bootstrap samples.\n",
        "\n",
        "5. Analyze the Bootstrap Distribution:\n",
        "Examine the distribution of the calculated statistic from the bootstrap samples. You can compute summary statistics (e.g., mean, median, standard error) and create visualizations (e.g., histograms, confidence interval plots) to understand the characteristics of the bootstrap distribution.\n",
        "\n",
        "6. Estimate Confidence Intervals:\n",
        "To make inferences or quantify uncertainty about the population parameter, you can calculate confidence intervals. The most common choice is a 95% confidence interval, which corresponds to the 2.5th and 97.5th percentiles of the bootstrap distribution. The lower and upper bounds of the confidence interval provide a range of plausible values for the population parameter.\n",
        "\n"
      ],
      "metadata": {
        "id": "JJJ6SvD0Se2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "dataa=np.array([10,15,12,14,18,20,11,13,16,19])\n",
        "num_bootstrap_samples=1000\n",
        "bootstrap_sample_means=[]\n",
        "for _ in range(num_bootstrap_samples):\n",
        "\n",
        "    bootstrap_sample = np.random.choice(data, size=len(data), replace=True)\n",
        "\n",
        "    bootstrap_sample_mean = np.mean(bootstrap_sample)\n",
        "    bootstrap_sample_means.append(bootstrap_sample_mean)\n",
        "\n",
        "lower_bound = np.percentile(bootstrap_sample_means, 2.5)\n",
        "upper_bound = np.percentile(bootstrap_sample_means, 97.5)\n",
        "\n",
        "print(f\"95% Confidence Interval for the Mean: [{lower_bound:.2f}, {upper_bound:.2f}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7FQDwwYSzSY",
        "outputId": "12fd5273-4445-459e-9762-483ab703248f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95% Confidence Interval for the Mean: [12.80, 16.60]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9 answer\n",
        "\n",
        "To estimate the 95% confidence interval for the population mean height of trees using bootstrap in Python, you can follow these steps:\n",
        "\n",
        "1. Collect the Data: The researcher has measured the height of a sample of 50 trees and obtained a mean height of 15 meters and a standard deviation of 2 meters.\n",
        "\n",
        "2. Specify the Number of Bootstrap Samples: Decide how many bootstrap samples you want to generate. A common choice is to create thousands of bootstrap samples.\n",
        "\n",
        "3. Perform Bootstrap Resampling: For each bootstrap sample, randomly select observations (tree heights) from the original sample with replacement. Each bootstrap sample should have the same size as the original sample (50 in this case).\n",
        "\n",
        "4. Calculate the Mean for Each Bootstrap Sample: Calculate the mean height for each bootstrap sample.\n",
        "\n",
        "5. Analyze the Bootstrap Distribution: Examine the distribution of the bootstrap sample means.\n",
        "\n",
        "6. Calculate the Confidence Interval: Compute the 95% confidence interval by finding the 2.5th and 97.5th percentiles of the bootstrap distribution."
      ],
      "metadata": {
        "id": "jkjPdFAuTmVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "sample_mean =15\n",
        "sample_std_dev =2\n",
        "sample_size =50\n",
        "\n",
        "num_bootstrap_samples =10000\n",
        "\n",
        "bootstrap_sample_means=[]\n",
        "\n",
        "for _ in range(num_bootstrap_samples):\n",
        "\n",
        "    bootstrap_sample = np.random.normal(sample_mean, sample_std_dev, sample_size)\n",
        "\n",
        "    bootstrap_sample_mean = np.mean(bootstrap_sample)\n",
        "    bootstrap_sample_means.append(bootstrap_sample_mean)\n",
        "\n",
        "lower_bound = np.percentile(bootstrap_sample_means, 2.5)\n",
        "upper_bound = np.percentile(bootstrap_sample_means, 97.5)\n",
        "\n",
        "print(f\"95% Confidence Interval for Population Mean Height: [{lower_bound:.2f} meters, {upper_bound:.2f} meters]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFWmrRCDT9cp",
        "outputId": "14564971-e47d-47b3-903f-555df1554ca0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95% Confidence Interval for Population Mean Height: [14.45 meters, 15.56 meters]\n"
          ]
        }
      ]
    }
  ]
}