{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwdwTz5CQuRm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 answer\n",
        "\n",
        "Anomaly detection is a data analysis technique used to identify patterns or instances that deviate significantly from the expected or normal behavior within a dataset. It is commonly used in various fields, including finance, cybersecurity, manufacturing, healthcare, and more, to detect unusual or potentially fraudulent activities, faults in machinery, outliers in data, and other irregularities. The main purpose of anomaly detection in Python or any other programming language is to find and flag data points or events that are unusual or potentially indicative of problems or interesting phenomena.\n",
        "\n",
        "Here are some key aspects of anomaly detection and its purposes in Python:\n",
        "\n",
        "1. Identifying Unusual Patterns: Anomaly detection helps in finding data points or patterns that do not conform to the expected behavior of the system or dataset. This can be achieved through statistical analysis, machine learning, or domain-specific knowledge.\n",
        "\n",
        "2. Applications:\n",
        "\n",
        "Fraud Detection: Detecting fraudulent transactions or activities in banking, credit card transactions, or insurance claims.\n",
        "Network Security: Identifying suspicious network traffic patterns indicative of cyberattacks or intrusion attempts.\n",
        "Manufacturing: Detecting defects or anomalies in manufacturing processes to maintain product quality.\n",
        "Healthcare: Identifying unusual medical readings that may indicate health issues.\n",
        "Environmental Monitoring: Detecting anomalies in environmental sensor data, such as abnormal weather patterns.\n",
        "3. Methods:\n",
        "\n",
        "Statistical Approaches: These methods involve calculating statistics like mean, standard deviation, or percentiles to determine what is normal and flagging data points that fall outside these statistical bounds.\n",
        "Machine Learning: Supervised or unsupervised machine learning algorithms can be used to model the normal behavior of data and detect deviations from it. Common algorithms include Isolation Forest, One-Class SVM, and autoencoders.\n",
        "Time Series Analysis: When dealing with time-series data, techniques like Seasonal Decomposition of Time Series (STL) or ARIMA models can be used for anomaly detection.\n",
        "4. Python Libraries: Python provides a wide range of libraries and tools for implementing anomaly detection, including:\n",
        "\n",
        "Scikit-learn: A machine learning library that includes various algorithms for anomaly detection.\n",
        "PyOD (Python Outlier Detection): A library specifically designed for outlier and anomaly detection, offering multiple algorithms.\n",
        "Statsmodels: A library for statistical modeling and hypothesis testing, useful for traditional statistical approaches.\n",
        "Time Series Libraries: Libraries like Pandas, Statsmodels, and Prophet can be used for time series anomaly detection.\n",
        "5. Evaluation: It's crucial to evaluate the performance of an anomaly detection model using appropriate metrics like precision, recall, F1-score, or area under the ROC curve (AUC) depending on the specific problem and the balance between false positives and false negatives."
      ],
      "metadata": {
        "id": "HWuciPypRffc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 answer\n",
        "\n",
        "Anomaly detection in machine learning poses several key challenges that practitioners need to address to build effective anomaly detection systems. These challenges include:\n",
        "\n",
        "1. Imbalanced Data: Anomalies are typically rare compared to normal data points, leading to class imbalance. This imbalance can affect the performance of machine learning models, as they may be biased towards the majority class. Special techniques, such as oversampling, undersampling, or using different evaluation metrics, may be needed to handle imbalanced data.\n",
        "\n",
        "2. Choosing the Right Algorithm: Selecting an appropriate anomaly detection algorithm can be challenging. Different algorithms work better for specific types of data and anomaly patterns. There is no one-size-fits-all solution, and experimentation is often required to determine the best approach for a particular problem.\n",
        "\n",
        "3. Feature Engineering: Feature selection and engineering play a crucial role in anomaly detection. Identifying relevant features that capture the underlying patterns and anomalies in the data is not always straightforward. Domain knowledge and data exploration are essential for effective feature engineering.\n",
        "\n",
        "4. Labeling Anomalies: In many real-world scenarios, it can be difficult to obtain labeled data for anomalies. Supervised approaches require labeled anomalies for training, but acquiring such data can be expensive and time-consuming. Semi-supervised or unsupervised methods are often preferred when labeled data is scarce.\n",
        "\n",
        "5. Model Interpretability: Understanding why a model flags a particular data point as an anomaly is essential for trust and decision-making. Many anomaly detection algorithms, especially deep learning models, lack interpretability, making it challenging to explain their decisions.\n",
        "\n",
        "6. Adaptability to Data Changes: Anomaly detection models need to adapt to changes in the data distribution over time. What was considered normal in the past may become anomalous in the future. Models should be able to update themselves or be retrained periodically to remain effective.\n",
        "\n",
        "7. Scalability: Anomaly detection algorithms should be scalable to handle large volumes of data efficiently. Processing large datasets can be computationally expensive, and models need to be designed to handle the data's size.\n",
        "\n",
        "8. False Positives: Anomaly detection models may produce false positives, flagging normal data points as anomalies. Minimizing false positives while maximizing true positives is often a delicate balance that needs to be struck, and it depends on the specific problem's requirements.\n",
        "\n",
        "9. Time Series Anomalies: Detecting anomalies in time series data presents unique challenges due to the temporal dependencies and seasonality. Specialized time series anomaly detection algorithms and preprocessing techniques are often required.\n",
        "\n",
        "10. Concept Drift: In dynamic environments, where the data distribution changes over time, models need to adapt to concept drift. This requires continuous monitoring and updating of the anomaly detection system.\n",
        "\n",
        "11. Evaluation Metrics: Choosing appropriate evaluation metrics for anomaly detection can be tricky. Traditional metrics like accuracy are not suitable for imbalanced datasets. Metrics like precision, recall, F1-score, ROC-AUC, and area under the precision-recall curve (AUC-PR) are more relevant.\n",
        "\n",
        "12. Data Quality and Noise: Anomaly detection models can be sensitive to noisy data and outliers that are not genuine anomalies. Data preprocessing and cleaning are crucial to ensure the quality of input data."
      ],
      "metadata": {
        "id": "DpVMXTTbRybM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 answer\n",
        "\n",
        "Unsupervised anomaly detection and supervised anomaly detection are two distinct approaches in machine learning for identifying anomalies, and they differ in several key ways:\n",
        "\n",
        "Unsupervised Anomaly Detection:\n",
        "\n",
        "1. Lack of Labels: In unsupervised anomaly detection, the training data consists of only normal data points without any labels or explicit information about which data points are anomalies. The algorithm's goal is to discover patterns or structures within the data and identify data points that deviate significantly from these patterns.\n",
        "\n",
        "2. No Prior Knowledge: Unsupervised anomaly detection assumes no prior knowledge of the characteristics of anomalies. It doesn't rely on a labeled dataset of anomalies for training.\n",
        "\n",
        "3. Algorithm Types: Unsupervised methods include techniques like clustering-based approaches (e.g., DBSCAN), density estimation (e.g., Gaussian Mixture Models), or dimensionality reduction (e.g., PCA) to capture the normal data distribution. Anomalies are typically identified as data points that fall far from this distribution.\n",
        "\n",
        "4. Applicability: Unsupervised anomaly detection is useful when you have limited or no labeled anomaly data, and you want to discover novel or previously unknown anomalies in your dataset. It's often used for exploratory analysis.\n",
        "\n",
        "5. Scalability: Some unsupervised methods may not scale well to large datasets because they rely on the entire dataset to estimate normal data patterns.\n",
        "\n",
        "Supervised Anomaly Detection:\n",
        "\n",
        "1. Labeled Data: Supervised anomaly detection relies on a labeled dataset where both normal and anomalous data points are explicitly labeled. The algorithm uses this labeled data for training.\n",
        "\n",
        "2. Prior Knowledge: Supervised methods assume that you have prior knowledge of what constitutes an anomaly based on the labeled training data.\n",
        "\n",
        "3. Algorithm Types: In supervised approaches, you typically use classification algorithms such as decision trees, support vector machines (SVM), random forests, or neural networks. The model learns to classify data points as either normal or anomalous based on the provided labels.\n",
        "\n",
        "4. Applicability: Supervised anomaly detection is suitable when you have a sufficient amount of labeled anomaly data and you want to build a model that can accurately classify new data points as normal or anomalous. It's often used when the definition of anomalies is well-understood.\n",
        "\n",
        "5. Performance: Supervised methods can potentially achieve higher accuracy and precision because they are trained on labeled data, which explicitly defines anomalies. However, they may not perform well when anomalies are rare and the dataset is highly imbalanced."
      ],
      "metadata": {
        "id": "-_lwx8VeSktP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 anser\n",
        "\n",
        "Anomaly detection algorithms in machine learning can be categorized into several main categories based on their underlying techniques and approaches. Here are the main categories of anomaly detection algorithms:\n",
        "\n",
        "1. Statistical Methods:\n",
        "\n",
        "Z-Score/Standard Deviation: This method identifies anomalies as data points that fall significantly outside the mean and standard deviation of the dataset.\n",
        "Percentiles/Quartiles: Anomalies can be detected by looking for data points that exceed a certain percentile threshold in the dataset.\n",
        "2. Density-Based Methods:\n",
        "\n",
        "DBSCAN (Density-Based Spatial Clustering of Applications with Noise): DBSCAN groups together data points that are close to each other in density-connected clusters and considers data points in low-density areas as anomalies.\n",
        "LOF (Local Outlier Factor): LOF measures the local density deviation of a data point with respect to its neighbors and flags points with low local density as anomalies.\n",
        "3. Distance-Based Methods:\n",
        "\n",
        "K-Nearest Neighbors (KNN): KNN calculates the distance between data points and identifies anomalies as those with few or no neighbors within a specified radius.\n",
        "Mahalanobis Distance: This distance metric considers correlations between features and identifies anomalies based on how far a data point is from the centroid of the data.\n",
        "4. Clustering Methods:\n",
        "\n",
        "K-Means Clustering: K-Means can be used for anomaly detection by treating data points far from cluster centers as anomalies.\n",
        "Gaussian Mixture Models (GMM): GMMs model data using a mixture of Gaussian distributions and can detect anomalies as data points with low likelihood under the model.\n",
        "5. Dimensionality Reduction Methods:\n",
        "\n",
        "Principal Component Analysis (PCA): PCA can reduce the dimensionality of the data while preserving variance. Anomalies can be detected by looking at data points with large reconstruction errors.\n",
        "Autoencoders: Autoencoders are neural network architectures that learn to encode data and then decode it. Anomalies are detected by measuring the reconstruction error.\n",
        "6. Ensemble Methods:\n",
        "\n",
        "Isolation Forest: This ensemble method constructs isolation trees and identifies anomalies as data points that require fewer splits to isolate.\n",
        "Random Forest: Random Forest can be adapted for anomaly detection by considering the anomaly score assigned to data points by the ensemble.\n",
        "7. One-Class SVM (Support Vector Machine): One-Class SVM learns a boundary around the majority class (normal data) and flags data points outside this boundary as anomalies.\n",
        "\n",
        "8. Time Series Methods:\n",
        "\n",
        "SARIMA (Seasonal Autoregressive Integrated Moving Average): SARIMA models are used for time series anomaly detection by forecasting values and flagging deviations from the forecast.\n",
        "Prophet: Facebook Prophet is a tool for time series forecasting and anomaly detection, particularly suited for datasets with seasonal patterns.\n",
        "9. Deep Learning Methods:\n",
        "\n",
        "Variational Autoencoders (VAE): VAEs are generative models that can learn to reconstruct data. Anomalies are detected based on high reconstruction error.\n",
        "Recurrent Neural Networks (RNNs): RNNs can model sequential data and are used for time series anomaly detection.\n",
        "Long Short-Term Memory (LSTM): LSTM, a type of RNN, is effective for sequence-based anomaly detection.\n",
        "10. Spectral Methods: These methods analyze the eigenvalues and eigenvectors of matrices derived from the data and identify anomalies based on spectral properties."
      ],
      "metadata": {
        "id": "4pDJmUQvTBR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 answer\n",
        "\n",
        "Distance-based anomaly detection methods in machine learning rely on specific assumptions about the data and the nature of anomalies. These assumptions form the basis for these algorithms to identify anomalies based on the distances between data points. Here are the main assumptions made by distance-based anomaly detection methods:\n",
        "\n",
        "1. Proximity Assumption: Distance-based methods assume that normal data points tend to be close to each other in the feature space. In other words, they assume that the majority of data points exhibit similar behavior and cluster together, forming dense regions.\n",
        "\n",
        "2. Isolation of Anomalies: These methods assume that anomalies are isolated or sparse in the feature space, meaning they are significantly different from normal data points and tend to have larger distances to their nearest neighbors.\n",
        "\n",
        "3. Homogeneity of Normal Data: Distance-based methods assume that normal data points are homogeneous, and anomalies are the exceptions. This assumption implies that normal data points are generated from the same underlying distribution and follow similar patterns.\n",
        "\n",
        "4. Euclidean Distance Metric: Many distance-based methods, such as k-nearest neighbors (KNN) and DBSCAN, assume the use of the Euclidean distance metric to measure distances between data points. This metric works well when data attributes are continuous and have similar scales.\n",
        "\n",
        "5. Fixed Neighborhood Radius (KNN): KNN, for example, assumes a fixed neighborhood radius (k) for identifying the k-nearest neighbors of a data point. Anomalies are those data points that have fewer than k neighbors within the specified radius.\n",
        "\n",
        "6. Single-Dimensional Assumption: While distance-based methods can work with multidimensional data, they may not perform optimally in high-dimensional spaces due to the \"curse of dimensionality.\" In high-dimensional spaces, distance metrics become less discriminative, and it becomes harder to identify anomalies.\n",
        "\n",
        "7. Scalability: Some distance-based methods assume that the data can be efficiently processed and distances can be computed in a reasonable time frame, which may not hold true for extremely large datasets.\n",
        "\n",
        "8. Noisy Data Handling: These methods may not perform well when the data contains a significant amount of noise or when outliers within normal data are present, as they can be wrongly classified as anomalies.\n",
        "\n",
        "9. Symmetry of Distances: Distance-based methods often assume that the distance metric used is symmetric, meaning the distance between point A and point B is the same as the distance between point B and point A. This assumption holds for common distance metrics like Euclidean distance."
      ],
      "metadata": {
        "id": "vrVuFvPLTpYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 answer\n",
        "\n",
        "The LOF (Local Outlier Factor) algorithm is a popular unsupervised anomaly detection method that computes anomaly scores for data points based on their local density compared to the density of their neighbors. The main idea behind LOF is to identify data points that have a significantly lower local density than their neighbors, indicating that they are potentially outliers or anomalies. Here's how LOF computes anomaly scores:\n",
        "\n",
        "1. Local Reachability Density (LRD):\n",
        "\n",
        "For each data point, LOF calculates its \"local reachability density\" (LRD). This is a measure of how dense the region around the data point is compared to the density of its neighbors.\n",
        "The LRD of a data point is computed as the inverse of the average reachability distance from the data point to its k nearest neighbors (where k is a user-defined parameter). The reachability distance between two data points measures how \"reachable\" one point is from another, taking into account the local density.\n",
        "\n",
        "Mathematically, the LRD of data point p is calculated as:"
      ],
      "metadata": {
        "id": "cHzMMqlYUDgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LRD(p) = 1 / (Σ reach_dist(p, o) for o in k-nearest neighbors of p)\n"
      ],
      "metadata": {
        "id": "m7yw7g8vUaR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Smaller LRD values indicate that a data point is in a region with lower density, while larger LRD values indicate higher local density.\n",
        "2. Local Outlier Factor (LOF):\n",
        "\n",
        "After calculating the LRD for each data point, LOF computes the \"local outlier factor\" (LOF) for each data point.\n",
        "The LOF of a data point p is a measure of how much the local density of p differs from the local densities of its neighbors. It is calculated as the ratio of the LRD of p to the LRDs of its k nearest neighbors.\n",
        "Mathematically, the LOF of data point p is calculated as:"
      ],
      "metadata": {
        "id": "qMDMrrjgUbQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOF(p) = (Σ LRD(o) for o in k-nearest neighbors of p) / (k * LRD(p))\n"
      ],
      "metadata": {
        "id": "uVxd8eljUi9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOF values greater than 1 indicate that a data point has a lower local density than its neighbors, suggesting that it may be an outlier or anomaly. The higher the LOF value, the more of an outlier the data point is considered to be.\n",
        "3. Anomaly Score:\n",
        "\n",
        "To obtain the final anomaly score for each data point, LOF subtracts 1 from the LOF value and takes the reciprocal. This is done to scale the anomaly score, so higher values indicate more anomalous data points.\n",
        "Mathematically, the anomaly score (AS) of data point p is calculated as:"
      ],
      "metadata": {
        "id": "b1JTfPMeUkw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AS(p) = 1 / (LOF(p) - 1)\n"
      ],
      "metadata": {
        "id": "KdOxUk4UUoYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The anomaly scores are then sorted in descending order, with higher scores indicating more anomalous data points."
      ],
      "metadata": {
        "id": "AZuyNRcNUqXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 answer\n",
        "\n",
        "The Isolation Forest algorithm is an ensemble-based anomaly detection method that's designed to efficiently identify anomalies or outliers in a dataset. In Python, when using libraries like Scikit-learn or other implementations, there are several key parameters you can adjust to fine-tune the behavior of the Isolation Forest algorithm:\n",
        "\n",
        "1. n_estimators:\n",
        "\n",
        "This parameter determines the number of base isolation trees in the ensemble. A larger number of trees can lead to better accuracy but can also increase computation time. It's a hyperparameter you can tune to balance performance and computational cost.\n",
        "2. max_samples:\n",
        "\n",
        "It specifies the maximum number of samples used to build each isolation tree. By default, it's set to \"auto,\" which means it's set to the size of the input dataset. You can set it to an integer or a float to control the number of samples used for each tree. A smaller value may lead to faster training times and potentially more isolated anomalies, but it may also reduce detection accuracy.\n",
        "3. contamination:\n",
        "\n",
        "This parameter defines the proportion of anomalies expected in the dataset. It is used to set a threshold for flagging data points as anomalies. The default value is 0.1, meaning that 10% of the data is assumed to be anomalies. You can adjust this value based on your domain knowledge or the specific requirements of your problem.\n",
        "4. max_features:\n",
        "\n",
        "It determines the maximum number of features to consider when splitting a node in an isolation tree. By default, it's set to the number of features in the dataset. Reducing this value can introduce randomness and potentially improve the algorithm's ability to isolate anomalies.\n",
        "5. bootstrap:\n",
        "\n",
        "This binary parameter controls whether subsampling of the data is performed with or without replacement when building the isolation trees. Setting it to True enables bootstrap sampling, which introduces randomness into the tree-building process. It can be useful for improving diversity among the trees.\n",
        "6. n_jobs:\n",
        "\n",
        "This parameter specifies the number of CPU cores to use for parallel computation. It can speed up training for large datasets by leveraging multiple CPU cores.\n",
        "7. random_state:\n",
        "\n",
        "It is a seed value for the random number generator. Setting this parameter to a specific integer value ensures reproducibility of results when you run the algorithm multiple times with the same dataset and parameters.\n",
        "8. behaviour (available in some implementations):\n",
        "\n",
        "This parameter controls the behavior of the algorithm when handling \"pure\" data, which consists of a single unique value across all features. It can be set to \"new\" or \"old\" to choose between different strategies for handling pure data."
      ],
      "metadata": {
        "id": "vVDBDq65UsY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 answer\n",
        "\n",
        "1. Calculate the reachability distance (reach_dist) for p:\n",
        "\n",
        "The reachability distance from p to each of its 10 nearest neighbors is calculated. Since there are 2 neighbors of the same class within a radius of 0.5, let's assume these two neighbors are included among the 10 nearest neighbors. For simplicity, let's denote these two neighbors as n1 and n2, and the other 8 neighbors as n3, n4, ..., n10.\n",
        "\n",
        "The reachability distance from p to each neighbor is calculated as the maximum of the Euclidean distance between p and the neighbor and the"
      ],
      "metadata": {
        "id": "CpsekIsnVTO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reach_dist(p, n1) = max(distance(p, n1), 0.5)\n",
        "reach_dist(p, n2) = max(distance(p, n2), 0.5)\n",
        "reach_dist(p, n3) = max(distance(p, n3), 0.5)\n",
        "...\n",
        "reach_dist(p, n10) = max(distance(p, n10), 0.5)\n"
      ],
      "metadata": {
        "id": "qvqqR6AtVdD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Calculate the local reachability density (LRD) for p:\n",
        "\n",
        "LRD is the inverse of the average reachability distance from p to its 10 nearest neighbors:"
      ],
      "metadata": {
        "id": "Lg5xRvI4VfOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LRD(p) = 1 / (Σ reach_dist(p, ni) for i = 1 to 10)\n"
      ],
      "metadata": {
        "id": "3RzoSk61Vhua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate this value for p.\n",
        "\n",
        "3. Calculate the local outlier factor (LOF) for p:\n",
        "\n",
        "LOF measures how much the LRD of p differs from the LRDs of its neighbors. Calculate the LOF as follows:"
      ],
      "metadata": {
        "id": "rsJ-y_JrVnHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOF(p) = (Σ LRD(ni) for i = 1 to 10) / (10 * LRD(p))\n"
      ],
      "metadata": {
        "id": "5Fu5tWD3Vjgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Calculate the anomaly score for p:\n",
        "\n",
        "Finally, calculate the anomaly score for p by subtracting 1 from the LOF value and taking the reciprocal:"
      ],
      "metadata": {
        "id": "IKAQm4ktVqzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Anomaly Score(p) = 1 / (LOF(p) - 1)\n"
      ],
      "metadata": {
        "id": "CxYiMFk_Vtbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9 answer\n",
        "\n",
        "In the Isolation Forest algorithm, the anomaly score for a data point is typically calculated based on the average path length (APL) of that data point as it traverses through the isolation trees in the forest. The APL of a data point is compared to the expected APL for a point in a random dataset. Specifically, a lower APL indicates that a data point is easier to isolate and is considered more anomalous.\n",
        "\n",
        "Given your scenario with 100 trees and a dataset of 3000 data points, if you have a data point with an average path length of 5.0 compared to the average path length of the trees in the forest, you can interpret it as follows:\n",
        "\n",
        "An APL of 5.0 is shorter than the expected APL for a random data point in a normal dataset. This suggests that the data point is relatively easy to isolate and is potentially more anomalous.\n",
        "\n",
        "Anomalies in the Isolation Forest algorithm are typically characterized by shorter APLs because they are less likely to follow the average path length of normal data points, which is generally longer.\n",
        "\n",
        "To quantify the anomaly score in Python, you can use the following formula:"
      ],
      "metadata": {
        "id": "k6xtCIznVu_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Anomaly Score = 2 ** (-APL / c(n))\n"
      ],
      "metadata": {
        "id": "QmMq-vrlV8Zh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where:\n",
        "\n",
        "APL is the average path length of the data point (in this case, 5.0).\n",
        "c(n) is a constant related to the expected APL for a random data point in a dataset with n points. For a dataset of 3000 points and 100 trees, c(n) can be approximated as follows:"
      ],
      "metadata": {
        "id": "rrs28XcoV-WA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c(n) = 2 * (log(n - 1) + 0.5772156649) - (2 * (n - 1) / n)\n"
      ],
      "metadata": {
        "id": "TqRCpNUmWAvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Substitute the values into the formula to calculate the anomaly score:"
      ],
      "metadata": {
        "id": "-PfEsHCtWCJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "APL = 5.0\n",
        "n = 3000\n",
        "\n",
        "c_n = 2 * (log(n - 1) + 0.5772156649) - (2 * (n - 1) / n)\n",
        "\n",
        "Anomaly_Score = 2 ** (-APL / c_n)\n"
      ],
      "metadata": {
        "id": "vei3cVc6WFQI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}