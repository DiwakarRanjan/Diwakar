{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1 answer"
      ],
      "metadata": {
        "id": "gLvwfcEJHrj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytube\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hxlpboVJ0bC",
        "outputId": "ebb7be45-80fb-4b2e-942e-7a9bc3f910d4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from pytube import YouTube\n",
        "\n",
        "# Define the URL of the YouTube channel\n",
        "channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
        "\n",
        "# Function to extract video URLs\n",
        "def extract_video_urls(channel_url, num_videos=5):\n",
        "    video_urls = []\n",
        "\n",
        "    try:\n",
        "        channel = YouTube(channel_url)\n",
        "        videos = channel.video_urls[:num_videos]\n",
        "\n",
        "        for video_url in videos:\n",
        "            video_urls.append(video_url)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return video_urls\n",
        "\n",
        "# Extract video URLs\n",
        "video_urls = extract_video_urls(channel_url)\n",
        "\n",
        "# Save the data to a CSV file\n",
        "csv_filename = \"video_urls.csv\"\n",
        "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    csv_writer.writerow([\"Video URL\"])\n",
        "\n",
        "    for video_url in video_urls:\n",
        "        csv_writer.writerow([video_url])\n",
        "\n",
        "print(f\"Video URLs of the first {len(video_urls)} videos saved to {csv_filename}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiZ5oJX4Jt_o",
        "outputId": "5c369276-3bd1-41b3-eafb-d06f422e9dab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: regex_search: could not find match for (?:v=|\\/)([0-9A-Za-z_-]{11}).*\n",
            "Video URLs of the first 0 videos saved to video_urls.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 answer"
      ],
      "metadata": {
        "id": "0DOaXYoHJ5HJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Define the URL of the YouTube channel\n",
        "channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
        "\n",
        "# Send an HTTP GET request to the channel URL\n",
        "response = requests.get(channel_url)\n",
        "\n",
        "# Parse the HTML content of the page using BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Function to extract thumbnail URLs\n",
        "def extract_thumbnail_urls(channel_soup, num_videos=5):\n",
        "    thumbnail_urls = []\n",
        "\n",
        "    # Find the first five video elements on the page\n",
        "    video_elements = channel_soup.find_all('div', class_='style-scope ytd-grid-video-renderer')[:num_videos]\n",
        "\n",
        "    for video in video_elements:\n",
        "        thumbnail = video.find('img')\n",
        "        if thumbnail:\n",
        "            thumbnail_url = thumbnail['src']\n",
        "            thumbnail_urls.append(thumbnail_url)\n",
        "\n",
        "    return thumbnail_urls\n",
        "\n",
        "# Extract thumbnail URLs\n",
        "thumbnail_urls = extract_thumbnail_urls(soup)\n",
        "\n",
        "# Save the data to a CSV file\n",
        "csv_filename = \"thumbnail_urls.csv\"\n",
        "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    csv_writer.writerow([\"Thumbnail URL\"])\n",
        "\n",
        "    for thumbnail_url in thumbnail_urls:\n",
        "        csv_writer.writerow([thumbnail_url])\n",
        "\n",
        "print(f\"Thumbnail URLs of the first {len(thumbnail_urls)} videos saved to {csv_filename}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JcUyqUJJ_Eu",
        "outputId": "a651f290-d82e-4ab7-a6b2-55b5e81783d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thumbnail URLs of the first 0 videos saved to thumbnail_urls.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 answer"
      ],
      "metadata": {
        "id": "ttzkZV-NKCKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Define the URL of the YouTube channel\n",
        "channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
        "\n",
        "# Send an HTTP GET request to the channel URL\n",
        "response = requests.get(channel_url)\n",
        "\n",
        "# Parse the HTML content of the page using BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Function to extract video titles\n",
        "def extract_video_titles(channel_soup, num_videos=5):\n",
        "    video_titles = []\n",
        "\n",
        "    # Find the first five video elements on the page\n",
        "    video_elements = channel_soup.find_all('div', class_='style-scope ytd-grid-video-renderer')[:num_videos]\n",
        "\n",
        "    for video in video_elements:\n",
        "        title_element = video.find('a', class_='yt-simple-endpoint style-scope ytd-grid-video-renderer')\n",
        "        if title_element:\n",
        "            title = title_element['title']\n",
        "            video_titles.append(title)\n",
        "\n",
        "    return video_titles\n",
        "\n",
        "# Extract video titles\n",
        "video_titles = extract_video_titles(soup)\n",
        "\n",
        "# Save the data to a CSV file\n",
        "csv_filename = \"video_titles.csv\"\n",
        "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    csv_writer.writerow([\"Video Title\"])\n",
        "\n",
        "    for title in video_titles:\n",
        "        csv_writer.writerow([title])\n",
        "\n",
        "print(f\"Titles of the first {len(video_titles)} videos saved to {csv_filename}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muOHrszYKDc8",
        "outputId": "0894de65-31bd-4c65-865f-1770f302903b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Titles of the first 0 videos saved to video_titles.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 answer"
      ],
      "metadata": {
        "id": "B--8vsx2KGXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Define the URL of the YouTube channel\n",
        "channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
        "\n",
        "# Send an HTTP GET request to the channel URL\n",
        "response = requests.get(channel_url)\n",
        "\n",
        "# Parse the HTML content of the page using BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Function to extract video view counts\n",
        "def extract_video_views(channel_soup, num_videos=5):\n",
        "    video_views = []\n",
        "\n",
        "    # Find the first five video elements on the page\n",
        "    video_elements = channel_soup.find_all('div', class_='style-scope ytd-grid-video-renderer')[:num_videos]\n",
        "\n",
        "    for video in video_elements:\n",
        "        views_element = video.find('span', class_='style-scope ytd-grid-video-renderer')\n",
        "        if views_element:\n",
        "            views = views_element.text.strip()\n",
        "            video_views.append(views)\n",
        "\n",
        "    return video_views\n",
        "\n",
        "# Extract video view counts\n",
        "video_views = extract_video_views(soup)\n",
        "\n",
        "# Save the data to a CSV file\n",
        "csv_filename = \"video_views.csv\"\n",
        "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    csv_writer.writerow([\"View Count\"])\n",
        "\n",
        "    for views in video_views:\n",
        "        csv_writer.writerow([views])\n",
        "\n",
        "print(f\"View counts of the first {len(video_views)} videos saved to {csv_filename}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLl-eYOpKHeJ",
        "outputId": "22be4fc2-4c5f-4eb8-f6c1-c8c9943201b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View counts of the first 0 videos saved to video_views.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 answer"
      ],
      "metadata": {
        "id": "bqSlRI85KKd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Define the URL of the YouTube channel\n",
        "channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
        "\n",
        "# Send an HTTP GET request to the channel URL\n",
        "response = requests.get(channel_url)\n",
        "\n",
        "# Parse the HTML content of the page using BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Function to extract video posting times\n",
        "def extract_video_posting_times(channel_soup, num_videos=5):\n",
        "    posting_times = []\n",
        "\n",
        "    # Find the first five video elements on the page\n",
        "    video_elements = channel_soup.find_all('div', class_='style-scope ytd-grid-video-renderer')[:num_videos]\n",
        "\n",
        "    for video in video_elements:\n",
        "        posting_time_element = video.find('div', class_='style-scope ytd-grid-video-renderer').next_sibling\n",
        "        if posting_time_element:\n",
        "            posting_time = posting_time_element.text.strip()\n",
        "            posting_times.append(posting_time)\n",
        "\n",
        "    return posting_times\n",
        "\n",
        "# Extract video posting times\n",
        "video_posting_times = extract_video_posting_times(soup)\n",
        "\n",
        "# Save the data to a CSV file\n",
        "csv_filename = \"video_posting_times.csv\"\n",
        "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    csv_writer.writerow([\"Posting Time\"])\n",
        "\n",
        "    for posting_time in video_posting_times:\n",
        "        csv_writer.writerow([posting_time])\n",
        "\n",
        "print(f\"Posting times of the first {len(video_posting_times)} videos saved to {csv_filename}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEGeeFDiKLgJ",
        "outputId": "2a29c5e8-1d59-4e51-85b2-432ba7bdf5fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Posting times of the first 0 videos saved to video_posting_times.csv.\n"
          ]
        }
      ]
    }
  ]
}