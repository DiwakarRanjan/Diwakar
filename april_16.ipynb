{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-PobgZR0xcd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 answer\n",
        "\n",
        "Boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers. It is done by building a model by using weak models in series. Firstly, a model is built from the training data. Then the second model is built which tries to correct the errors present in the first model. This procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models are added.\n",
        "\n",
        "Advantages of Boosting\n",
        "Improved Accuracy – Boosting can improve the accuracy of the model by combining several weak models’ accuracies and averaging them for regression or voting over them for classification to increase the accuracy of the final model.\n",
        "Robustness to Overfitting – Boosting can reduce the risk of overfitting by reweighting the inputs that are classified wrongly.\n",
        "Better handling of imbalanced data – Boosting can handle the imbalance data by focusing more on the data points that are misclassified\n",
        "Better Interpretability – Boosting can increase the interpretability of the model by breaking the model decision process into multiple processes."
      ],
      "metadata": {
        "id": "acQAwjyZ0zT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 answer\n",
        "\n",
        "Advantages of Boosting\n",
        "Improved Accuracy – Boosting can improve the accuracy of the model by combining several weak models’ accuracies and averaging them for regression or voting over them for classification to increase the accuracy of the final model.\n",
        "Robustness to Overfitting – Boosting can reduce the risk of overfitting by reweighting the inputs that are classified wrongly.\n",
        "Better handling of imbalanced data – Boosting can handle the imbalance data by focusing more on the data points that are misclassified\n",
        "Better Interpretability – Boosting can increase the interpretability of the model by breaking the model decision process into multiple processes.  \n",
        "Training of Boosting Model\n",
        "\n",
        "1. Initialise the dataset and assign equal weight to each of the data point.\n",
        "2. Provide this as input to the model and identify the wrongly classified data points.\n",
        "3. Increase the weight of the wrongly classified data points.\n",
        "4. if (got required results)\n",
        "  Goto step 5\n",
        "5. else\n",
        "  Goto step 2\n",
        "End"
      ],
      "metadata": {
        "id": "5gMPbUfC03hn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 answer\n",
        "\n",
        "To find weak rule, we apply base learning (ML) algorithms with a different distribution. Each time base learning algorithm is applied, it generates a new weak prediction rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.\n",
        "\n",
        "Here’s another question which might haunt you, ‘How do we choose different distribution for each round?’\n",
        "\n",
        "For choosing the right distribution, here are the following steps:\n",
        "\n",
        "Step 1:  The base learner takes all the distributions and assign equal weight or attention to each observation.\n",
        "\n",
        "Step 2: If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.\n",
        "\n",
        "Step 3: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.\n",
        "\n",
        "Finally, it combines the outputs from weak learner and creates  a strong learner which eventually improves the prediction power of the model. Boosting pays higher focus on examples which are mis-classiﬁed or have higher errors by preceding weak rules."
      ],
      "metadata": {
        "id": "O-rA-SDD1lqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 answer\n",
        "\n",
        "1. Gradient Boosting – It is a boosting technique that builds a final model from the sum of several weak learning algorithms that were trained on the same dataset. It operates on the idea of stagewise addition. The first weak learner in the gradient boosting algorithm will not be trained on the dataset; instead, it will simply return the mean of the relevant column. The residual for the first weak learner algorithm’s output will then be calculated and used as the output column or target column for the next weak learning algorithm that will be trained. The second weak learner will be trained using the same methodology, and the residuals will be computed and utilized as an output column once more for the third weak learner, and so on until we achieve zero residuals. The dataset for gradient boosting must be in the form of numerical or categorical data, and the loss function used to generate the residuals must be differential at all times.\n",
        "2. XGBoost – In addition to the gradient boosting technique, XGBoost is another boosting machine learning approach. The full name of the XGBoost algorithm is the eXtreme Gradient Boosting algorithm, which is an extreme variation of the previous gradient boosting technique. The key distinction between XGBoost and GradientBoosting is that XGBoost applies a regularisation approach. It is a regularised version of the current gradient-boosting technique. Because of this, XGBoost outperforms a standard gradient boosting method, which explains why it is also faster than that. Additionally, it works better when the dataset contains both numerical and categorical variables.\n",
        "3. Adaboost – AdaBoost is a boosting algorithm that also works on the principle of the stagewise addition method where multiple weak learners are used for getting strong learners. The value of the alpha parameter, in this case, will be indirectly proportional to the error of the weak learner, Unlike Gradient Boosting in XGBoost, the alpha parameter calculated is related to the errors of the weak learner, here the value of the alpha parameter will be indirectly proportional to the error of the weak learner.\n",
        "4. CatBoost – The growth of decision trees inside CatBoost is the primary distinction that sets it apart from and improves upon competitors. The decision trees that are created in CatBoost are symmetric. As there is a unique sort of approach for handling categorical datasets, CatBoost works very well on categorical datasets compared to any other algorithm in the field of machine learning. The categorical features in CatBoost are encoded based on the output columns. As a result, the output column’s weight will be taken into account while training or encoding the categorical features, increasing its accuracy on categorical datasets."
      ],
      "metadata": {
        "id": "m8F1vYPT2MZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 answer\n",
        "\n",
        "To find weak rule, we apply base learning (ML) algorithms with a different distribution. Each time base learning algorithm is applied, it generates a new weak prediction rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.\n",
        "\n",
        "Here’s another question which might haunt you, ‘How do we choose different distribution for each round?’\n",
        "\n",
        "For choosing the right distribution, here are the following steps:\n",
        "\n",
        "Step 1:  The base learner takes all the distributions and assign equal weight or attention to each observation.\n",
        "\n",
        "Step 2: If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.\n",
        "\n",
        "Step 3: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.\n",
        "\n",
        "Finally, it combines the outputs from weak learner and creates  a strong learner which eventually improves the prediction power of the model. Boosting pays higher focus on examples which are mis-classiﬁed or have higher errors by preceding weak rules.\n",
        "\n",
        "\n",
        "\n",
        "Types of Boosting Algorithms\n",
        "Underlying engine used for boosting algorithms can be anything.  It can be decision stamp, margin-maximizing classification algorithm etc. There are many boosting algorithms which use other types of engine such as:\n",
        "\n",
        "1. AdaBoost (Adaptive Boosting)\n",
        "2. Gradient Tree Boosting\n",
        "3. XGBoost\n",
        "In this article, we will focus on AdaBoost and Gradient Boosting followed by their respective python codes and will focus on XGboost in upcoming article.\n",
        "\n",
        "\n",
        "1. AdaBoost (Adaptive Boosting) : It works on similar method as discussed above. It fits a sequence of weak learners on different weighted training data. It starts by predicting original data set and gives equal weight to each observation. If prediction is incorrect using the first learner, then it gives higher weight to observation which have been predicted incorrectly. Being an iterative process, it continues to add learner(s) until a limit is reached in the number of models or accuracy.\n",
        "\n",
        "Mostly, we use decision stamps with AdaBoost. But, we can use any machine learning algorithms as base learner if it accepts weight on training data set. We can use AdaBoost algorithms for both classification and regression problem.\n",
        "\n",
        "\n",
        "2. In gradient boosting, it trains many model sequentially. Each new model gradually minimizes the loss function (y = ax + b + e, e needs special attention as it is an error term) of the whole system using Gradient Descent method. The learning procedure consecutively fit new models to provide a more accurate estimate of the response variable."
      ],
      "metadata": {
        "id": "HrgXyvkl2gxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 answer\n",
        "\n",
        "Boosting algorithms combine weak learners to create a strong learner by iteratively training a series of weak models, giving more weight to the data points that the previous weak models misclassified. The idea is to focus on the examples that are difficult to classify correctly and to iteratively improve the overall model's performance. Here's a general overview of how boosting works:\n",
        "\n",
        "1. Initialize weights: At the beginning of the boosting process, all data points are assigned equal weights.\n",
        "\n",
        "2. Iterative training of weak learners: Boosting algorithms typically use a simple base learner, often referred to as a \"weak learner\" or \"base classifier.\" This could be a decision stump (a one-level decision tree), a simple linear model, or any other weak model.\n",
        "\n",
        "3. Fit the weak learner: Train the weak learner on the dataset with the current weights assigned to each data point. The weak learner's objective is to minimize the weighted classification error. After training, the weak learner makes predictions on the entire dataset.\n",
        "\n",
        "4. Compute the weighted error: Calculate the weighted error of the weak learner's predictions. The weighted error is the sum of the weights of the misclassified examples.\n",
        "\n",
        "5. Update weights: Increase the weights of the misclassified data points. This gives more importance to the examples that were difficult to classify correctly in the previous iteration. The idea is to make these examples more likely to be correctly classified in the next iteration.\n",
        "\n",
        "6. Repeat: Steps 3-5 are repeated for a predefined number of iterations or until a certain stopping criterion is met.\n",
        "\n",
        "7. Combine weak learners: Finally, the boosting algorithm combines all the weak learners by assigning them weights based on their performance. The weak learners that perform better at correctly classifying the examples are given higher weights in the final ensemble model.\n",
        "\n",
        "8. Final prediction: To make a prediction on a new data point, each weak learner's prediction is weighted according to its importance in the ensemble, and the weighted predictions are combined to produce the final prediction."
      ],
      "metadata": {
        "id": "UuXNNe-Y4GlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 answer\n",
        "\n",
        "Ada Boost\n",
        "\n",
        "Ada boost or adaptive boosting is the first stepping stone in the world of boosting algorithms.\n",
        "Ada boost is the first boosting algorithm among all the boosting algorithms. The main idea behind this algorithm is to combine multiple weak learners to into a strong classifier. It can be used for both classification and regression tasks.\n",
        "One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor has incorrectly classified. This results in new predictors focusing more and more on the hard cases. This is the technique used by Ada‐Boost.\n",
        "For example, to build an Ada Boost classifier, a first base classifier (such as a Decision Tree) is trained and used to make predictions on the training set. The relative weight of misclassified training instances is then increased. A second classifier is trained using the updated weights and again it makes predictions on the training set, weights are updated, and so on\n",
        "When the random forest is used, the algorithm makes n number of trees. It makes proper trees that consist of a start node with several leaves nodes. Some trees might be bigger than others, but there is no fixed depth in a random forest. But with Adaboost, that’s not the case. In AdaBoost, the algorithm only makes a node with two leaves, and this is known as Stump.\n",
        "\n",
        "Working of Ada Boost\n",
        "\n",
        "Step 1:\n",
        "\n",
        "At very first step Ada Boost reads the dataset and assigns equal weight to each feature.\n",
        "\n",
        "Step 2:\n",
        "\n",
        "In this step Ada boost will create it’s first base learner i.e. the first stump. It will create the same number of stumps as the number of features.\n",
        "\n",
        "Stumps\n",
        "\n",
        "Out of these 3 models, the algorithm selects only one. For selecting a base learner, there are two properties, those are, Gini and Entropy. We must calculate Gini or Entropy the same way it is calculated for decision trees. The stump that has the least value will be the first base learner.\n",
        "In the figure, all the 3 stumps can be made with 3 features. The number below the leaves represents the correctly and incorrectly classified records. By using these records, the Gini or entropy index is calculated. The stump that has the least entropy or Gini will be selected for the base learner.\n",
        "Now, we will assume that the entropy index is the least for stump1. So, let’s take stump 1, i.e., feature 1 as our first base learner.\n",
        "\n",
        "Step 3:\n",
        "\n",
        "Now, Ada boost will calculate the total error by using the formula :\n",
        "\n",
        "No. of Samples classified as wrong / Total No. of Samples\n",
        "\n",
        "In our case, there is only 1 error, so Total Error (TE) = 1/5.\n",
        "\n",
        "Step 4:\n",
        "\n",
        "After calculating the total error now Ada boost will calculate the performance of stump.\n",
        "\n",
        "Performance of Stump Formula\n",
        "\n",
        "Why its necessary to calculate the TE and performance of stump?\n",
        "\n",
        "The answer is, we must update the sample weight before proceeding for the next model or stage because if the same weight is applied, we receive the output from the first model. In boosting, only the wrong records/incorrectly classified records got more preference than the correctly classified records. Thus, only the wrong records from the decision tree/stump are passed on to another stump. While in Ada Boost, both records were allowed to pass, the wrong records are repeated more than the correct ones. We must increase the weight for the wrongly classified records and decrease the weight for the correctly classified records. In the next step, we will be updating the weights based on the performance of the stump.\n",
        "\n",
        "Step 5 :\n",
        "\n",
        "Now, Ada boost will update the weights of correctly and in correctly classified samples :\n",
        "\n",
        "Weights of incorrectly classified samples = Weight * ePerf Say\n",
        "\n",
        "Weights of correctly classified samples = Weight * e-Perf Say\n",
        "\n",
        "Row No.\tFeature 1\tFeature 2\tFeature 3\tOutput\tSample Weight\n",
        "Updated\n",
        "\n",
        "\n",
        "Step 6:\n",
        "\n",
        "Now, in this last step the Ada boost will create a new dataset for the next weak learner\n",
        "\n",
        "The algorithm will run 5 iterations to select different-different records from the older dataset. In each iteration the algorithm will take one random value and selects the record whose bucket contains that value. Like this there is a high probability for the wrong records to get selected several times.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lCgkAHza4zO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 answer\n",
        "\n",
        "In machine learning and mathematical optimization, loss functions for classification are computationally feasible loss functions representing the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to).[1] Given\n",
        "\n",
        "{\\mathcal {X}} as the space of all possible inputs (usually\n",
        "\n",
        "{\\displaystyle {\\mathcal {X}}\\subset \\mathbb {R} ^{d}}), and\n",
        "\n",
        "{\\displaystyle {\\mathcal {Y}}=\\{-1,1\\}} as the set of labels (possible outputs), a typical goal of classification algorithms is to find a function\n",
        "\n",
        "f:{\\mathcal  {X}}\\to {\\mathcal  {Y}} which best predicts a label\n",
        "\n",
        "y for a given input\n",
        "\n",
        "→{\\vec {x}}.[2] However, because of incomplete information, noise in the measurement, or probabilistic components in the underlying process, it is possible for the same\n",
        "\n",
        "→{\\vec {x}} to generate different\n",
        "\n",
        "y.[3] As a result, the goal of the learning problem is to minimize expected loss (also known as the risk), defined as\n",
        "\n",
        "{\\displaystyle I[f]=\\displaystyle \\int _{{\\mathcal {X}}\\times {\\mathcal {Y}}}V(f({\\vec {x}}),y)\\,p({\\vec {x}},y)\\,d{\\vec {x}}\\,dy}"
      ],
      "metadata": {
        "id": "tJRuTZqL5Z3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9 answer\n",
        "\n",
        "AdaBoost (Adaptive Boosting) is an ensemble learning algorithm that combines multiple weak classifiers to create a strong classifier. It assigns weights to each sample in the training dataset and updates these weights in each iteration to focus on the samples that are misclassified by the current weak classifier. Here's how AdaBoost updates the weights of misclassified samples:\n",
        "\n",
        "Initialization: Initially, all samples are assigned equal weights, so each sample has a weight of 1/N, where N is the total number of samples in the training dataset.\n",
        "\n",
        "1. Training Weak Classifier: AdaBoost trains a weak classifier on the training data. A weak classifier is a simple and often shallow model, like a decision stump (a one-level decision tree). The classifier's goal is to minimize the weighted error rate of the training samples.\n",
        "\n",
        "2. Weighted Error Rate: For each sample, AdaBoost calculates the weighted error rate of the weak classifier. The weighted error rate is the sum of the weights of the misclassified samples divided by the sum of all weights. It is used to measure how well the weak classifier is performing.\n",
        "\n",
        "Weighted Error Rate (ε) = Σ(w_i * I(y_i ≠ h_t(x_i))) / Σ(w_i)\n",
        "\n",
        "w_i: The weight of sample i.\n",
        "y_i: The true label of sample i.\n",
        "h_t(x_i): The prediction of the weak classifier for sample i.\n",
        "3. I(condition): Indicator function that returns 1 if the condition is true and 0 otherwise.\n",
        "4. . Update Weights: AdaBoost updates the weights of the training samples based on the weighted error rate of the weak classifier. Samples that are misclassified by the weak classifier receive higher weights, making them more important for the next iteration. Samples that are correctly classified receive lower weights.\n",
        "\n",
        "New Weight (w_i) = w_i * exp(α * I(y_i ≠ h_t(x_i)))\n",
        "\n",
        "α (alpha) is a coefficient that represents the contribution of the weak classifier to the final strong classifier. It is calculated as 0.5 * ln((1 - ε) / ε), where ε is the weighted error rate.\n",
        "5. Normalization: After updating the weights, AdaBoost normalizes them so that they sum to 1. This ensures that the weights remain a probability distribution.\n",
        "\n",
        "6. Repeat: Steps 2 to 5 are repeated for a predefined number of iterations (or until a stopping criterion is met), with each iteration focusing on the samples that were misclassified in the previous iteration.\n",
        "\n",
        "7. Combine Weak Classifiers: Finally, AdaBoost combines the weak classifiers into a strong classifier by giving more weight to the classifiers that performed well in terms of their weighted error rates (α values)."
      ],
      "metadata": {
        "id": "6_d_aMWe7Bf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10 answer\n",
        "\n",
        "In the AdaBoost algorithm, the number of estimators refers to the number of weak classifiers (often referred to as \"base learners\" or \"stumps\") that are combined to create a strong ensemble classifier. Increasing the number of estimators in AdaBoost can have several effects on the algorithm's performance and behavior:\n",
        "\n",
        "1. Improved Classification Accuracy: One of the most notable effects of increasing the number of estimators is an improvement in classification accuracy. As you add more weak classifiers to the ensemble, the algorithm has the potential to better fit the training data and capture complex patterns and decision boundaries.\n",
        "\n",
        "2. Reduced Bias: With more estimators, AdaBoost becomes less prone to underfitting (high bias). This is because the ensemble has more capacity to model the training data accurately. As a result, the algorithm can reduce the bias in the final ensemble classifier.\n",
        "\n",
        "3. Increased Variance: While increasing the number of estimators reduces bias, it can also increase variance. AdaBoost can become more sensitive to noise in the data as it tries to fit the training samples more closely. This can lead to overfitting, especially if the dataset is noisy or if weak classifiers are too complex.\n",
        "\n",
        "4. Slower Training: Training AdaBoost with a larger number of estimators typically requires more iterations, each involving the training of a weak classifier. This can lead to longer training times, especially if the weak classifiers are computationally expensive.\n",
        "\n",
        "5. Diminishing Returns: There are diminishing returns associated with increasing the number of estimators. Initially, adding more estimators can lead to substantial improvements in performance. However, after a certain point, further increases may only result in marginal gains in accuracy.\n",
        "\n",
        "6. Risk of Overfitting: If the number of estimators is set too high without appropriate regularization techniques, AdaBoost can overfit the training data. Regularization techniques like limiting the depth of individual weak classifiers or early stopping can help mitigate overfitting.\n",
        "\n",
        "7. Greater Sensitivity to Outliers: As the number of estimators increases, AdaBoost can become more sensitive to outliers. Misclassified samples with high weights can dominate the learning process, potentially causing the algorithm to focus too much on difficult-to-classify outliers."
      ],
      "metadata": {
        "id": "5EXARab78VVt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F2CA6hqj4GEw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}